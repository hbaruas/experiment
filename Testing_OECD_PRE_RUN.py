{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Spark for DAP environment (4 CPU, 16GB RAM)\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\"\"\"\n",
    "SAFE WRITE OPERATIONS GUIDE\n",
    "=========================\n",
    "\n",
    "The safe_write_parquet function provides a secure way to write Spark DataFrames\n",
    "to parquet files with multiple safety checks to prevent accidental data loss.\n",
    "\n",
    "Key Features:\n",
    "1. Path validation\n",
    "2. Backup creation\n",
    "3. Atomic writes\n",
    "4. Error recovery\n",
    "\n",
    "Usage Example:\n",
    "-------------\n",
    "```python\n",
    "# Basic usage\n",
    "spark.safe_write_parquet(\n",
    "    df=my_dataframe,\n",
    "    output_path=\"/path/to/output.parquet\",\n",
    "    root_path=\"/path/to/root\"\n",
    ")\n",
    "\n",
    "# With backup\n",
    "spark.safe_write_parquet(\n",
    "    df=my_dataframe,\n",
    "    output_path=\"/path/to/output.parquet\",\n",
    "    root_path=\"/path/to/root\",\n",
    "    create_backup=True\n",
    ")\n",
    "```\n",
    "\n",
    "Safety Measures:\n",
    "--------------\n",
    "1. Path validation\n",
    "   - Ensures output path is within project boundaries\n",
    "   - Prevents writing to system directories\n",
    "   \n",
    "2. Backup (Optional)\n",
    "   - Creates timestamped backup of existing data\n",
    "   - Maintains version history\n",
    "   \n",
    "3. Atomic write\n",
    "   - Uses temporary directory for writing\n",
    "   - Moves to final location only after successful write\n",
    "   \n",
    "4. Error handling\n",
    "   - Restores from backup on failure\n",
    "   - Cleans up temporary files\n",
    "\"\"\"\n",
    "\n",
    "# Safety check for root path\n",
    "def validate_root_path(path):\n",
    "    \"\"\"Ensure the path exists and is within our project boundaries\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to validate\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if path is valid\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If path is invalid or outside project boundary\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError(f\"Root path does not exist: {path}\")\n",
    "    # Add any additional safety checks specific to your environment\n",
    "    return True\n",
    "\n",
    "# Calculate memory configurations (leaving overhead for system and other processes)\n",
    "executor_memory = \"13g\"  # 13GB for executor to leave room for overhead\n",
    "driver_memory = \"3g\"    # 3GB for driver operations\n",
    "\n",
    "# Create Spark session with DAP-specific configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BGT_Data_Processing\") \\\n",
    "    .config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .config(\"spark.driver.memory\", driver_memory) \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.cores\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Safe write function for Spark DataFrames\n",
    "def safe_write_parquet(df, output_path, root_path, create_backup=True):\n",
    "    \"\"\"\n",
    "    Safely write a DataFrame to parquet with multiple safety checks and backup option\n",
    "    \n",
    "    This function provides several safety measures:\n",
    "    1. Path validation to prevent writing outside project boundaries\n",
    "    2. Optional backup of existing data\n",
    "    3. Atomic write operation using temporary directory\n",
    "    4. Error handling and recovery\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to write\n",
    "        output_path: Target path for writing\n",
    "        root_path: Project root path for safety boundary\n",
    "        create_backup: Whether to create backup of existing data (default: True)\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If paths are invalid or operations fail\n",
    "        \n",
    "    Example:\n",
    "        >>> safe_write_parquet(\n",
    "        ...     df=my_dataframe,\n",
    "        ...     output_path=\"/path/to/output.parquet\",\n",
    "        ...     root_path=\"/path/to/root\",\n",
    "        ...     create_backup=True\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Normalize paths for comparison\n",
    "    norm_output = os.path.normpath(output_path)\n",
    "    norm_root = os.path.normpath(root_path)\n",
    "    \n",
    "    # 1. Safety checks\n",
    "    if not norm_output.startswith(norm_root):\n",
    "        raise ValueError(f\"Output path {output_path} is outside project root {root_path}\")\n",
    "    \n",
    "    # 2. Create parent directory if it doesn't exist\n",
    "    parent_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    \n",
    "    # 3. Backup existing data if needed\n",
    "    if create_backup and os.path.exists(output_path):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_path = f\"{output_path}_backup_{timestamp}\"\n",
    "        try:\n",
    "            # Read existing parquet file\n",
    "            existing_df = spark.read.parquet(output_path)\n",
    "            # Write backup\n",
    "            existing_df.write \\\n",
    "                .mode('overwrite') \\\n",
    "                .option('compression', 'snappy') \\\n",
    "                .option('path', backup_path) \\\n",
    "                .save()\n",
    "            print(f\"Created backup at: {backup_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not create backup: {str(e)}\")\n",
    "    \n",
    "    # 4. Create temporary directory for atomic write\n",
    "    temp_path = f\"{output_path}_temp_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    try:\n",
    "        # 5. Write to temporary location\n",
    "        df.write \\\n",
    "            .mode('overwrite') \\\n",
    "            .option('compression', 'snappy') \\\n",
    "            .option('path', temp_path) \\\n",
    "            .save()\n",
    "        \n",
    "        # 6. Validate the write was successful\n",
    "        try:\n",
    "            # Try reading back the data\n",
    "            spark.read.parquet(temp_path).count()\n",
    "        except:\n",
    "            raise ValueError(\"Failed to validate written data\")\n",
    "        \n",
    "        # 7. Move to final location (atomic operation)\n",
    "        if os.path.exists(output_path):\n",
    "            os.rename(output_path, f\"{output_path}_old\")\n",
    "        os.rename(temp_path, output_path)\n",
    "        \n",
    "        # 8. Clean up old file if exists\n",
    "        if os.path.exists(f\"{output_path}_old\"):\n",
    "            os.remove(f\"{output_path}_old\")\n",
    "            \n",
    "        print(f\"Successfully wrote data to: {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # 9. Clean up on error\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        raise ValueError(f\"Failed to write data: {str(e)}\")\n",
    "\n",
    "# Add safe_write_parquet to SparkSession for easy access\n",
    "setattr(SparkSession, 'safe_write_parquet', safe_write_parquet)\n",
    "\n",
    "# Display configuration for verification\n",
    "print(\"Spark Configuration:\")\n",
    "print(f\"Executor Memory: {executor_memory}\")\n",
    "print(f\"Driver Memory: {driver_memory}\")\n",
    "print(f\"Executor/Driver Cores: 4\")\n",
    "print(f\"Adaptive Query Execution: Enabled\")\n",
    "print(f\"Safe Write Function: Registered with backup support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bc398",
   "metadata": {},
   "source": [
    "# Multi-Year Data Analysis for Data-Intensive Jobs\n",
    "\n",
    "## Overview and Usage Guide\n",
    "\n",
    "This notebook analyzes job postings data to identify and analyze data-intensive jobs across multiple years. Here's how to use this notebook:\n",
    "\n",
    "### 1. User Configuration\n",
    "\n",
    "Key parameters you can modify:\n",
    "```python\n",
    "# In the File Path Configuration section:\n",
    "YEARS = \"2025\"          # Single year\n",
    "YEARS = \"ALL\"           # All available years\n",
    "YEARS = \"2020-2023\"     # Range of years\n",
    "YEARS = [2020, 2022]    # Specific years\n",
    "\n",
    "# In the Data Processing section:\n",
    "SIM_THRESHOLD = 0.45    # Similarity threshold (0.0 to 1.0)\n",
    "DATA_THRESHOLD = 3      # Minimum data mentions\n",
    "REL_SHARE = 10.0        # Relative frequency threshold\n",
    "```\n",
    "\n",
    "### 2. Notebook Structure\n",
    "\n",
    "1. **Configuration** (Cells 1-6):\n",
    "   - Spark environment setup\n",
    "   - Memory and processing parameters\n",
    "   - File path configuration\n",
    "   - Year selection and validation\n",
    "\n",
    "2. **Data Loading** (Cells 7-8):\n",
    "   - Loading job posting data for each year\n",
    "   - Data cleaning and validation\n",
    "   - Multi-year data combination\n",
    "\n",
    "3. **Processing Pipeline** (Cells 9-12):\n",
    "   - NLP processing for noun chunk extraction\n",
    "   - Data reduction and filtering\n",
    "   - Category analysis and aggregation\n",
    "\n",
    "4. **Visualization** (Cells 13-16):\n",
    "   - Year-specific visualizations\n",
    "   - Multi-year comparisons\n",
    "   - Interactive dashboards\n",
    "   - Trend analysis\n",
    "\n",
    "### 3. Output Organization\n",
    "\n",
    "Results are organized in two ways:\n",
    "1. Year-specific folders: `/results/{year}/`\n",
    "2. Combined analysis: `/results/combined/`\n",
    "\n",
    "### 4. Running the Notebook\n",
    "\n",
    "1. Set your configuration parameters\n",
    "2. Run all cells in sequence\n",
    "3. Check visualizations in the output folders\n",
    "4. Monitor memory usage for large datasets\n",
    "\n",
    "### 5. Memory Requirements\n",
    "\n",
    "- Single year: ~16GB RAM\n",
    "- Multiple years: 32GB+ RAM recommended\n",
    "- Adjust Spark configuration if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for reading large partitioned Parquet dataset (300M rows, 141.8GB)\n",
    "\"\"\"\n",
    "# Reading partitioned Parquet data\n",
    "# Assuming the data is partitioned by date and stored in a structure like:\n",
    "# /path/to/parquet_data/\n",
    "#   └── date=2023-01-01/\n",
    "#   └── date=2023-01-02/\n",
    "#   └── date=2023-01-03/\n",
    "#   ...and so on\n",
    "\n",
    "# The partitioning is handled automatically by Spark\n",
    "df = spark.read \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .option(\"basePath\", \"/path/to/parquet_data\") \\\n",
    "    .parquet(\"/path/to/parquet_data/*\") \\\n",
    "    .repartition(200)  # Adjust based on data size and available resources\n",
    "\n",
    "# You can also read specific partitions for testing\n",
    "# df_sample = spark.read.parquet(\"/path/to/parquet_data/date=2023-01-01\")\n",
    "\n",
    "# Print schema and partition information\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nPartition Information:\")\n",
    "print(f\"Number of partitions: {df.rdd.getNumPartitions()}\")\n",
    "print(f\"Total rows: {df.count():,}\")\n",
    "\n",
    "# Optional: You can also filter by partition for faster processing\n",
    "# df_filtered = df.filter(col(\"date\") >= \"2023-01-01\")\n",
    "\n",
    "# Memory optimization tips for large datasets:\n",
    "# 1. Use selective column reading\n",
    "# df = spark.read.parquet(...).select(\"job_id\", \"full_text\", \"date\", \"soc_2020\")\n",
    "#\n",
    "# 2. Cache only when necessary\n",
    "# df.cache() # Only if you'll reuse the DataFrame multiple times\n",
    "#\n",
    "# 3. Use partition pruning\n",
    "# df = df.filter(col(\"date\").between(\"2023-01-01\", \"2023-12-31\"))\n",
    "#\n",
    "# 4. Monitor memory usage\n",
    "# print(f\"Memory used: {df.select('*').explain()}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a483f",
   "metadata": {},
   "source": [
    "# Data-Intensive Jobs Identification Pipeline\n",
    "\n",
    "This notebook implements the complete methodology for identifying and analyzing data-intensive jobs from Lightcast job advertisements. The pipeline consists of four main stages:\n",
    "\n",
    "1. **Data Loading**: Reading job posting data from S3a/Hadoop (parquet format)\n",
    "2. **NLP Processing**: Using spaCy for noun chunk extraction and data-relatedness scoring\n",
    "3. **Data Reduction**: Aggregating and filtering noun chunks based on similarity and frequency\n",
    "4. **Category Analysis**: Classifying jobs into data-related categories and producing occupation/sector-level metrics\n",
    "\n",
    "Key Parameters:\n",
    "- SIM_THRESHOLD: 0.5 (minimum similarity score for data-related terms)\n",
    "- REL_SHARE_THRESHOLD: 10.0 (relative frequency threshold for category selection)\n",
    "- DATA_THRESHOLD: 3 (minimum mentions for category assignment)\n",
    "\n",
    "The pipeline is designed to process the full UK job advertisements dataset (85M rows) using PySpark for distributed processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configure Spark session with optimized settings for large-scale processing\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('data_jobs_pipeline') \\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', 'true') \\\n",
    "    .config('spark.driver.memory', '32g') \\\n",
    "    .config('spark.executor.memory', '32g') \\\n",
    "    .config('spark.executor.cores', '8') \\\n",
    "    .config('spark.sql.shuffle.partitions', '400') \\\n",
    "    .config('spark.sql.adaptive.enabled', 'true') \\\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config('spark.kryoserializer.buffer.max', '2000m') \\\n",
    "    .config('spark.sql.broadcastTimeout', '7200') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# S3a/Hadoop configuration for DAP\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "print(\"Spark session created successfully with configuration for large-scale processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ae3a2",
   "metadata": {},
   "source": [
    "## File Path Configuration\n",
    "\n",
    "Configure all input and output paths here. This makes it easy to:\n",
    "1. Test with the 30k dataset locally\n",
    "2. Scale up to the 85M dataset later\n",
    "3. Keep all file paths organized in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3a/DAP Path Configuration\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "USER CONFIGURATION GUIDE\n",
    "======================\n",
    "\n",
    "This notebook processes job posting data to analyze data-intensive jobs. Here are the key parameters\n",
    "you can configure:\n",
    "\n",
    "1. YEARS Parameter\n",
    "----------------\n",
    "Controls which years of data to process. Can be set in multiple formats:\n",
    "- \"ALL\"              : Process all available years\n",
    "- \"2020-2023\"       : Process a range of years\n",
    "- [2020, 2022, 2023]: Process specific years\n",
    "- \"2023\"            : Process a single year\n",
    "\n",
    "Examples:\n",
    "YEARS = \"ALL\"            # Will process all available years\n",
    "YEARS = \"2020-2023\"      # Will process years 2020, 2021, 2022, and 2023\n",
    "YEARS = [2020, 2022]     # Will process only 2020 and 2022\n",
    "YEARS = \"2023\"           # Will process only 2023\n",
    "\n",
    "2. Processing Parameters (in other cells)\n",
    "-------------------------------------\n",
    "- SIM_THRESHOLD    : Minimum similarity score for data-related terms (default: 0.45)\n",
    "                     Higher values mean stricter matching to \"data\" related terms\n",
    "- DATA_THRESHOLD   : Minimum mentions for category assignment (default: 3)\n",
    "                     Higher values require more data-related terms in a job posting\n",
    "- REL_SHARE       : Relative frequency threshold for terms (default: 10.0)\n",
    "                     Controls how common a term needs to be within a category\n",
    "\n",
    "3. Spark Configuration\n",
    "-------------------\n",
    "If you need to adjust memory or processing power:\n",
    "- executor_memory : Memory per executor (default: \"13g\")\n",
    "- driver_memory  : Memory for driver (default: \"3g\")\n",
    "- num_partitions : Number of partitions for processing (auto-calculated)\n",
    "\n",
    "4. Output Organization\n",
    "-------------------\n",
    "Results are organized in:\n",
    "- Year-specific folders: For single-year analysis\n",
    "- Combined folder: For multi-year analysis\n",
    "- Visualizations: Both per-year and combined views\n",
    "\n",
    "Note: The notebook will automatically create all necessary directories.\n",
    "\"\"\"\n",
    "\n",
    "# Define base OECD_DATA path\n",
    "BASE_S3A_PATH = \"s3a://em_sources/_SDD/OECD_DATA\"  # Main data folder\n",
    "\n",
    "# Function to get available years from the data\n",
    "def get_available_years():\n",
    "    \"\"\"Get list of available years in the input data\"\"\"\n",
    "    # This would need to be implemented based on your storage structure\n",
    "    # For now, returning a sample range\n",
    "    return list(range(2020, 2026))\n",
    "\n",
    "def parse_year_input(year_input):\n",
    "    \"\"\"Parse year input to handle multiple years or 'ALL'\n",
    "    \n",
    "    Args:\n",
    "        year_input: Can be:\n",
    "            - \"ALL\" for all available years\n",
    "            - A single year (str or int)\n",
    "            - A list of years [2020, 2021, 2022]\n",
    "            - A range \"2020-2023\"\n",
    "    \n",
    "    Returns:\n",
    "        List of years to process\n",
    "    \n",
    "    Examples:\n",
    "        >>> parse_year_input(\"ALL\")\n",
    "        [2020, 2021, 2022, 2023, 2024, 2025]\n",
    "        >>> parse_year_input(\"2020-2023\")\n",
    "        [2020, 2021, 2022, 2023]\n",
    "        >>> parse_year_input([2020, 2022])\n",
    "        [2020, 2022]\n",
    "    \"\"\"\n",
    "    available_years = get_available_years()\n",
    "    \n",
    "    if isinstance(year_input, str):\n",
    "        if year_input.upper() == \"ALL\":\n",
    "            return available_years\n",
    "        elif \"-\" in year_input:\n",
    "            # Handle range like \"2020-2023\"\n",
    "            start, end = map(int, year_input.split(\"-\"))\n",
    "            years = list(range(start, end + 1))\n",
    "        else:\n",
    "            # Handle single year\n",
    "            years = [int(year_input)]\n",
    "    elif isinstance(year_input, (list, tuple)):\n",
    "        # Handle list of years\n",
    "        years = [int(y) for y in year_input]\n",
    "    else:\n",
    "        # Handle single integer year\n",
    "        years = [int(year_input)]\n",
    "    \n",
    "    # Validate years are available\n",
    "    invalid_years = [y for y in years if y not in available_years]\n",
    "    if invalid_years:\n",
    "        raise ValueError(f\"Years {invalid_years} not available. Available years: {available_years}\")\n",
    "    \n",
    "    return sorted(years)\n",
    "\n",
    "#############################################\n",
    "# USER CONFIGURATION - SET YOUR PARAMETERS HERE\n",
    "#############################################\n",
    "\n",
    "# Set YEARS - Modify this value based on your needs\n",
    "YEARS = \"2025\"  # Default to current year\n",
    "\n",
    "# Optional: Uncomment and modify one of these examples:\n",
    "# YEARS = \"ALL\"           # Process all available years\n",
    "# YEARS = \"2020-2023\"     # Process a range of years\n",
    "# YEARS = [2020, 2022]    # Process specific years\n",
    "# YEARS = \"2023\"          # Process single year\n",
    "\n",
    "#############################################\n",
    "# END USER CONFIGURATION\n",
    "#############################################\n",
    "\n",
    "# Convert YEARS to list of years to process\n",
    "years_to_process = parse_year_input(YEARS)\n",
    "print(f\"Processing years: {years_to_process}\")\n",
    "\n",
    "# Safety function to ensure we stay within OECD_DATA\n",
    "def validate_oecd_path(path):\n",
    "    \"\"\"Ensure the path stays within OECD_DATA boundary\"\"\"\n",
    "    if not path.startswith(BASE_S3A_PATH):\n",
    "        raise ValueError(f\"Path {path} is outside OECD_DATA folder!\")\n",
    "    return True\n",
    "\n",
    "# Configure paths - everything stays within OECD_DATA\n",
    "def get_paths_for_year(year):\n",
    "    \"\"\"Get paths configuration for a specific year\"\"\"\n",
    "    return {\n",
    "        # Input data (existing data)\n",
    "        'input_parquet': os.path.join(BASE_S3A_PATH, 'parquet_data', str(year)),\n",
    "        'input_csv': os.path.join(BASE_S3A_PATH, 'csv_data', str(year)),\n",
    "        \n",
    "        # New processing outputs (will be created inside OECD_DATA)\n",
    "        'noun_chunks': os.path.join(BASE_S3A_PATH, 'processed_data', str(year), 'noun_chunks'),\n",
    "        'reduced_data': os.path.join(BASE_S3A_PATH, 'processed_data', str(year), 'reduced_data'),\n",
    "        \n",
    "        # Results (will be created inside OECD_DATA)\n",
    "        'results': os.path.join(BASE_S3A_PATH, 'results', str(year)),\n",
    "        'visualizations': os.path.join(BASE_S3A_PATH, 'visualizations', str(year))\n",
    "    }\n",
    "\n",
    "# Create paths for all years to process\n",
    "ALL_PATHS = {year: get_paths_for_year(year) for year in years_to_process}\n",
    "\n",
    "# Aggregate paths for multi-year results\n",
    "MULTI_YEAR_PATHS = {\n",
    "    'combined_results': os.path.join(BASE_S3A_PATH, 'results', 'combined'),\n",
    "    'combined_visualizations': os.path.join(BASE_S3A_PATH, 'visualizations', 'combined')\n",
    "}\n",
    "\n",
    "# Verify all paths are within OECD_DATA\n",
    "for year_paths in ALL_PATHS.values():\n",
    "    for path in year_paths.values():\n",
    "        validate_oecd_path(path)\n",
    "for path in MULTI_YEAR_PATHS.values():\n",
    "    validate_oecd_path(path)\n",
    "\n",
    "# Function to safely check path existence\n",
    "def check_s3a_path(path):\n",
    "    \"\"\"Check if an S3a path exists without modifying anything\"\"\"\n",
    "    try:\n",
    "        # Add your specific S3a path checking logic here\n",
    "        # This should only READ, never modify\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not verify path {path}\\nError: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "print(\"OECD_DATA Configuration:\")\n",
    "print(f\"Base Path: {BASE_S3A_PATH}\")\n",
    "print(f\"Years to process: {years_to_process}\")\n",
    "print(\"\\nFolder Structure:\")\n",
    "for year, paths in ALL_PATHS.items():\n",
    "    print(f\"\\nYear {year}:\")\n",
    "    for key, path in paths.items():\n",
    "        print(f\"{key:20}: {path}\")\n",
    "        exists = check_s3a_path(path)\n",
    "        if exists:\n",
    "            print(f\"{' ':22}Status: Found\")\n",
    "        else:\n",
    "            print(f\"{' ':22}Status: Will be created (no existing data will be deleted)\")\n",
    "\n",
    "print(\"\\nCombined Results Paths:\")\n",
    "for key, path in MULTI_YEAR_PATHS.items():\n",
    "    print(f\"{key:20}: {path}\")\n",
    "\n",
    "print(\"\\nNOTE: All processing will:\")\n",
    "print(\"1. READ from existing parquet_data and csv_data folders for each year\")\n",
    "print(\"2. CREATE new subfolders for outputs for each year\")\n",
    "print(\"3. PRESERVE all existing data\")\n",
    "print(\"4. STAY within OECD_DATA boundary\")\n",
    "print(\"5. CREATE combined results for multi-year analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc82a64",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "We'll load the job posting data from S3a/Hadoop. The data is stored in parquet format and contains:\n",
    "- Job ID\n",
    "- Full text of job posting\n",
    "- Occupation codes (SOC 2020)\n",
    "- Sector information\n",
    "- Other metadata\n",
    "\n",
    "The data has been pre-processed from XML to parquet format using the `conversion.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data for a specific year\n",
    "def read_year_data(year_paths):\n",
    "    \"\"\"Read data for a specific year with proper options\"\"\"\n",
    "    df = spark.read.option('header', True) \\\n",
    "        .option('multiline', True) \\\n",
    "        .option('escape', '\"') \\\n",
    "        .option('quote', '\"') \\\n",
    "        .csv(year_paths['input_csv'])\n",
    "\n",
    "    # Select and clean the original columns exactly as provided\n",
    "    df = df.select(\n",
    "        'date',           # Original date column\n",
    "        'job_id',         # Original job ID\n",
    "        'soc_2020',       # Original SOC code\n",
    "        'job_title',      # Original job title\n",
    "        F.col('full_text').cast('string')  # Original full text\n",
    "    ).filter(\n",
    "        F.col('full_text').isNotNull() & \n",
    "        (F.length('full_text') > 0)\n",
    "    )\n",
    "\n",
    "    # Convert date to proper format if needed\n",
    "    df = df.withColumn('date', F.to_date('date', 'yyyy-MM-dd'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read data for all years to process\n",
    "all_data = {}\n",
    "total_jobs = 0\n",
    "\n",
    "for year in years_to_process:\n",
    "    print(f\"\\nProcessing year {year}:\")\n",
    "    year_df = read_year_data(ALL_PATHS[year])\n",
    "    year_count = year_df.count()\n",
    "    total_jobs += year_count\n",
    "    print(f\"Jobs loaded for {year}: {year_count:,}\")\n",
    "    all_data[year] = year_df\n",
    "\n",
    "print(f\"\\nTotal jobs loaded across all years: {total_jobs:,}\")\n",
    "\n",
    "# If processing multiple years, combine the data\n",
    "if len(years_to_process) > 1:\n",
    "    print(\"\\nCombining data from all years...\")\n",
    "    combined_df = None\n",
    "    for year_df in all_data.values():\n",
    "        if combined_df is None:\n",
    "            combined_df = year_df\n",
    "        else:\n",
    "            combined_df = combined_df.union(year_df)\n",
    "    \n",
    "    # Cache the combined data if we have enough memory\n",
    "    combined_df.cache()\n",
    "    \n",
    "    print(\"\\nSample records from combined dataset:\")\n",
    "    combined_df.select(\n",
    "        'date', \n",
    "        'job_id', \n",
    "        'soc_2020', \n",
    "        'job_title',\n",
    "        F.substring('full_text', 1, 100).alias('full_text_sample')\n",
    "    ).orderBy('date').show(5, truncate=False)\n",
    "    \n",
    "    print(\"\\nData distribution by year:\")\n",
    "    combined_df.groupBy(F.year('date').alias('year')).count().orderBy('year').show()\n",
    "else:\n",
    "    # Single year processing\n",
    "    combined_df = all_data[years_to_process[0]]\n",
    "    print(\"\\nSchema of original columns:\")\n",
    "    combined_df.printSchema()\n",
    "    print(\"\\nSample records:\")\n",
    "    combined_df.select(\n",
    "        'date', \n",
    "        'job_id', \n",
    "        'soc_2020', \n",
    "        'job_title',\n",
    "        F.substring('full_text', 1, 100).alias('full_text_sample')\n",
    "    ).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fe009",
   "metadata": {},
   "source": [
    "## 2. NLP Processing: Noun Chunk Extraction\n",
    "\n",
    "In this stage, we:\n",
    "1. Load the spaCy model (en_core_web_md)\n",
    "2. Process job descriptions to extract noun chunks\n",
    "3. Calculate similarity scores between each chunk and the word \"data\"\n",
    "4. Use PySpark's mapInPandas for efficient parallel processing\n",
    "\n",
    "The noun chunks provide a focused way to identify data-related terms in job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for derived columns in noun chunk extraction\n",
    "schema = StructType([\n",
    "    StructField('doc_date', StringType()),      # Derived from 'date'\n",
    "    StructField('doc_JobID', StringType()),     # Derived from 'job_id'\n",
    "    StructField('doc_BGTOcc', StringType()),    # Derived from 'soc_2020'\n",
    "    StructField('noun_chunk', StringType()),    # Generated from NLP\n",
    "    StructField('sim_data', DoubleType())       # Generated similarity score\n",
    "])\n",
    "\n",
    "def extract_noun_chunks(iterator):\n",
    "    \"\"\"Process batches of job descriptions to extract and score noun chunks\"\"\"\n",
    "    import spacy\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_lg', exclude=['lemmatizer', 'ner'])\n",
    "    target = nlp('data')\n",
    "    \n",
    "    for pdf in iterator:\n",
    "        rows = []\n",
    "        # Use original column names when accessing the data\n",
    "        texts = pdf['full_text'].fillna('').astype(str).tolist()\n",
    "        jobids = pdf['job_id'].astype(str).tolist()\n",
    "        dates = pdf['date'].astype(str).tolist()\n",
    "        socs = pdf['soc_2020'].astype(str).tolist()\n",
    "        \n",
    "        # Process texts in batch\n",
    "        docs = nlp.pipe(texts, batch_size=50, n_process=1)\n",
    "        for i, doc in enumerate(docs):\n",
    "            for chunk in doc.noun_chunks:\n",
    "                if chunk.has_vector:\n",
    "                    try:\n",
    "                        sim = float(chunk.similarity(target))\n",
    "                        # Clean chunk text by removing numbers\n",
    "                        clean_chunk = ''.join([c for c in chunk.text if not c.isdigit()]).strip()\n",
    "                        if clean_chunk:  # Only add if non-empty after cleaning\n",
    "                            rows.append({\n",
    "                                'doc_date': dates[i],\n",
    "                                'doc_JobID': jobids[i],\n",
    "                                'doc_BGTOcc': socs[i],\n",
    "                                'noun_chunk': clean_chunk,\n",
    "                                'sim_data': sim\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        \n",
    "        if rows:\n",
    "            yield pd.DataFrame(rows)\n",
    "        else:\n",
    "            yield pd.DataFrame(columns=['doc_date', 'doc_JobID', 'doc_BGTOcc', 'noun_chunk', 'sim_data'])\n",
    "\n",
    "# Calculate partitions based on data size\n",
    "estimated_row_size = 1000  # bytes per row estimate\n",
    "total_rows = df.count()\n",
    "target_partition_size = 128 * 1024 * 1024  # 128MB target partition size\n",
    "num_partitions = max(8, (total_rows * estimated_row_size) // target_partition_size)\n",
    "\n",
    "print(f\"Processing with {num_partitions} partitions\")\n",
    "\n",
    "# Process in parallel using standard PySpark DataFrame operations\n",
    "df_repartitioned = df.repartition(num_partitions)\n",
    "noun_chunks_df = df_repartitioned.mapInPandas(extract_noun_chunks, schema=schema)\n",
    "\n",
    "# Cache results for multiple uses\n",
    "noun_chunks_df.cache()\n",
    "\n",
    "print(\"Sample of extracted noun chunks with derived columns:\")\n",
    "noun_chunks_df.show(10, truncate=False)\n",
    "\n",
    "# Save results\n",
    "noun_chunks_df.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .option('compression', 'snappy') \\\n",
    "    .partitionBy('doc_date') \\\n",
    "    .parquet(PATHS['noun_chunks'])\n",
    "\n",
    "print(f\"Noun chunks saved to: {PATHS['noun_chunks']}\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nExtraction Statistics:\")\n",
    "print(f\"Total noun chunks: {noun_chunks_df.count():,}\")\n",
    "print(f\"Unique jobs processed: {noun_chunks_df.select('doc_JobID').distinct().count():,}\")\n",
    "print(f\"Average chunks per job: {noun_chunks_df.count() / noun_chunks_df.select('doc_JobID').distinct().count():.2f}\")\n",
    "\n",
    "# Clean up memory\n",
    "noun_chunks_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e4931",
   "metadata": {},
   "source": [
    "## 3. Data Reduction\n",
    "\n",
    "This stage reduces the volume of noun chunks by:\n",
    "1. Filtering by similarity score threshold\n",
    "2. Aggregating identical chunks within jobs\n",
    "3. Computing frequency statistics\n",
    "4. Applying relative frequency thresholds\n",
    "\n",
    "This step significantly reduces the data volume while preserving meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb178f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load noun chunks - reading partitioned data efficiently\n",
    "noun_chunks = spark.read.parquet(PATHS['noun_chunks'])\n",
    "\n",
    "\"\"\"\n",
    "PROCESSING PARAMETERS CONFIGURATION\n",
    "================================\n",
    "\n",
    "Adjust these parameters to control how the data is processed and filtered:\n",
    "\n",
    "1. SIM_THRESHOLD (Similarity Threshold)\n",
    "------------------------------------\n",
    "Controls how similar a term needs to be to \"data\" to be included\n",
    "- Range: 0.0 to 1.0\n",
    "- Default: 0.45\n",
    "- Higher values (e.g., 0.6): More strict matching, fewer but more relevant terms\n",
    "- Lower values (e.g., 0.3): More lenient matching, more terms but possible noise\n",
    "- Recommended range: 0.4 to 0.6\n",
    "\n",
    "2. DATA_THRESHOLD (in job_analysis function)\n",
    "----------------------------------------\n",
    "Minimum number of data-related terms needed to classify a job as data-intensive\n",
    "- Default: 3\n",
    "- Higher values: More strict classification, fewer but more clearly data-intensive jobs\n",
    "- Lower values: More lenient classification, more jobs but possible false positives\n",
    "- Recommended range: 2 to 5\n",
    "\n",
    "3. REL_SHARE (Relative Share Threshold)\n",
    "-----------------------------------\n",
    "Controls how common a term needs to be within its category\n",
    "- Default: 10.0\n",
    "- Higher values: More strict filtering, only very common terms\n",
    "- Lower values: More lenient filtering, includes rarer terms\n",
    "- Recommended range: 5.0 to 15.0\n",
    "\"\"\"\n",
    "\n",
    "#############################################\n",
    "# USER CONFIGURATION - PROCESSING PARAMETERS\n",
    "#############################################\n",
    "\n",
    "# Similarity score threshold\n",
    "SIM_THRESHOLD = 0.45  # Adjust this value based on your needs (0.0 to 1.0)\n",
    "\n",
    "# Note: DATA_THRESHOLD and REL_SHARE are set in the job_analysis function\n",
    "# You can modify them there if needed:\n",
    "# DATA_THRESHOLD = 3    # Minimum mentions for data-intensive classification\n",
    "# REL_SHARE = 10.0     # Relative frequency threshold\n",
    "\n",
    "#############################################\n",
    "# END USER CONFIGURATION\n",
    "#############################################\n",
    "\n",
    "# Clean and transform - optimized for large-scale processing\n",
    "cleaned = noun_chunks.withColumn('counter', F.lit(1)) \\\n",
    "    .withColumn('noun_chunk', F.lower(F.col('noun_chunk'))) \\\n",
    "    .withColumn('doc_BGTOcc', F.regexp_replace(F.col('doc_BGTOcc'), '[.-]', ''))\n",
    "\n",
    "# Cache if we have enough memory\n",
    "cleaned.cache()\n",
    "\n",
    "# Filter and aggregate with optimized groupBy\n",
    "filtered = cleaned.filter(F.col('sim_data') >= SIM_THRESHOLD)\n",
    "\n",
    "# Use adaptive query execution for complex aggregations\n",
    "reduced = filtered.groupBy('noun_chunk', 'doc_JobID', 'doc_BGTOcc').agg(\n",
    "    F.avg('sim_data').alias('sim_data'),\n",
    "    F.sum('counter').alias('counter')\n",
    ")\n",
    "\n",
    "# Repartition before writing if needed\n",
    "num_partitions = max(8, reduced.rdd.getNumPartitions() // 2)\n",
    "reduced = reduced.repartition(num_partitions)\n",
    "\n",
    "print(\"Sample of reduced data:\")\n",
    "reduced.show(10)\n",
    "\n",
    "# Save reduced dataset with optimizations\n",
    "reduced.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .option('compression', 'snappy') \\\n",
    "    .partitionBy('doc_BGTOcc') \\  # Partition by occupation for efficient category analysis\n",
    "    .parquet(PATHS['reduced_data'])\n",
    "\n",
    "print(f\"Reduced dataset saved to: {PATHS['reduced_data']}\")\n",
    "\n",
    "# Compute and display statistics\n",
    "total_chunks = reduced.count()\n",
    "unique_jobs = reduced.select('doc_JobID').distinct().count()\n",
    "unique_chunks = reduced.select('noun_chunk').distinct().count()\n",
    "\n",
    "print(f\"\\nReduction Statistics:\")\n",
    "print(f\"Total noun chunk records: {total_chunks:,}\")\n",
    "print(f\"Unique jobs: {unique_jobs:,}\")\n",
    "print(f\"Unique noun chunks: {unique_chunks:,}\")\n",
    "print(f\"Compression ratio: {noun_chunks.count() / total_chunks:.2f}x\")\n",
    "\n",
    "# Cleanup\n",
    "cleaned.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae837a6",
   "metadata": {},
   "source": [
    "## 4. Category Analysis and Final Aggregation\n",
    "\n",
    "In this final stage, we:\n",
    "1. Define occupation categories (data_entry, database, data_analytics)\n",
    "2. Select category-specific noun chunks using similarity and frequency thresholds\n",
    "3. Classify jobs based on noun chunk occurrences\n",
    "4. Aggregate results by occupation and sector\n",
    "5. Compute final metrics and shares\n",
    "\n",
    "This produces the final analysis of data-intensive jobs across the economy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9891f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reduced dataset\n",
    "reduced = spark.read.parquet(PATHS['reduced_data'])\n",
    "\n",
    "# Rest of the code remains the same until the final save...\n",
    "\n",
    "# Save results\n",
    "occupation_summary.write.mode('overwrite').parquet(PATHS['occupation_summary'])\n",
    "job_categories.write.mode('overwrite').parquet(PATHS['job_categories'])\n",
    "\n",
    "print(f\"\\nResults saved to: {OUTPUT_DIR}/final_results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae56eb",
   "metadata": {},
   "source": [
    "# Visualizations and Analysis\n",
    "\n",
    "This section creates visualizations from either live processed data or saved results. These visualizations help demonstrate:\n",
    "1. Data processing pipeline effectiveness\n",
    "2. Distribution of data-intensive jobs\n",
    "3. Temporal trends and patterns\n",
    "4. Sector-specific insights\n",
    "5. Methodology validation\n",
    "\n",
    "Each visualization includes explanatory text suitable for inclusion in final reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Setup\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to load saved results or use live data\n",
    "def get_analysis_data(paths, use_saved=True):\n",
    "    \"\"\"\n",
    "    Load data for visualization from either saved files or live DataFrames\n",
    "    \n",
    "    Args:\n",
    "        paths: Dictionary of data paths\n",
    "        use_saved: Boolean to indicate whether to use saved data\n",
    "    \"\"\"\n",
    "    if use_saved:\n",
    "        occupation_summary = spark.read.parquet(paths['occupation_summary']).toPandas()\n",
    "        job_categories = spark.read.parquet(paths['job_categories']).toPandas()\n",
    "        reduced_data = spark.read.parquet(paths['reduced_data']).toPandas()\n",
    "    else:\n",
    "        # Use live DataFrames\n",
    "        occupation_summary = occupation_summary_df.toPandas()\n",
    "        job_categories = job_categories_df.toPandas()\n",
    "        reduced_data = reduced_df.toPandas()\n",
    "    \n",
    "    return occupation_summary, job_categories, reduced_data\n",
    "\n",
    "# Set style configurations\n",
    "plt.style.use('seaborn')\n",
    "colors = px.colors.qualitative.Set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ae0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VISUALIZATION FUNCTIONS GUIDE\n",
    "===========================\n",
    "\n",
    "This section contains the core visualization functions for analyzing data-intensive jobs.\n",
    "Each function creates specific types of visualizations for both single-year and multi-year analysis.\n",
    "\n",
    "Main Functions:\n",
    "1. plot_methodology_validation: Creates 4 validation plots\n",
    "2. plot_occupational_insights: Creates 4 occupation analysis plots\n",
    "3. plot_year_comparisons: Creates year-over-year comparison plots\n",
    "4. create_multi_year_dashboard: Creates interactive dashboards\n",
    "\n",
    "Usage:\n",
    "1. Single Year Analysis:\n",
    "   ```python\n",
    "   # For a single year\n",
    "   method_fig = plot_methodology_validation(year_data['reduced_data'])\n",
    "   method_fig.write_html('methodology_validation.html')\n",
    "   ```\n",
    "\n",
    "2. Multi-Year Analysis:\n",
    "   ```python\n",
    "   # For multiple years\n",
    "   combined_fig = plot_methodology_validation(combined_data['reduced_data'])\n",
    "   combined_fig.write_html('methodology_all_years.html')\n",
    "   ```\n",
    "\n",
    "3. Customization:\n",
    "   - Modify subplot titles\n",
    "   - Adjust chart parameters\n",
    "   - Change color schemes\n",
    "   - Configure layout options\n",
    "\"\"\"\n",
    "\n",
    "# 1. Methodology Validation Visualizations\n",
    "def plot_methodology_validation(reduced_data):\n",
    "    \"\"\"Create visualizations that validate the methodology with year-over-year comparison\n",
    "    \n",
    "    This function creates four subplots:\n",
    "    1. Similarity Score Distribution: Shows how well terms match with \"data\"\n",
    "    2. Noun Chunks per Job: Distribution of data-related terms in job postings\n",
    "    3. Top Data Terms: Most common data-related terms across all years\n",
    "    4. Temporal Distribution: Time series of data term occurrences\n",
    "    \n",
    "    Args:\n",
    "        reduced_data (DataFrame): Processed data with noun chunks and similarity scores\n",
    "        \n",
    "    Returns:\n",
    "        plotly.Figure: Figure with 4 subplots for methodology validation\n",
    "    \"\"\"\n",
    "    # Add year column if not present\n",
    "    if 'year' not in reduced_data.columns:\n",
    "        reduced_data['year'] = pd.to_datetime(reduced_data['doc_date']).dt.year\n",
    "    \n",
    "    # Create subplot layout\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Similarity Score Distribution by Year',\n",
    "            'Noun Chunks per Job by Year',\n",
    "            'Top Data-Related Terms (All Years)',\n",
    "            'Temporal Distribution'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 1. Similarity Score Distribution by Year\n",
    "    # Shows how well terms match with \"data\" across years\n",
    "    for year in sorted(reduced_data['year'].unique()):\n",
    "        year_data = reduced_data[reduced_data['year'] == year]\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=year_data['sim_data'], \n",
    "                name=str(year),\n",
    "                opacity=0.7, \n",
    "                nbinsx=30,\n",
    "                hovertemplate=\"Score: %{x}<br>Count: %{y}\"\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Noun Chunks per Job by Year\n",
    "    # Shows distribution of data-related terms in job postings\n",
    "    for year in sorted(reduced_data['year'].unique()):\n",
    "        year_data = reduced_data[reduced_data['year'] == year]\n",
    "        chunks_per_job = year_data.groupby('doc_JobID').size()\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=chunks_per_job, \n",
    "                name=str(year),\n",
    "                opacity=0.7, \n",
    "                nbinsx=30,\n",
    "                hovertemplate=\"Terms per Job: %{x}<br>Count: %{y}\"\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Top Data-Related Terms (All Years)\n",
    "    # Shows most common data-related terms\n",
    "    top_terms = reduced_data.groupby('noun_chunk')['counter'].sum() \\\n",
    "        .sort_values(ascending=False).head(20)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=top_terms.index, \n",
    "            y=top_terms.values,\n",
    "            hovertemplate=\"Term: %{x}<br>Frequency: %{y}\"\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Temporal Distribution with Year-over-Year Comparison\n",
    "    # Shows trends over time\n",
    "    time_dist = reduced_data.groupby(['year', 'doc_date'])['counter'].count() \\\n",
    "        .reset_index()\n",
    "    for year in sorted(time_dist['year'].unique()):\n",
    "        year_data = time_dist[time_dist['year'] == year]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=year_data['doc_date'], \n",
    "                y=year_data['counter'],\n",
    "                name=str(year), \n",
    "                mode='lines',\n",
    "                hovertemplate=\"Date: %{x}<br>Count: %{y}\"\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout with comprehensive formatting\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Methodology Validation Metrics (Year-over-Year Analysis)\",\n",
    "        showlegend=True,\n",
    "        # Add subplot titles\n",
    "        annotations=[\n",
    "            dict(\n",
    "                text=\"Higher scores indicate stronger relation to data\",\n",
    "                showarrow=False,\n",
    "                x=0.225, y=1.1,\n",
    "                xref='paper', yref='paper'\n",
    "            ),\n",
    "            dict(\n",
    "                text=\"Distribution of data terms per job posting\",\n",
    "                showarrow=False,\n",
    "                x=0.775, y=1.1,\n",
    "                xref='paper', yref='paper'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Similarity Score\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Number of Terms\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Term\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Number of Jobs\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Total Occurrences\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Daily Count\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Rest of the visualization functions with similar detailed documentation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da326b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VISUALIZATION GENERATION GUIDE\n",
    "===========================\n",
    "\n",
    "This section handles the generation and saving of all visualizations for both\n",
    "single-year and multi-year analyses. The process includes:\n",
    "\n",
    "1. Loading Data:\n",
    "   - Year-specific data from parquet files\n",
    "   - Combining data for multi-year analysis\n",
    "\n",
    "2. Creating Visualizations:\n",
    "   - Year-specific visualizations\n",
    "   - Combined multi-year visualizations\n",
    "   - Trend analysis and comparisons\n",
    "\n",
    "3. Output Organization:\n",
    "   - Year-specific folders for individual analysis\n",
    "   - Combined folder for multi-year analysis\n",
    "   - Interactive HTML files for each visualization\n",
    "\n",
    "Usage Examples:\n",
    "--------------\n",
    "1. Generate visualizations for all processed years:\n",
    "   ```python\n",
    "   viz_path = generate_report_visualizations(use_saved=True)\n",
    "   ```\n",
    "\n",
    "2. Access specific visualizations:\n",
    "   ```python\n",
    "   # For single year\n",
    "   IFrame(src='path/to/year/methodology_validation.html', width=1000, height=600)\n",
    "   \n",
    "   # For multiple years\n",
    "   IFrame(src='path/to/combined/year_over_year_comparison.html', width=1000, height=800)\n",
    "   ```\n",
    "\n",
    "Output Structure:\n",
    "---------------\n",
    "/visualizations/\n",
    "    /{year}/\n",
    "        - methodology_validation.html\n",
    "        - occupational_insights.html\n",
    "    /combined/\n",
    "        - methodology_validation_all_years.html\n",
    "        - occupational_insights_all_years.html\n",
    "        - year_over_year_comparison.html\n",
    "        - temporal_trends_all_years.html\n",
    "        - sector_analysis_all_years.html\n",
    "\"\"\"\n",
    "\n",
    "# Generate all visualizations with multi-year support\n",
    "def generate_report_visualizations(use_saved=True):\n",
    "    \"\"\"Generate comprehensive visualization report for data-intensive jobs analysis\n",
    "    \n",
    "    This function creates both year-specific and multi-year visualizations,\n",
    "    organizing them in a clear directory structure for easy access.\n",
    "    \n",
    "    Args:\n",
    "        use_saved (bool): Whether to use saved parquet files (True) or \n",
    "                         live DataFrames (False)\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the combined visualizations directory\n",
    "    \n",
    "    Creates:\n",
    "    1. Year-specific visualizations:\n",
    "       - Methodology validation\n",
    "       - Occupational analysis\n",
    "    \n",
    "    2. Combined visualizations:\n",
    "       - Multi-year methodology validation\n",
    "       - Multi-year occupational analysis\n",
    "       - Year-over-year comparisons\n",
    "       - Temporal trends\n",
    "       - Sector analysis\n",
    "    \"\"\"\n",
    "    # Load data for all years\n",
    "    print(\"Step 1: Loading data...\")\n",
    "    all_years_data = {}\n",
    "    for year in years_to_process:\n",
    "        if use_saved:\n",
    "            year_paths = ALL_PATHS[year]\n",
    "            occupation_summary = spark.read.parquet(year_paths['occupation_summary']).toPandas()\n",
    "            job_categories = spark.read.parquet(year_paths['job_categories']).toPandas()\n",
    "            reduced_data = spark.read.parquet(year_paths['reduced_data']).toPandas()\n",
    "            all_years_data[year] = {\n",
    "                'occupation_summary': occupation_summary,\n",
    "                'job_categories': job_categories,\n",
    "                'reduced_data': reduced_data\n",
    "            }\n",
    "    \n",
    "    # Combine data from all years\n",
    "    print(\"Step 2: Combining multi-year data...\")\n",
    "    combined_data = {\n",
    "        'occupation_summary': pd.concat([d['occupation_summary'] for d in all_years_data.values()]),\n",
    "        'job_categories': pd.concat([d['job_categories'] for d in all_years_data.values()]),\n",
    "        'reduced_data': pd.concat([d['reduced_data'] for d in all_years_data.values()])\n",
    "    }\n",
    "    \n",
    "    # Create output directories\n",
    "    print(\"Step 3: Setting up output directories...\")\n",
    "    viz_paths = {}\n",
    "    for year in years_to_process:\n",
    "        year_viz_path = os.path.join(ALL_PATHS[year]['visualizations'])\n",
    "        os.makedirs(year_viz_path, exist_ok=True)\n",
    "        viz_paths[year] = year_viz_path\n",
    "    \n",
    "    combined_viz_path = MULTI_YEAR_PATHS['combined_visualizations']\n",
    "    os.makedirs(combined_viz_path, exist_ok=True)\n",
    "    \n",
    "    # Generate year-specific visualizations\n",
    "    print(\"Step 4: Generating year-specific visualizations...\")\n",
    "    for year in years_to_process:\n",
    "        print(f\"\\nProcessing year {year}...\")\n",
    "        year_data = all_years_data[year]\n",
    "        \n",
    "        # 4.1 Methodology Validation\n",
    "        method_fig = plot_methodology_validation(year_data['reduced_data'])\n",
    "        method_fig.write_html(os.path.join(viz_paths[year], 'methodology_validation.html'))\n",
    "        \n",
    "        # 4.2 Occupational Analysis\n",
    "        occ_fig = plot_occupational_insights(year_data['occupation_summary'])\n",
    "        occ_fig.write_html(os.path.join(viz_paths[year], 'occupational_insights.html'))\n",
    "    \n",
    "    # Generate combined multi-year visualizations\n",
    "    print(\"\\nStep 5: Generating multi-year visualizations...\")\n",
    "    \n",
    "    # 5.1 Methodology Validation across all years\n",
    "    combined_method_fig = plot_methodology_validation(combined_data['reduced_data'])\n",
    "    combined_method_fig.write_html(os.path.join(combined_viz_path, 'methodology_validation_all_years.html'))\n",
    "    \n",
    "    # 5.2 Occupational Analysis across all years\n",
    "    combined_occ_fig = plot_occupational_insights(combined_data['occupation_summary'])\n",
    "    combined_occ_fig.write_html(os.path.join(combined_viz_path, 'occupational_insights_all_years.html'))\n",
    "    \n",
    "    # 5.3 Year-over-Year Comparisons\n",
    "    year_comparison_fig = plot_year_comparisons(\n",
    "        combined_data['reduced_data'],\n",
    "        combined_data['occupation_summary']\n",
    "    )\n",
    "    year_comparison_fig.write_html(os.path.join(combined_viz_path, 'year_over_year_comparison.html'))\n",
    "    \n",
    "    # 5.4 Trend Analysis\n",
    "    trend_fig = px.line(\n",
    "        combined_data['reduced_data'].groupby(['year', 'doc_date'])['counter'].sum().reset_index(),\n",
    "        x='doc_date',\n",
    "        y='counter',\n",
    "        color='year',\n",
    "        title='Temporal Trends in Data-Related Job Postings'\n",
    "    )\n",
    "    trend_fig.write_html(os.path.join(combined_viz_path, 'temporal_trends_all_years.html'))\n",
    "    \n",
    "    # 5.5 Sector Analysis with Year Comparison\n",
    "    sector_fig = px.treemap(\n",
    "        combined_data['occupation_summary'],\n",
    "        path=['year', 'sector', 'category'],\n",
    "        values='job_count',\n",
    "        title='Data Jobs Distribution by Year, Sector and Category'\n",
    "    )\n",
    "    sector_fig.write_html(os.path.join(combined_viz_path, 'sector_analysis_all_years.html'))\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nVisualization Generation Complete!\")\n",
    "    print(f\"Year-specific visualizations:\")\n",
    "    for year, path in viz_paths.items():\n",
    "        print(f\"  {year}: {path}\")\n",
    "    print(f\"Combined visualizations: {combined_viz_path}\")\n",
    "    \n",
    "    return combined_viz_path\n",
    "\n",
    "# Run visualization generation\n",
    "if len(years_to_process) > 1:\n",
    "    print(\"Generating multi-year visualizations...\")\n",
    "else:\n",
    "    print(f\"Generating visualizations for year {years_to_process[0]}...\")\n",
    "\n",
    "viz_path = generate_report_visualizations(use_saved=True)\n",
    "\n",
    "# Display sample visualization in notebook\n",
    "from IPython.display import IFrame\n",
    "if len(years_to_process) > 1:\n",
    "    # Show combined visualization if multiple years\n",
    "    IFrame(src=os.path.join(viz_path, 'year_over_year_comparison.html'), width=1000, height=800)\n",
    "else:\n",
    "    # Show single year visualization if only one year\n",
    "    IFrame(src=os.path.join(viz_path, 'methodology_validation.html'), width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392bca9b",
   "metadata": {},
   "source": [
    "## Notes on Production Deployment\n",
    "\n",
    "For production deployment with 85M rows:\n",
    "\n",
    "1. **Spark Configuration**\n",
    "   - Increase executor memory and cores based on cluster resources\n",
    "   - Tune partition counts for optimal parallelism\n",
    "   - Consider using `spark.sql.adaptive.enabled` for better shuffle optimization\n",
    "\n",
    "2. **NLP Processing**\n",
    "   - Consider using Spark NLP for distributed processing\n",
    "   - Or use careful partitioning with spaCy (one model per executor)\n",
    "   - Monitor memory usage with large language models\n",
    "\n",
    "3. **Data Storage**\n",
    "   - Partition output files by year/month for efficient querying\n",
    "   - Use appropriate compression codecs (e.g., snappy)\n",
    "   - Consider caching frequently accessed DataFrames\n",
    "\n",
    "4. **Performance Monitoring**\n",
    "   - Monitor executor memory usage\n",
    "   - Watch for data skew in job distributions\n",
    "   - Adjust thresholds based on data characteristics\n",
    "\n",
    "5. **Validation**\n",
    "   - Compare category distributions across time periods\n",
    "   - Verify occupation code mappings\n",
    "   - Sample and manually verify category assignments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
