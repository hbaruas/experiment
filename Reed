import requests
import pandas as pd
from tqdm import tqdm
import time
import urllib3
import os

# ‚úÖ Suppress SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ‚úÖ Your REED API Key here
API_KEY = 'YOUR_API_KEY_HERE'

# Constants
BASE_URL = 'https://www.reed.co.uk/api/1.0/search'
PAGE_SIZE = 25
MAX_EMPTY_RESPONSES = 5  # stop after multiple empty pages

# Output files
FINAL_FILE = 'reed_jobs_uk_extended.csv'
PARTIAL_FILE = 'reed_jobs_partial.csv'

# Resumable storage
all_jobs = []
page = 1
empty_page_count = 0

print("üîÑ Starting job scraping...")

# Create progress bar without knowing total jobs ahead of time
progress = tqdm(desc='Fetching Jobs', ncols=100)

try:
    while True:
        params = {
            'resultsToTake': PAGE_SIZE,
            'resultsToSkip': (page - 1) * PAGE_SIZE
        }

        response = requests.get(
            BASE_URL,
            headers={'Accept': 'application/json'},
            auth=(API_KEY, ''),
            params=params,
            verify=False
        )

        if response.status_code != 200:
            print(f"‚ùå Error {response.status_code}: {response.text}")
            break

        jobs = response.json().get('results', [])

        if not jobs:
            empty_page_count += 1
            print(f"‚ö†Ô∏è Empty page #{empty_page_count} (Page {page})")
            if empty_page_count >= MAX_EMPTY_RESPONSES:
                print("‚õî Too many empty pages, assuming end of data.")
                break
        else:
            empty_page_count = 0  # Reset counter on success

        for job in jobs:
            job_id = job.get('jobId')
            title = job.get('jobTitle')
            description = job.get('jobDescription')

            if job_id and title and description:
                all_jobs.append({
                    'job_id': job_id,
                    'title': title,
                    'description': description,
                    'location': job.get('locationName'),
                    'employer': job.get('employerName'),
                    'salary_min': job.get('minimumSalary'),
                    'salary_max': job.get('maximumSalary'),
                    'currency': job.get('currency'),
                    'date_posted': job.get('datePosted'),
                    'url': job.get('jobUrl')
                })
                progress.update(1)

        # Save progress after every page
        pd.DataFrame(all_jobs).to_csv(PARTIAL_FILE, index=False)

        page += 1
        time.sleep(0.5)

except Exception as e:
    print(f"\n‚ö†Ô∏è Script failed with error: {e}")
    print("üíæ Saving partial progress...")

finally:
    progress.close()
    pd.DataFrame(all_jobs).to_csv(FINAL_FILE, index=False)
    print(f"\n‚úÖ Finished. Total jobs scraped: {len(all_jobs)}")
    print(f"üìÑ Final saved to: {FINAL_FILE}")
