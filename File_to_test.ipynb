{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa37668c",
   "metadata": {},
   "source": [
    "CELL 0 — Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b463d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "import datetime\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe76fac",
   "metadata": {},
   "source": [
    "CELL 1 — Spark session (reuse if already running)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7550637",
   "metadata": {},
   "source": [
    "try:\n",
    "    spark  # noqa: F821\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    print(\"Re-using existing Spark session.\")\n",
    "except NameError:\n",
    "    auto_config = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"OECD_Data_Intensity_Pipeline\")\n",
    "        .config(\"spark.executor.memory\", \"13g\")\n",
    "        .config(\"spark.driver.memory\", \"3g\")\n",
    "        .config(\"spark.executor.cores\", \"4\")\n",
    "        .config(\"spark.driver.cores\", \"4\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "    )\n",
    "    spark = auto_config.getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    print(\"Spark session created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e48bb",
   "metadata": {},
   "source": [
    "CELL 2 — Global parameters (NO loose vars later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4874df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Pipeline parameters\n",
    "# ---------------------------\n",
    "YEARS = \"2020-2025\"   # \"ALL\" or \"2020-2025\" or \"2025\" or [2020,2022]\n",
    "\n",
    "# Sampling\n",
    "SAMPLE_FRACTION = 0.01       # 1% dev mode; set to 1.0 for full run\n",
    "SAMPLE_SEED = 42\n",
    "STRATIFIED_BY_SOC = False   # True = more representative; False = fastest\n",
    "\n",
    "# NLP thresholds (OECD-style)\n",
    "SIM_THRESHOLD = 0.45         # filter for \"data-related\" semantic similarity\n",
    "DATA_THRESHOLD = 3           # ≥ 3 data-related chunks => data-intensive job\n",
    "\n",
    "# How much to store per job\n",
    "TOP_K = 10                   # store top K most \"data-like\" chunks\n",
    "\n",
    "# Re-run behaviour\n",
    "FORCE_RECOMPUTE = False      # if True, overwrite outputs even if exist\n",
    "WRITE_DEBUG_CHUNKS = False   # only for tiny tests\n",
    "DEBUG_SAMPLE_FRACTION = 0.0001\n",
    "\n",
    "print(\"Parameters set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e5686",
   "metadata": {},
   "source": [
    "CELL 3 — Paths (S3A root + per-year output folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07997004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory: keep everything inside OECD_DATA\n",
    "BASE_S3A_PATH = \"s3a://onscdp-prd-data01-d4946922/dapsen/workspace_zone/online_job_ads/OECD_DATA\"\n",
    "\n",
    "# If you ever want to read from a single parquet instead of CSV partitions\n",
    "PARQUET_PATH = None  # e.g. f\"{BASE_S3A_PATH}/somefile.parquet\"\n",
    "CSV_PATH = None      # if None, uses BASE_S3A_PATH/csv_data/<year>\n",
    "\n",
    "def get_available_years():\n",
    "    return list(range(2020, 2026))\n",
    "\n",
    "def parse_year_input(year_input):\n",
    "    available = get_available_years()\n",
    "    if isinstance(year_input, str):\n",
    "        y = year_input.strip().upper()\n",
    "        if y == \"ALL\":\n",
    "            years = available\n",
    "        elif \"-\" in y:\n",
    "            s, e = map(int, y.split(\"-\"))\n",
    "            years = list(range(s, e + 1))\n",
    "        else:\n",
    "            years = [int(y)]\n",
    "    elif isinstance(year_input, (list, tuple)):\n",
    "        years = [int(x) for x in year_input]\n",
    "    else:\n",
    "        years = [int(year_input)]\n",
    "    bad = [yy for yy in years if yy not in available]\n",
    "    if bad:\n",
    "        raise ValueError(f\"Invalid years {bad}. Available: {available}\")\n",
    "    return sorted(years)\n",
    "\n",
    "years_to_process = parse_year_input(YEARS)\n",
    "print(\"Years to process:\", years_to_process)\n",
    "\n",
    "def validate_oecd_path(path: str) -> bool:\n",
    "    if not path.startswith(BASE_S3A_PATH):\n",
    "        raise ValueError(f\"Path {path} is outside OECD_DATA root\")\n",
    "    return True\n",
    "\n",
    "def get_paths_for_year(year: int):\n",
    "    base_year_path = os.path.join(BASE_S3A_PATH, \"processed_data\", str(year))\n",
    "    input_csv = os.path.join(BASE_S3A_PATH, \"csv_data\", str(year)) if CSV_PATH is None else os.path.join(CSV_PATH, str(year))\n",
    "\n",
    "    return {\n",
    "        # input\n",
    "        \"input_csv\": input_csv,\n",
    "\n",
    "        # outputs (job-level, efficient)\n",
    "        \"job_features\": os.path.join(base_year_path, \"job_features\"),\n",
    "        \"debug_chunks\": os.path.join(base_year_path, \"debug_chunks\"),\n",
    "\n",
    "        # classification outputs\n",
    "        \"job_categories\": os.path.join(base_year_path, \"job_categories\"),\n",
    "        \"occupation_summary\": os.path.join(base_year_path, \"occupation_summary\"),\n",
    "\n",
    "        # synthetic industry outputs\n",
    "        \"job_categories_sic\": os.path.join(base_year_path, \"job_categories_sic\"),\n",
    "        \"sector_summary_sic\": os.path.join(base_year_path, \"sector_summary_sic\"),\n",
    "    }\n",
    "\n",
    "ALL_PATHS = {yr: get_paths_for_year(yr) for yr in years_to_process}\n",
    "\n",
    "# Validate\n",
    "for yr, paths in ALL_PATHS.items():\n",
    "    for k, p in paths.items():\n",
    "        validate_oecd_path(p)\n",
    "\n",
    "print(\"Path validation complete.\")\n",
    "print(\"Example year paths:\", years_to_process[0], ALL_PATHS[years_to_process[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e72185",
   "metadata": {},
   "source": [
    "CELL 4 — Safe writing utilities (S3A-safe, versioned, restartable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_write_parquet_s3a(df, output_base_path, mode=\"overwrite\", create_version=True, verify_read=True):\n",
    "    \"\"\"\n",
    "    Safe-ish write for object stores (s3a://).\n",
    "    Writes to version folder output_base_path/_v=YYYYMMDD_HHMMSS and updates a _LATEST_POINTER file.\n",
    "    \"\"\"\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    version_path = f\"{output_base_path}/_v={ts}\" if create_version else output_base_path\n",
    "\n",
    "    print(f\"[WRITE] parquet -> {version_path}\")\n",
    "    df.write.mode(mode).option(\"compression\", \"snappy\").parquet(version_path)\n",
    "\n",
    "    if verify_read:\n",
    "        _ = spark.read.parquet(version_path).limit(1).count()\n",
    "\n",
    "    # marker (optional)\n",
    "    try:\n",
    "        marker_df = spark.createDataFrame([(ts,)], [\"written_at\"])\n",
    "        marker_df.coalesce(1).write.mode(\"overwrite\").json(f\"{version_path}/_SUCCESS_MARKER\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: marker write failed: {e}\")\n",
    "\n",
    "    # pointer\n",
    "    if create_version:\n",
    "        try:\n",
    "            latest_df = spark.createDataFrame([(version_path, ts)], [\"latest_path\", \"timestamp\"])\n",
    "            latest_df.coalesce(1).write.mode(\"overwrite\").json(f\"{output_base_path}/_LATEST_POINTER\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: pointer update failed: {e}\")\n",
    "\n",
    "    return version_path\n",
    "\n",
    "def read_latest_version(output_base_path):\n",
    "    pointer_path = f\"{output_base_path}/_LATEST_POINTER\"\n",
    "    pointer = spark.read.json(pointer_path)\n",
    "    latest = pointer.orderBy(F.col(\"timestamp\").desc()).limit(1).collect()[0][\"latest_path\"]\n",
    "    return spark.read.parquet(latest), latest\n",
    "\n",
    "print(\"Utilities ready: safe_write_parquet_s3a(), read_latest_version()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf6f53",
   "metadata": {},
   "source": [
    "CELL 5 — Step 1: Load data per year (with sampling parameter)\n",
    "This is where you control 1% / 2% / 10% without touching downstream logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "total_jobs = 0\n",
    "\n",
    "if PARQUET_PATH:\n",
    "    print(f\"[LOAD] unified parquet from {PARQUET_PATH}\")\n",
    "    full_df = spark.read.parquet(PARQUET_PATH).withColumn(\"date\", F.to_date(\"date\"))\n",
    "\n",
    "    for yr in tqdm(years_to_process, desc=\"Reading data (Parquet)\"):\n",
    "        df = full_df.filter(F.year(\"date\") == yr)\n",
    "\n",
    "        if SAMPLE_FRACTION is not None and float(SAMPLE_FRACTION) < 1.0:\n",
    "            frac = float(SAMPLE_FRACTION)\n",
    "            df = df.sample(False, frac, seed=SAMPLE_SEED)\n",
    "            print(f\"[LOAD] Year {yr}: sampled {frac*100:.2f}%\")\n",
    "\n",
    "        cnt = df.count()\n",
    "        all_data[yr] = df\n",
    "        total_jobs += cnt\n",
    "        print(f\"[LOAD] Year {yr}: {cnt:,} rows\")\n",
    "\n",
    "else:\n",
    "    for yr in tqdm(years_to_process, desc=\"Reading data (CSV)\"):\n",
    "        print(f\"\\n[LOAD] Year {yr} from {ALL_PATHS[yr]['input_csv']} ...\")\n",
    "\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"multiline\", True)\n",
    "            .csv(ALL_PATHS[yr][\"input_csv\"])\n",
    "            .select(\n",
    "                \"date\",\n",
    "                \"job_id\",\n",
    "                \"soc_2020\",\n",
    "                \"job_title\",\n",
    "                F.col(\"full_text\").cast(\"string\")\n",
    "            )\n",
    "            .filter(F.col(\"full_text\").isNotNull() & (F.length(\"full_text\") > 0))\n",
    "            .withColumn(\"date\", F.to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "        )\n",
    "\n",
    "        # Sampling\n",
    "        if SAMPLE_FRACTION is not None and float(SAMPLE_FRACTION) < 1.0:\n",
    "            frac = float(SAMPLE_FRACTION)\n",
    "\n",
    "            if STRATIFIED_BY_SOC:\n",
    "                # approximate stratified sample\n",
    "                soc_vals = [r[\"soc_2020\"] for r in df.select(\"soc_2020\").distinct().limit(50000).collect()]\n",
    "                fractions = {s: frac for s in soc_vals if s is not None}\n",
    "                df = df.sampleBy(\"soc_2020\", fractions=fractions, seed=SAMPLE_SEED)\n",
    "                print(f\"[LOAD] Year {yr}: stratified sample {frac*100:.2f}% by SOC\")\n",
    "            else:\n",
    "                df = df.sample(False, frac, seed=SAMPLE_SEED)\n",
    "                print(f\"[LOAD] Year {yr}: uniform sample {frac*100:.2f}%\")\n",
    "\n",
    "        cnt = df.count()\n",
    "        all_data[yr] = df\n",
    "        total_jobs += cnt\n",
    "        print(f\"[LOAD] Year {yr}: {cnt:,} rows\")\n",
    "\n",
    "print(f\"\\n[LOAD] Total jobs loaded: {total_jobs:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3734d8c1",
   "metadata": {},
   "source": [
    "CELL 6 — Step 2: NLP job-level feature extraction (efficient storage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87168d",
   "metadata": {},
   "source": [
    "OECD alignment (adapted)\n",
    "\t•\tOECD extracts noun chunks from “pure data” occupations then spreads list across all jobs.\n",
    "\t•\tYour efficient adaptation: still extracts noun chunks + similarity to “data”, but stores job-level summaries and top chunks. This keeps the pipeline scalable while preserving interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d033c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema: one row per job advert\n",
    "job_features_schema = StructType([\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"job_id\", StringType()),\n",
    "    StructField(\"soc_2020\", StringType()),\n",
    "    StructField(\"n_chunks_total\", IntegerType()),\n",
    "    StructField(\"n_chunks_data\", IntegerType()),\n",
    "    StructField(\"avg_sim_data\", DoubleType()),\n",
    "    StructField(\"top_chunks\", ArrayType(StringType())),\n",
    "    StructField(\"top_sims\", ArrayType(DoubleType())),\n",
    "])\n",
    "\n",
    "# Optional debug schema: chunk-level (ONLY for tiny sample)\n",
    "debug_schema = StructType([\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"job_id\", StringType()),\n",
    "    StructField(\"soc_2020\", StringType()),\n",
    "    StructField(\"noun_chunk\", StringType()),\n",
    "    StructField(\"sim_data\", DoubleType()),\n",
    "])\n",
    "\n",
    "def extract_job_features(iterator):\n",
    "    import spacy\n",
    "    import pandas as pd\n",
    "\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_lg\", exclude=[\"lemmatizer\", \"ner\"])\n",
    "    except OSError:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    target = nlp(\"data\")\n",
    "\n",
    "    for pdf in iterator:\n",
    "        pdf = pdf.copy()\n",
    "        pdf[\"full_text\"] = pdf[\"full_text\"].fillna(\"\").astype(str)\n",
    "        pdf[\"job_id\"] = pdf[\"job_id\"].astype(str)\n",
    "        pdf[\"soc_2020\"] = pdf[\"soc_2020\"].astype(str)\n",
    "        pdf[\"date\"] = pdf[\"date\"].astype(str)\n",
    "\n",
    "        out_rows = []\n",
    "        texts = pdf[\"full_text\"].tolist()\n",
    "        jobids = pdf[\"job_id\"].tolist()\n",
    "        socs = pdf[\"soc_2020\"].tolist()\n",
    "        dates = pdf[\"date\"].tolist()\n",
    "\n",
    "        for i, doc in enumerate(nlp.pipe(texts, batch_size=50, n_process=1)):\n",
    "            total_chunks = 0\n",
    "            data_chunks = []\n",
    "\n",
    "            for chunk in doc.noun_chunks:\n",
    "                total_chunks += 1\n",
    "                if chunk.has_vector:\n",
    "                    sim = float(chunk.similarity(target))\n",
    "                    if sim >= SIM_THRESHOLD:\n",
    "                        cleaned = \"\".join(c for c in chunk.text if not c.isdigit()).strip().lower()\n",
    "                        if cleaned:\n",
    "                            data_chunks.append((cleaned, sim))\n",
    "\n",
    "            n_data = len(data_chunks)\n",
    "            if n_data > 0:\n",
    "                data_chunks.sort(key=lambda x: x[1], reverse=True)\n",
    "                top = data_chunks[:TOP_K]\n",
    "                top_chunks = [t[0] for t in top]\n",
    "                top_sims = [float(t[1]) for t in top]\n",
    "                avg_sim = float(sum(t[1] for t in data_chunks) / n_data)\n",
    "            else:\n",
    "                top_chunks, top_sims, avg_sim = [], [], None\n",
    "\n",
    "            out_rows.append({\n",
    "                \"date\": dates[i],\n",
    "                \"job_id\": jobids[i],\n",
    "                \"soc_2020\": socs[i],\n",
    "                \"n_chunks_total\": int(total_chunks),\n",
    "                \"n_chunks_data\": int(n_data),\n",
    "                \"avg_sim_data\": avg_sim,\n",
    "                \"top_chunks\": top_chunks,\n",
    "                \"top_sims\": top_sims,\n",
    "            })\n",
    "\n",
    "        yield pd.DataFrame(out_rows)\n",
    "\n",
    "def extract_debug_chunks(iterator):\n",
    "    import spacy\n",
    "    import pandas as pd\n",
    "\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_lg\", exclude=[\"lemmatizer\", \"ner\"])\n",
    "    except OSError:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    target = nlp(\"data\")\n",
    "\n",
    "    for pdf in iterator:\n",
    "        pdf = pdf.copy()\n",
    "        pdf[\"full_text\"] = pdf[\"full_text\"].fillna(\"\").astype(str)\n",
    "        pdf[\"job_id\"] = pdf[\"job_id\"].astype(str)\n",
    "        pdf[\"soc_2020\"] = pdf[\"soc_2020\"].astype(str)\n",
    "        pdf[\"date\"] = pdf[\"date\"].astype(str)\n",
    "\n",
    "        rows = []\n",
    "        texts = pdf[\"full_text\"].tolist()\n",
    "        jobids = pdf[\"job_id\"].tolist()\n",
    "        socs = pdf[\"soc_2020\"].tolist()\n",
    "        dates = pdf[\"date\"].tolist()\n",
    "\n",
    "        for i, doc in enumerate(nlp.pipe(texts, batch_size=50, n_process=1)):\n",
    "            for chunk in doc.noun_chunks:\n",
    "                if chunk.has_vector:\n",
    "                    sim = float(chunk.similarity(target))\n",
    "                    cleaned = \"\".join(c for c in chunk.text if not c.isdigit()).strip().lower()\n",
    "                    if cleaned:\n",
    "                        rows.append({\n",
    "                            \"date\": dates[i],\n",
    "                            \"job_id\": jobids[i],\n",
    "                            \"soc_2020\": socs[i],\n",
    "                            \"noun_chunk\": cleaned,\n",
    "                            \"sim_data\": sim,\n",
    "                        })\n",
    "\n",
    "        if rows:\n",
    "            yield pd.DataFrame(rows)\n",
    "        else:\n",
    "            yield pd.DataFrame(columns=[\"date\",\"job_id\",\"soc_2020\",\"noun_chunk\",\"sim_data\"])\n",
    "\n",
    "for yr in tqdm(years_to_process, desc=\"NLP job feature extraction\"):\n",
    "    out_base = ALL_PATHS[yr][\"job_features\"]\n",
    "\n",
    "    # Skip if already computed\n",
    "    if not FORCE_RECOMPUTE:\n",
    "        try:\n",
    "            _df, latest_path = read_latest_version(out_base)\n",
    "            print(f\"\\n[SKIP] Year {yr} job_features already exist at: {latest_path}\")\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(f\"\\n[NLP] Year {yr} starting…\")\n",
    "    df = all_data[yr]\n",
    "\n",
    "    # Partition tuning (important)\n",
    "    total_rows = df.count()\n",
    "    estimated_row_size = 1000\n",
    "    target_partition_bytes = 128 * 1024 * 1024\n",
    "    num_partitions = max(16, int((total_rows * estimated_row_size) / target_partition_bytes))\n",
    "    print(f\"[NLP] Year {yr} rows={total_rows:,} partitions={num_partitions}\")\n",
    "\n",
    "    df_part = df.repartition(num_partitions)\n",
    "\n",
    "    job_feat = df_part.mapInPandas(extract_job_features, schema=job_features_schema)\n",
    "    job_feat = job_feat.withColumn(\"year\", F.year(F.to_date(\"date\"))) \\\n",
    "                       .withColumn(\"month\", F.month(F.to_date(\"date\")))\n",
    "\n",
    "    latest_written = safe_write_parquet_s3a(\n",
    "        job_feat,\n",
    "        out_base,\n",
    "        mode=\"overwrite\",\n",
    "        create_version=True,\n",
    "        verify_read=True\n",
    "    )\n",
    "\n",
    "    print(f\"[NLP] Year {yr} job_features written to: {latest_written}\")\n",
    "\n",
    "    if WRITE_DEBUG_CHUNKS:\n",
    "        dbg_base = ALL_PATHS[yr][\"debug_chunks\"]\n",
    "        dbg_df = df_part.sample(False, DEBUG_SAMPLE_FRACTION, seed=SAMPLE_SEED)\n",
    "        dbg_chunks = dbg_df.mapInPandas(extract_debug_chunks, schema=debug_schema) \\\n",
    "                           .withColumn(\"year\", F.year(F.to_date(\"date\"))) \\\n",
    "                           .withColumn(\"month\", F.month(F.to_date(\"date\")))\n",
    "        dbg_written = safe_write_parquet_s3a(dbg_chunks, dbg_base, create_version=True)\n",
    "        print(f\"[NLP-DEBUG] Year {yr} debug chunks written to: {dbg_written}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30f2e9",
   "metadata": {},
   "source": [
    "CELL 7 — Step 3: Job classification + Occupation summary (SOC4)\n",
    "Digit level answer: This produces occupation estimates at SOC 4-digit (soc4 = first 4 chars of soc_2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landmark SOC4 groups (your current “OECD categories via SOC” approach)\n",
    "data_entry_soc = {\"4111\",\"4112\",\"4113\",\"4114\",\"4121\",\"4131\",\"4132\",\"4150\"}\n",
    "database_soc = {\"2423\",\"2136\"}\n",
    "data_analytics_soc = {\"2421\",\"2424\",\"2133\",\"2135\"}\n",
    "\n",
    "for yr in tqdm(years_to_process, desc=\"Job classification + occ summary\"):\n",
    "    print(f\"\\n[CLASSIFY] Year {yr}\")\n",
    "\n",
    "    job_feat_df, job_feat_path = read_latest_version(ALL_PATHS[yr][\"job_features\"])\n",
    "    print(f\"[CLASSIFY] Using job_features from: {job_feat_path}\")\n",
    "\n",
    "    job_feat_df = job_feat_df.filter(F.year(F.to_date(\"date\")) == yr)\n",
    "    job_feat_df = job_feat_df.withColumn(\"soc4\", F.substring(F.col(\"soc_2020\"), 1, 4))\n",
    "\n",
    "    is_data_intensive = (F.col(\"n_chunks_data\") >= F.lit(DATA_THRESHOLD))\n",
    "\n",
    "    job_categories = (\n",
    "        job_feat_df.select(\"job_id\",\"soc_2020\",\"soc4\",\"date\",\"n_chunks_data\",\"avg_sim_data\",\"top_chunks\",\"top_sims\")\n",
    "        .withColumn(\"data_entry\", (F.col(\"soc4\").isin(list(data_entry_soc)) & is_data_intensive).cast(\"int\"))\n",
    "        .withColumn(\"database\", (F.col(\"soc4\").isin(list(database_soc)) & is_data_intensive).cast(\"int\"))\n",
    "        .withColumn(\"data_analytics\", (F.col(\"soc4\").isin(list(data_analytics_soc)) & is_data_intensive).cast(\"int\"))\n",
    "        .withColumn(\"any_data_intensive\", is_data_intensive.cast(\"int\"))\n",
    "        .withColumn(\"year\", F.year(F.to_date(\"date\")))\n",
    "        .withColumn(\"month\", F.month(F.to_date(\"date\")))\n",
    "    )\n",
    "\n",
    "    # Save job-level categories\n",
    "    jc_base = ALL_PATHS[yr][\"job_categories\"]\n",
    "    if FORCE_RECOMPUTE:\n",
    "        jc_written = safe_write_parquet_s3a(job_categories, jc_base, create_version=True)\n",
    "    else:\n",
    "        try:\n",
    "            _df, latest = read_latest_version(jc_base)\n",
    "            print(f\"[SKIP] job_categories exists: {latest}\")\n",
    "            jc_written = latest\n",
    "        except Exception:\n",
    "            jc_written = safe_write_parquet_s3a(job_categories, jc_base, create_version=True)\n",
    "\n",
    "    # Occupation summary (SOC4)\n",
    "    occ = (\n",
    "        job_categories.groupBy(\"soc4\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_jobs\"),\n",
    "            F.sum(\"data_entry\").alias(\"data_entry_jobs\"),\n",
    "            F.sum(\"database\").alias(\"database_jobs\"),\n",
    "            F.sum(\"data_analytics\").alias(\"data_analytics_jobs\"),\n",
    "            F.sum(\"any_data_intensive\").alias(\"any_data_intensive_jobs\"),\n",
    "        )\n",
    "        .withColumn(\"data_entry_share\", 100 * F.col(\"data_entry_jobs\") / F.col(\"total_jobs\"))\n",
    "        .withColumn(\"database_share\", 100 * F.col(\"database_jobs\") / F.col(\"total_jobs\"))\n",
    "        .withColumn(\"data_analytics_share\", 100 * F.col(\"data_analytics_jobs\") / F.col(\"total_jobs\"))\n",
    "        .withColumn(\"total_data_share\", 100 * F.col(\"any_data_intensive_jobs\") / F.col(\"total_jobs\"))\n",
    "        .withColumn(\"year\", F.lit(int(yr)))\n",
    "    )\n",
    "\n",
    "    os_base = ALL_PATHS[yr][\"occupation_summary\"]\n",
    "    if FORCE_RECOMPUTE:\n",
    "        os_written = safe_write_parquet_s3a(occ, os_base, create_version=True)\n",
    "    else:\n",
    "        try:\n",
    "            _df, latest = read_latest_version(os_base)\n",
    "            print(f\"[SKIP] occupation_summary exists: {latest}\")\n",
    "            os_written = latest\n",
    "        except Exception:\n",
    "            os_written = safe_write_parquet_s3a(occ, os_base, create_version=True)\n",
    "\n",
    "    print(f\"[CLASSIFY] Saved year {yr}: job_categories={jc_written} | occupation_summary={os_written}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbab08",
   "metadata": {},
   "source": [
    "CELL 8 — Step 4: Synthetic SIC Section (Option 1 rule-based SOC→SIC)\n",
    "This produces industry at SIC “Section” level (A–U style) — same level OECD often reports when rolling up to broad industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic SIC mapping (DEMO ONLY)\n",
    "# Rule: SOC major group (first digit) -> SIC Section letter\n",
    "SOC_MAJOR_TO_SIC = {\n",
    "    \"1\": \"M\",  # managers -> professional/technical proxy\n",
    "    \"2\": \"M\",\n",
    "    \"3\": \"J\",  # associate prof -> info/comm proxy\n",
    "    \"4\": \"N\",  # admin -> admin/support proxy\n",
    "    \"5\": \"F\",  # skilled trades -> construction proxy\n",
    "    \"6\": \"Q\",  # caring -> health/social proxy\n",
    "    \"7\": \"G\",  # sales -> wholesale/retail proxy\n",
    "    \"8\": \"C\",  # process/plant -> manufacturing proxy\n",
    "    \"9\": \"N\",  # elementary -> admin/support proxy\n",
    "}\n",
    "\n",
    "def soc_to_sic_section(soc):\n",
    "    if soc is None:\n",
    "        return None\n",
    "    s = str(soc).strip()\n",
    "    if len(s) == 0:\n",
    "        return None\n",
    "    return SOC_MAJOR_TO_SIC.get(s[0], None)\n",
    "\n",
    "sic_udf = F.udf(soc_to_sic_section, StringType())\n",
    "\n",
    "for yr in tqdm(years_to_process, desc=\"Synthetic SIC + sector summary\"):\n",
    "    print(f\"\\n[SIC-SYNTH] Year {yr}\")\n",
    "\n",
    "    jc_df, jc_path = read_latest_version(ALL_PATHS[yr][\"job_categories\"])\n",
    "    jc_df = jc_df.filter(F.year(F.to_date(\"date\")) == yr)\n",
    "    print(f\"[SIC-SYNTH] Using job_categories from: {jc_path}\")\n",
    "\n",
    "    jc_sic = jc_df.withColumn(\"SICSection_synth\", sic_udf(F.col(\"soc_2020\")))\n",
    "\n",
    "    # Save job-level with SIC\n",
    "    jc_sic_base = ALL_PATHS[yr][\"job_categories_sic\"]\n",
    "    if FORCE_RECOMPUTE:\n",
    "        jc_sic_written = safe_write_parquet_s3a(jc_sic, jc_sic_base, create_version=True)\n",
    "    else:\n",
    "        try:\n",
    "            _df, latest = read_latest_version(jc_sic_base)\n",
    "            print(f\"[SKIP] job_categories_sic exists: {latest}\")\n",
    "            jc_sic_written = latest\n",
    "        except Exception:\n",
    "            jc_sic_written = safe_write_parquet_s3a(jc_sic, jc_sic_base, create_version=True)\n",
    "\n",
    "    # Sector summary by synthetic SIC section\n",
    "    sec = (\n",
    "        jc_sic.filter(F.col(\"SICSection_synth\").isNotNull())\n",
    "        .groupBy(\"SICSection_synth\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_jobs\"),\n",
    "            F.sum(\"data_entry\").alias(\"data_entry_jobs\"),\n",
    "            F.sum(\"database\").alias(\"database_jobs\"),\n",
    "            F.sum(\"data_analytics\").alias(\"data_analytics_jobs\"),\n",
    "            F.sum(\"any_data_intensive\").alias(\"any_data_intensive_jobs\"),\n",
    "        )\n",
    "        .withColumn(\"data_entry_share\", 100 * F.col(\"data_entry_jobs\") / F.col(\"total_jobs\"))\n",
    "        .withColumn(\"database_share\", 100 * F.col(\"database_jobs\") / F.col(\"total_jobs\"))\n",
    "        .withColumn(\"data_analytics_share\", 100 * F.col(\"data_analytics_jobs\") / F.col(\"total_jobs\"))\n",
    "        .withColumn(\"total_data_share\", 100 * F.col(\"any_data_intensive_jobs\") / F.col(\"total_jobs\"))\n",
    "        .withColumn(\"year\", F.lit(int(yr)))\n",
    "    )\n",
    "\n",
    "    sec_base = ALL_PATHS[yr][\"sector_summary_sic\"]\n",
    "    if FORCE_RECOMPUTE:\n",
    "        sec_written = safe_write_parquet_s3a(sec, sec_base, create_version=True)\n",
    "    else:\n",
    "        try:\n",
    "            _df, latest = read_latest_version(sec_base)\n",
    "            print(f\"[SKIP] sector_summary_sic exists: {latest}\")\n",
    "            sec_written = latest\n",
    "        except Exception:\n",
    "            sec_written = safe_write_parquet_s3a(sec, sec_base, create_version=True)\n",
    "\n",
    "    print(f\"[SIC-SYNTH] Saved year {yr}: job_categories_sic={jc_sic_written} | sector_summary={sec_written}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34db541",
   "metadata": {},
   "source": [
    "CELL 9 — Step 5: Load outputs into pandas for visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520cb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "occ_frames = []\n",
    "sec_frames = []\n",
    "\n",
    "for yr in years_to_process:\n",
    "    occ_df, occ_path = read_latest_version(ALL_PATHS[yr][\"occupation_summary\"])\n",
    "    sec_df, sec_path = read_latest_version(ALL_PATHS[yr][\"sector_summary_sic\"])\n",
    "\n",
    "    occ_frames.append(occ_df.toPandas())\n",
    "    sec_frames.append(sec_df.toPandas())\n",
    "\n",
    "occupation_df = pd.concat(occ_frames, ignore_index=True)\n",
    "sector_df = pd.concat(sec_frames, ignore_index=True)\n",
    "\n",
    "occupation_df[\"year\"] = occupation_df[\"year\"].astype(int)\n",
    "sector_df[\"year\"] = sector_df[\"year\"].astype(int)\n",
    "\n",
    "print(\"Loaded occupation_df:\", occupation_df.shape)\n",
    "print(\"Loaded sector_df:\", sector_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce28cb3",
   "metadata": {},
   "source": [
    "CELL 10 — Step 6: Core plots (multi-year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a3ace",
   "metadata": {},
   "source": [
    "A) overall trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69635de",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly = (\n",
    "    occupation_df.groupby(\"year\")[[\"total_jobs\",\"any_data_intensive_jobs\"]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "yearly[\"data_intensive_share\"] = 100 * yearly[\"any_data_intensive_jobs\"] / yearly[\"total_jobs\"]\n",
    "\n",
    "fig = px.line(\n",
    "    yearly.sort_values(\"year\"),\n",
    "    x=\"year\",\n",
    "    y=\"data_intensive_share\",\n",
    "    markers=True,\n",
    "    title=\"Share of data-intensive job adverts over time\",\n",
    "    labels={\"year\":\"Year\", \"data_intensive_share\":\"Data-intensive jobs (%)\"}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f0ec4",
   "metadata": {},
   "source": [
    "B) SOC4 top occupations per year (facets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62605c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20\n",
    "top_each_year = (\n",
    "    occupation_df.sort_values([\"year\",\"total_data_share\"], ascending=[True,False])\n",
    "    .groupby(\"year\")\n",
    "    .head(top_n)\n",
    ")\n",
    "\n",
    "melted = top_each_year.melt(\n",
    "    id_vars=[\"year\",\"soc4\"],\n",
    "    value_vars=[\"data_entry_share\",\"database_share\",\"data_analytics_share\"],\n",
    "    var_name=\"category\",\n",
    "    value_name=\"share\"\n",
    ")\n",
    "\n",
    "label_map = {\n",
    "    \"data_entry_share\":\"Data entry\",\n",
    "    \"database_share\":\"Database\",\n",
    "    \"data_analytics_share\":\"Data analytics\"\n",
    "}\n",
    "melted[\"category_label\"] = melted[\"category\"].map(label_map)\n",
    "\n",
    "fig = px.bar(\n",
    "    melted,\n",
    "    x=\"soc4\",\n",
    "    y=\"share\",\n",
    "    color=\"category_label\",\n",
    "    facet_col=\"year\",\n",
    "    facet_col_wrap=2,\n",
    "    title=f\"Top {top_n} occupations by data intensity — each year (stacked components)\",\n",
    "    labels={\"soc4\":\"SOC (4-digit)\", \"share\":\"Share (%)\", \"category_label\":\"Layer\"},\n",
    "    height=900\n",
    ")\n",
    "fig.update_layout(barmode=\"stack\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a7c02",
   "metadata": {},
   "source": [
    "C) Synthetic SIC sector view (multi-year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64276a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    sector_df.sort_values([\"year\",\"SICSection_synth\"]),\n",
    "    x=\"year\",\n",
    "    y=\"total_data_share\",\n",
    "    color=\"SICSection_synth\",\n",
    "    markers=True,\n",
    "    title=\"Synthetic SIC Section — data-intensive share over time\",\n",
    "    labels={\"year\":\"Year\",\"total_data_share\":\"Data-intensive share (%)\",\"SICSection_synth\":\"Synthetic SIC Section\"}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfecc89d",
   "metadata": {},
   "source": [
    "CELL 11 — Step 7 (placeholder): SUT merge later (OECD “complete” step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0fb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "SUT merge placeholder ready.\n",
    "\n",
    "When you provide:\n",
    "- SUT dataset path (s3a/hdfs)\n",
    "- Columns for: Year, SICSection (or Activity mapped to SICSection), GVA, COMP_EMP (or equivalents)\n",
    "\n",
    "We will:\n",
    "1) Read SUT\n",
    "2) Clean keys to SICSection + Year\n",
    "3) Merge with sector_df (synthetic SIC section)\n",
    "4) Compute OECD-style valuation metrics:\n",
    "   - data_entry_invest = data_entry_share * COMP_EMP * alpha\n",
    "   - database_invest = ...\n",
    "   - data_science_invest = ...\n",
    "   - total_data_invest\n",
    "   - total_data_invest / GVA (data-as-asset share proxy)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
