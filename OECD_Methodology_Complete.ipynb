{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Complete Data‑Intensive Job Classification Pipeline\n",
    "\n",
    "This notebook implements the full methodology described in the OECD working paper for identifying and measuring data‑intensive jobs in online job postings.  \n",
    "\n",
    "The pipeline reads multi‑year job advertisement data, extracts noun chunks from the job descriptions using spaCy, assesses their similarity to the word *data*, applies dispersion and landmark‑occupation filters, assigns each qualifying noun chunk to one of three categories (data entry, databases, or analytics), then aggregates those classifications from the job level up to the occupation (SOC‑2020) and sector levels.  \n",
    "\n",
    "This notebook preserves the original multi‑year handling and atomic Parquet write logic from the provided template, but replaces the approximate landmark occupation matching based on job titles with precise matching on SOC‑2020 codes for the three landmark occupations.  \n",
    "\n",
    "Before running the notebook you should ensure that a column named `soc_2020` is present in your data and contains a 4‑ or 6‑digit SOC‑2020 code; the first four digits are used to determine membership in the three landmark occupations identified in the paper.  \n",
    "\n",
    "**Landmark occupations (SOC‑2020 codes):**\n",
    "\n",
    "* **4152** – Data entry administrators (data entry clerks)\n",
    "* **3133** – Database administrators and web content technicians\n",
    "* **2433** – Data scientists\n",
    "\n",
    "Jobs belonging to these three unit groups provide the reference set for deciding whether a noun chunk is genuinely about data and therefore should be classified as data entry, database or analytics. The paper uses these landmark occupations to build a relative frequency ratio (dispersion) that filters out generic terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Spark Setup and Helper Functions\n",
    "\n",
    "A single Spark session is created and configured to access your distributed storage (S3A in the provided template). A helper function, `safe_write_parquet`, ensures that Parquet writes are atomic and optionally backed up before overwriting existing data.  \n",
    "\n",
    "If you need to adjust memory or executor settings, modify the `SparkSession.builder` call accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "spark-setup-code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, count as F_count, sum as F_sum, max as F_max\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, IntegerType\n",
    "\n",
    "# Initialise a single SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"OECD_Data_Intensive_Jobs\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Ensure that the session is only created once in interactive environments\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Helper to validate that paths are rooted under the OECD data directory\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def validate_oecd_path(path: str, base: str = 's3a://em_sources/_SDD/OECD_DATA') -> None:\n",
    "    \"\"\"\n",
    "    Ensure that a path resides inside the OECD_DATA base directory.\n",
    "    Raises a ValueError if the path is outside of this directory.\n",
    "    \"\"\"\n",
    "    if not path.startswith(base):\n",
    "        raise ValueError(f'Path {path} is not within the allowed base directory {base}')\n",
    "\n",
    "\n",
    "def safe_write_parquet(df, output_path: str, backup_root: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Atomically writes a DataFrame to Parquet.\n",
    "    If a backup_root is provided and the output path exists, the existing data is moved to the backup prior to the write.\n",
    "    After a successful write, the temporary directory is renamed to the final output.\n",
    "    \"\"\"\n",
    "    validate_oecd_path(output_path)\n",
    "    import uuid, os\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "    tmp_path = f'{output_path}_tmp_{uuid.uuid4().hex}'\n",
    "    # Write to temp location\n",
    "    df.write.mode('overwrite').parquet(tmp_path)\n",
    "    # Optionally move existing data to backup\n",
    "    if backup_root:\n",
    "        validate_oecd_path(backup_root)\n",
    "        backup_path = f'{backup_root}/{uuid.uuid4().hex}'\n",
    "        try:\n",
    "            spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()).rename(output_path, backup_path)\n",
    "        except Exception:\n",
    "            pass  # If the destination does not exist, no backup needed\n",
    "    # Rename temp to final\n",
    "    spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()).rename(tmp_path, output_path)\n",
    "    # Verify the write\n",
    "    try:\n",
    "        _ = spark.read.parquet(output_path).take(1)\n",
    "    except AnalysisException as e:\n",
    "        raise RuntimeError(f'Failed to read back Parquet file at {output_path}: {e}')\n",
    "    \n",
    "    # Optionally clean up tmp if rename failed\n",
    "    try:\n",
    "        spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()).delete(tmp_path, True)\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-config",
   "metadata": {},
   "source": [
    "## 2. Data Path Configuration\n",
    "\n",
    "Specify the base S3A path for your OECD data, the years to process, and optionally the name of a unified Parquet file.  \n",
    "\n",
    "The multi‑year loader will read each year's CSV from `BASE_S3A_PATH/<year>/raw_data.csv` unless a `PARQUET_PATH` is provided, in which case the Parquet file is loaded and split by year.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "paths-and-years",
   "metadata": {},
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Root directory for OECD data (do not change to a location outside of OECD_DATA)\n",
    "BASE_S3A_PATH = 's3a://em_sources/_SDD/OECD_DATA'\n",
    "\n",
    "# List of years to process. Modify as needed.\n",
    "YEARS = [2021, 2022, 2023]\n",
    "\n",
    "# Path to a unified Parquet file; leave None to load from year‑partitioned CSVs\n",
    "PARQUET_PATH = None\n",
    "\n",
    "# Internal dictionary mapping each year to its CSV input path.\n",
    "ALL_PATHS = {\n",
    "    yr: {\n",
    "        'input_csv': f'{BASE_S3A_PATH}/{yr}/raw_data.csv',\n",
    "        'noun_chunks': f'{BASE_S3A_PATH}/{yr}/noun_chunks.parquet'\n",
    "    }\n",
    "    for yr in YEARS\n",
    "}\n",
    "\n",
    "# Validate paths for safety\n",
    "for yr in YEARS:\n",
    "    validate_oecd_path(ALL_PATHS[yr]['input_csv'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Load job advertisements either from a single Parquet file (`PARQUET_PATH`) or from CSV files partitioned by year. Only the columns relevant to the analysis are selected: `date`, `job_id`, `soc_2020`, `job_title`, and `full_text`. Rows with a null or empty `full_text` are filtered out.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "load-data-code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# Dictionary to hold DataFrames by year\n",
    "all_data = {}\n",
    "total_jobs = 0\n",
    "years_to_process = YEARS\n",
    "\n",
    "if PARQUET_PATH:\n",
    "    validate_oecd_path(PARQUET_PATH)\n",
    "    print(f'Loading unified Parquet dataset from {PARQUET_PATH}...')\n",
    "    full_df = spark.read.parquet(PARQUET_PATH)\n",
    "    # Ensure the date column is converted to date type for filtering\n",
    "    full_df = full_df.withColumn('date', F.to_date('date'))\n",
    "    for yr in years_to_process:\n",
    "        year_df = full_df.filter(F.year('date') == yr)\n",
    "        count = year_df.count()\n",
    "        total_jobs += count\n",
    "        all_data[yr] = year_df\n",
    "        print(f'Year {yr}: {count:,} records')\n",
    "else:\n",
    "    for yr in years_to_process:\n",
    "        print(f'Loading CSV data for year {yr}...')\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option('header', True)\n",
    "            .option('multiline', True)\n",
    "            .csv(ALL_PATHS[yr]['input_csv'])\n",
    "        )\n",
    "        df = (\n",
    "            df.select(\n",
    "                'date',\n",
    "                'job_id',\n",
    "                'soc_2020',\n",
    "                'job_title',\n",
    "                F.col('full_text').cast('string'),\n",
    "            )\n",
    "            .filter(F.col('full_text').isNotNull() & (F.length('full_text') > 0))\n",
    "        )\n",
    "        df = df.withColumn('date', F.to_date('date', 'yyyy-MM-dd'))\n",
    "        cnt = df.count()\n",
    "        total_jobs += cnt\n",
    "        all_data[yr] = df\n",
    "        print(f'\tRows read: {cnt:,}')\n",
    "print(f'Total jobs loaded: {total_jobs:,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nlp-processing",
   "metadata": {},
   "source": [
    "## 4. NLP Processing – Noun Chunk Extraction\n",
    "\n",
    "Using spaCy, noun phrases (noun chunks) are extracted from the `full_text` of each job advertisement. For each noun chunk, its cosine similarity to the token ‘data’ is computed.  \n",
    "\n",
    "We use a Pandas UDF to perform this operation in a distributed fashion. The UDF emits a row per noun chunk with the following fields:\n",
    "\n",
    "* `doc_date` – the date of the job posting\n",
    "* `doc_job_id` – the unique identifier of the job posting\n",
    "* `noun_chunk` – the text of the noun phrase (lower‑cased and stripped)\n",
    "* `similarity` – the cosine similarity between the noun chunk and the token ‘data’ (range 0–1)\n",
    "* `soc_code` – the SOC‑2020 code associated with the job posting\n",
    "\n",
    "> **Note:** spaCy’s `en_core_web_lg` model must be installed and available to the executor nodes. If you do not have internet access, install the model offline prior to running the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "noun-chunk-extraction-code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Load the large English model for similarity calculations\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "data_token = nlp('data')[0]  # token representing the word \"data\"\n",
    "\n",
    "# Define the schema for the UDF output\n",
    "noun_schema = StructType([\n",
    "    StructField('doc_date', StringType()),\n",
    "    StructField('doc_job_id', StringType()),\n",
    "    StructField('noun_chunk', StringType()),\n",
    "    StructField('similarity', DoubleType()),\n",
    "    StructField('soc_code', StringType()),\n",
    "])\n",
    "\n",
    "@pandas_udf(noun_schema)\n",
    "def extract_noun_chunks(\n",
    "    doc_dates: pd.Series,\n",
    "    doc_job_ids: pd.Series,\n",
    "    texts: pd.Series,\n",
    "    soc_codes: pd.Series\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each document, extract noun chunks and compute similarity to the token 'data'.\n",
    "    Returns one row per noun chunk.\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    for date, jid, text, soc in zip(doc_dates, doc_job_ids, texts, soc_codes):\n",
    "        if text is None or text.strip() == '':\n",
    "            continue\n",
    "        doc = nlp(text)\n",
    "        for chunk in doc.noun_chunks:\n",
    "            chunk_text = chunk.text.lower().strip()\n",
    "            # Avoid empty or punctuation only chunks\n",
    "            if not chunk_text or chunk_text.replace(' ', '').replace('\t', '') == '':\n",
    "                continue\n",
    "            # Compute similarity (handle zero vectors)\n",
    "            sim = 0.0\n",
    "            if chunk.vector_norm and data_token.vector_norm:\n",
    "                sim = float(chunk.similarity(data_token))\n",
    "            out_rows.append((str(date), str(jid), chunk_text, sim, str(soc) if soc is not None else None))\n",
    "    return pd.DataFrame(out_rows, columns=['doc_date', 'doc_job_id', 'noun_chunk', 'similarity', 'soc_code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "apply-nlp-code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# Parameters for similarity and dispersion\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "XC_THRESHOLD = 1.5  # relative frequency ratio threshold\n",
    "\n",
    "# Define the landmark SOC codes (four digits) used for reference occupations\n",
    "LANDMARK_SOC_CODES = ['4152', '3133', '2433']\n",
    "\n",
    "# Extract noun chunks for each year and write them to Parquet for intermediate storage\n",
    "noun_chunks_dfs = []\n",
    "for yr, df in all_data.items():\n",
    "    print(f'Extracting noun chunks for year {yr}...')\n",
    "    # Apply UDF; this produces many rows per job\n",
    "    chunks_df = df.select(\n",
    "        'date',\n",
    "        'job_id',\n",
    "        'full_text',\n",
    "        'soc_2020'\n",
    "    ).withColumnRenamed('date', 'doc_date').withColumnRenamed('job_id', 'doc_job_id')\n",
    "    chunks_df = chunks_df.withColumn('soc_2020', F.col('soc_2020'))\n",
    "    chunks_df = chunks_df.select('doc_date', 'doc_job_id', 'full_text', 'soc_2020')\n",
    "    chunks_df = chunks_df.applyInPandas(\n",
    "        lambda pdf: extract_noun_chunks(\n",
    "            pdf['doc_date'], pdf['doc_job_id'], pdf['full_text'], pdf['soc_2020']\n",
    "        ),\n",
    "        schema=noun_schema\n",
    "    )\n",
    "    # Persist intermediate noun chunks (optional)\n",
    "    noun_chunks_dfs.append(chunks_df)\n",
    "    # Save noun chunks for each year (optional)\n",
    "    # safe_write_parquet(chunks_df, ALL_PATHS[yr]['noun_chunks'])\n",
    "\n",
    "# Union all years' noun chunks\n",
    "noun_chunks_all = noun_chunks_dfs[0]\n",
    "for ndf in noun_chunks_dfs[1:]:\n",
    "    noun_chunks_all = noun_chunks_all.union(ndf)\n",
    "\n",
    "# Filter noun chunks by similarity threshold\n",
    "filtered_chunks = noun_chunks_all.filter(F.col('similarity') >= SIMILARITY_THRESHOLD)\n",
    "\n",
    "# Compute 4‑digit SOC code for each row\n",
    "filtered_chunks = filtered_chunks.withColumn(\n",
    "    'soc4', F.expr("substr(soc_code, 1, 4)")\n",
    ")\n",
    "\n",
    "# Determine whether each row comes from a landmark occupation\n",
    "filtered_chunks = filtered_chunks.withColumn(\n",
    "    'is_landmark', F.col('soc4').isin(*LANDMARK_SOC_CODES)\n",
    ")\n",
    "\n",
    "# Aggregate counts of each noun chunk within each category (data entry, database, analytics)\n",
    "# For each noun_chunk, we count occurrences in each landmark occupation separately\n",
    "category_counts = (\n",
    "    filtered_chunks.groupBy('noun_chunk').agg(\n",
    "        F_sum(F.when(F.col('soc4') == LANDMARK_SOC_CODES[0], 1).otherwise(0)).alias('data_entry_count'),\n",
    "        F_sum(F.when(F.col('soc4') == LANDMARK_SOC_CODES[1], 1).otherwise(0)).alias('database_count'),\n",
    "        F_sum(F.when(F.col('soc4') == LANDMARK_SOC_CODES[2], 1).otherwise(0)).alias('analytics_count'),\n",
    "        F_count('*').alias('total_count')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compute the dispersion ratio: total_count divided by the maximum count across categories\n",
    "category_counts = category_counts.withColumn(\n",
    "    'max_count', F.greatest('data_entry_count', 'database_count', 'analytics_count')\n",
    ").withColumn(\n",
    "    'xc_ratio', F.when(F.col('max_count') > 0, F.col('total_count') / F.col('max_count')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Keep only noun chunks with sufficient dispersion\n",
    "accepted_noun_chunks = category_counts.filter(F.col('xc_ratio') >= XC_THRESHOLD)\n",
    "\n",
    "# Assign each accepted noun chunk to the category with the maximum count\n",
    "accepted_noun_chunks = accepted_noun_chunks.withColumn(\n",
    "    'category',\n",
    "    F.when(F.col('data_entry_count') == F.col('max_count'), F.lit('data_entry')).when(\n",
    "        F.col('database_count') == F.col('max_count'), F.lit('database')).otherwise(\n",
    "        F.lit('analytics')\n",
    "    )\n",
    ")\n",
    "\n",
    "accepted_noun_chunks = accepted_noun_chunks.select(\n",
    "    'noun_chunk', 'category', 'data_entry_count', 'database_count', 'analytics_count', 'total_count', 'xc_ratio'\n",
    ")\n",
    "\n",
    "# Join accepted noun chunk categories back to filtered chunks\n",
    "labelled_chunks = filtered_chunks.join(accepted_noun_chunks, on='noun_chunk', how='inner')\n",
    "\n",
    "# At this point, labelled_chunks contains one row per occurrence of an accepted noun chunk\n",
    "# along with its category and similarity.\n",
    "\n",
    "# Group by job to compute per‑job counts for each category\n",
    "job_category_counts = (\n",
    "    labelled_chunks.groupBy('doc_job_id').agg(\n",
    "        F_sum(F.when(F.col('category') == 'data_entry', 1).otherwise(0)).alias('data_entry_count'),\n",
    "        F_sum(F.when(F.col('category') == 'database', 1).otherwise(0)).alias('database_count'),\n",
    "        F_sum(F.when(F.col('category') == 'analytics', 1).otherwise(0)).alias('analytics_count'),\n",
    "        F_count('*').alias('total_chunks')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Identify data‑intensive jobs: at least 3 accepted noun chunks across all categories\n",
    "job_category_counts = job_category_counts.withColumn(\n",
    "    'is_data_intensive', F.col('total_chunks') >= 3\n",
    ")\n",
    "\n",
    "# Compute category shares (relative contribution per job)\n",
    "job_category_counts = job_category_counts.withColumn(\n",
    "    'data_entry_share', F.when(F.col('total_chunks') > 0, F.col('data_entry_count') / F.col('total_chunks')).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'database_share', F.when(F.col('total_chunks') > 0, F.col('database_count') / F.col('total_chunks')).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'analytics_share', F.when(F.col('total_chunks') > 0, F.col('analytics_count') / F.col('total_chunks')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Bring back occupation and date information from the original data\n",
    "job_data = (\n",
    "    all_data[YEARS[0]]  # any year's schema contains the necessary columns\n",
    "    .select('job_id', 'soc_2020', 'job_title', 'date')\n",
    "    .withColumnRenamed('job_id', 'doc_job_id')\n",
    ")\n",
    "job_level_df = job_category_counts.join(job_data, on='doc_job_id', how='left')\n",
    "\n",
    "# Compute 4‑digit SOC code and month for aggregation\n",
    "job_level_df = job_level_df.withColumn('soc4', F.expr("substr(soc_2020, 1, 4)"))\n",
    "job_level_df = job_level_df.withColumn('year', F.year('date')).withColumn('month', F.month('date'))\n",
    "\n",
    "# Aggregate by occupation (SOC4)\n",
    "occ_results = (\n",
    "    job_level_df.groupBy('soc4').agg(\n",
    "        F_count('*').alias('num_jobs'),\n",
    "        F_sum(F.when(F.col('is_data_intensive'), 1).otherwise(0)).alias('num_data_intensive_jobs'),\n",
    "        F_sum('data_entry_share').alias('sum_data_entry_share'),\n",
    "        F_sum('database_share').alias('sum_database_share'),\n",
    "        F_sum('analytics_share').alias('sum_analytics_share')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compute mean shares and share of data‑intensive jobs per occupation\n",
    "occ_results = occ_results.withColumn(\n",
    "    'data_intensive_share', F.col('num_data_intensive_jobs') / F.col('num_jobs')\n",
    ").withColumn(\n",
    "    'mean_data_entry_share', F.col('sum_data_entry_share') / F.col('num_jobs')\n",
    ").withColumn(\n",
    "    'mean_database_share', F.col('sum_database_share') / F.col('num_jobs')\n",
    ").withColumn(\n",
    "    'mean_analytics_share', F.col('sum_analytics_share') / F.col('num_jobs')\n",
    ")\n",
    "\n",
    "occ_results = occ_results.select(\n",
    "    'soc4', 'num_jobs', 'num_data_intensive_jobs', 'data_intensive_share',\n",
    "    'mean_data_entry_share', 'mean_database_share', 'mean_analytics_share'\n",
    ")\n",
    "\n",
    "# Optionally aggregate by sector (if your data includes an ISIC code or sector identifier).\n",
    "# Uncomment and adapt the following section if a sector column exists (e.g. 'isic4').\n",
    "# if 'isic4' in all_data[YEARS[0]].columns:\n",
    "#     job_level_df = job_level_df.withColumn('isic4', F.col('isic4'))\n",
    "#     sector_results = (\n",
    "#         job_level_df.groupBy('isic4').agg(\n",
    "#             F_count('*').alias('num_jobs'),\n",
    "#             F_sum(F.when(F.col('is_data_intensive'), 1).otherwise(0)).alias('num_data_intensive_jobs'),\n",
    "#             F_sum('data_entry_share').alias('sum_data_entry_share'),\n",
    "#             F_sum('database_share').alias('sum_database_share'),\n",
    "#             F_sum('analytics_share').alias('sum_analytics_share')\n",
    "#         )\n",
    "#     ).withColumn(\n",
    "#         'data_intensive_share', F.col('num_data_intensive_jobs') / F.col('num_jobs')\n",
    "#     ).withColumn(\n",
    "#         'mean_data_entry_share', F.col('sum_data_entry_share') / F.col('num_jobs')\n",
    "#     ).withColumn(\n",
    "#         'mean_database_share', F.col('sum_database_share') / F.col('num_jobs')\n",
    "#     ).withColumn(\n",
    "#         'mean_analytics_share', F.col('sum_analytics_share') / F.col('num_jobs')\n",
    "#     )\n",
    "#     sector_results = sector_results.select(\n",
    "#         'isic4', 'num_jobs', 'num_data_intensive_jobs', 'data_intensive_share',\n",
    "#         'mean_data_entry_share', 'mean_database_share', 'mean_analytics_share'\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-saving",
   "metadata": {},
   "source": [
    "## 5. Saving Results\n",
    "\n",
    "Finally, write the occupation‑level results (and optional sector‑level results) back to Parquet using the `safe_write_parquet` function.  \n",
    "\n",
    "Update the output paths below to your desired location inside the OECD data hierarchy.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "save-results-code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# Define output locations inside the OECD data area\n",
    "OCC_OUTPUT_PATH = f'{BASE_S3A_PATH}/results/data_intensive_occupations.parquet'\n",
    "# SECTOR_OUTPUT_PATH = f'{BASE_S3A_PATH}/results/data_intensive_sectors.parquet'\n",
    "\n",
    "# Save occupation‑level results\n",
    "safe_write_parquet(occ_results, OCC_OUTPUT_PATH)\n",
    "print(f'Occupation‑level results written to {OCC_OUTPUT_PATH}')\n",
    "\n",
    "# If sector_results is defined, save it too\n",
    "# if 'sector_results' in locals():\n",
    "#     safe_write_parquet(sector_results, SECTOR_OUTPUT_PATH)\n",
    "#     print(f'Sector‑level results written to {SECTOR_OUTPUT_PATH}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook faithfully implements the OECD data‑intensive job classification methodology:\n",
    "\n",
    "1. **Extraction** – noun chunks are extracted from job descriptions using spaCy and filtered by their cosine similarity to ‘data’.\n",
    "2. **Dispersion filtering** – noun chunks are retained only if they are sufficiently dispersed across the three landmark occupations (data entry administrators, database administrators and web content technicians, and data scientists).\n",
    "3. **Classification** – each retained noun chunk is assigned to the category in which it appears most frequently.\n",
    "4. **Job‑level scoring** – jobs with at least three qualifying noun chunks are labelled data‑intensive; within these jobs, shares are computed for each category.\n",
    "5. **Aggregation** – shares are averaged across jobs within each 4‑digit SOC‑2020 occupation and (optionally) sector.\n",
    "\n",
    "The multi‑year handling and atomic write semantics mirror the original pipeline, while the landmark occupation identification now relies on SOC codes rather than approximate job title keywords.\n",
    "\n",
    "You can adjust the similarity threshold (`SIMILARITY_THRESHOLD`), dispersion threshold (`XC_THRESHOLD`), or the list of landmark SOC codes (`LANDMARK_SOC_CODES`) to fine‑tune the classification according to future research or data characteristics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}