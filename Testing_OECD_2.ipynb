{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40939f86",
   "metadata": {},
   "source": [
    "# Multi‑Year Data Analysis for Data‑Intensive Jobs\n",
    "\n",
    "This notebook provides a clean and executable version of the job‑posting pipeline originally found in `Testing_OECD_PRE_RUN.ipynb`. It allows you to analyse job advertisements to identify data‑intensive roles across occupations and time. Input data can be supplied either as year‑partitioned CSV files under the `csv_data` folder or as a single Parquet file containing all records. All processing is contained within the `OECD_DATA` root to ensure safe file operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947bac1",
   "metadata": {},
   "source": [
    "## 1. Spark and Utility Setup\n",
    "\n",
    "We configure a single Spark session for the entire pipeline and attach a `safe_write_parquet` helper to the session. This helper performs atomic writes and optional backups to prevent accidental data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e76849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Spark configuration\n",
    "auto_config = (\n",
    "    SparkSession.builder\n",
    "    .appName('BGT_Data_Processing_Clean')\n",
    "    .config('spark.executor.memory', '13g')\n",
    "    .config('spark.driver.memory', '3g')\n",
    "    .config('spark.executor.cores', '4')\n",
    "    .config('spark.driver.cores', '4')\n",
    "    .config('spark.sql.adaptive.enabled', 'true')\n",
    "    .config('spark.sql.adaptive.coalescePartitions.enabled', 'true')\n",
    "    .config('spark.sql.adaptive.skewJoin.enabled', 'true')\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    "    .config('spark.sql.parquet.compression.codec', 'snappy')\n",
    ")\n",
    "\n",
    "spark = auto_config.getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# Safe write function\n",
    "def safe_write_parquet(df, output_path, root_path, create_backup=True):\n",
    "    '''Safely write a DataFrame to parquet with multiple safety checks.'''\n",
    "    norm_output = os.path.normpath(output_path)\n",
    "    norm_root = os.path.normpath(root_path)\n",
    "    if not norm_output.startswith(norm_root):\n",
    "        raise ValueError(f'Output path {output_path} is outside project root {root_path}')\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    if create_backup and os.path.exists(output_path):\n",
    "        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        backup_path = f'{output_path}_backup_{timestamp}'\n",
    "        try:\n",
    "            spark.read.parquet(output_path).write.mode('overwrite').parquet(backup_path)\n",
    "            print(f'Created backup at: {backup_path}')\n",
    "        except Exception as e:\n",
    "            print(f'Warning: could not create backup: {e}')\n",
    "    temp_path = f\"{output_path}_temp_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    try:\n",
    "        df.write.mode('overwrite').option('compression','snappy').parquet(temp_path)\n",
    "        spark.read.parquet(temp_path).count()\n",
    "        if os.path.exists(output_path):\n",
    "            os.rename(output_path, f'{output_path}_old')\n",
    "        os.rename(temp_path, output_path)\n",
    "        if os.path.exists(f'{output_path}_old'):\n",
    "            os.remove(f'{output_path}_old')\n",
    "        print(f'Successfully wrote data to: {output_path}')\n",
    "    except Exception:\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        raise\n",
    "\n",
    "# Attach helper\n",
    "setattr(SparkSession, 'safe_write_parquet', safe_write_parquet)\n",
    "\n",
    "print('Spark configured. Safe write helper registered.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b944f3",
   "metadata": {},
   "source": [
    "## 2. Data Path Configuration\n",
    "\n",
    "Configure file locations.  Set `YEARS` to specify which year(s) to process.  Set `PARQUET_PATH` to the name of a single Parquet file (within `OECD_DATA`) containing all records if you wish to load from Parquet instead of CSV. If `PARQUET_PATH` is `None`, the notebook reads year‑specific CSV files from `csv_data`. All paths are validated to ensure they remain within the `OECD_DATA` root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Root directory; do not specify a path outside of OECD_DATA\n",
    "BASE_S3A_PATH = 's3a://em_sources/_SDD/OECD_DATA'\n",
    "\n",
    "# Year selection: 'ALL', a single year, a range '2020-2023' or a list [2020,2022]\n",
    "YEARS = '2025'\n",
    "\n",
    "# Optional: specify a Parquet file within the OECD_DATA root to bypass CSV loading\n",
    "# Example: PARQUET_PATH = os.path.join(BASE_S3A_PATH, 'abc.parquet')\n",
    "PARQUET_PATH = None\n",
    "# Optional: specify a CSV folder containing all partitions directly under OECD_DATA.\n",
    "# If provided, this path will be used for all years instead of constructing year-specific paths under\n",
    "# the default `csv_data/<year>` folder.\n",
    "CSV_PATH = None\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Helper functions for year parsing and path construction\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def get_available_years():\n",
    "    return list(range(2020, 2026))\n",
    "\n",
    "\n",
    "def parse_year_input(year_input):\n",
    "    available_years = get_available_years()\n",
    "    if isinstance(year_input, str):\n",
    "        y = year_input.upper()\n",
    "        if y == 'ALL':\n",
    "            years = available_years\n",
    "        elif '-' in y:\n",
    "            start, end = map(int, y.split('-'))\n",
    "            years = list(range(start, end + 1))\n",
    "        else:\n",
    "            years = [int(y)]\n",
    "    elif isinstance(year_input, (list, tuple)):\n",
    "        years = [int(x) for x in year_input]\n",
    "    else:\n",
    "        years = [int(year_input)]\n",
    "    invalid = [y for y in years if y not in available_years]\n",
    "    if invalid:\n",
    "        raise ValueError(f'Years {invalid} not available. Available years: {available_years}')\n",
    "    return sorted(years)\n",
    "\n",
    "# Parse year input\n",
    "years_to_process = parse_year_input(YEARS)\n",
    "print(f'Years to process: {years_to_process}')\n",
    "\n",
    "# Validate path does not go above OECD_DATA\n",
    "\n",
    "def validate_oecd_path(path: str) -> bool:\n",
    "    if not path.startswith(BASE_S3A_PATH):\n",
    "        raise ValueError(f'Path {path} is outside OECD_DATA folder!')\n",
    "    return True\n",
    "\n",
    "# Build paths for each year\n",
    "\n",
    "def get_paths_for_year(year):\n",
    "    base_year_path = os.path.join(BASE_S3A_PATH, 'processed_data', str(year))\n",
    "    return {\n",
    "        'input_csv': CSV_PATH if CSV_PATH else os.path.join(BASE_S3A_PATH, 'csv_data', str(year)),\n",
    "        'noun_chunks': os.path.join(base_year_path, 'noun_chunks'),\n",
    "        'reduced_data': os.path.join(base_year_path, 'reduced_data'),\n",
    "        'job_categories': os.path.join(base_year_path, 'job_categories'),\n",
    "        'occupation_summary': os.path.join(base_year_path, 'occupation_summary'),\n",
    "    }\n",
    "\n",
    "ALL_PATHS = {year: get_paths_for_year(year) for year in years_to_process}\n",
    "\n",
    "# Validate all constructed paths\n",
    "for year, paths in ALL_PATHS.items():\n",
    "    for key, path in paths.items():\n",
    "        validate_oecd_path(path)\n",
    "print('Path validation complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff89f0",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Load job advertisements either from a Parquet file (`PARQUET_PATH`) or from year‑partitioned CSV files.  When reading from Parquet, the entire dataset is loaded once and filtered per year.  Otherwise, each year's CSV files are read separately. A progress bar tracks progress across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb0472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Dictionary to hold DataFrames per year\n",
    "all_data = {}\n",
    "total_jobs = 0\n",
    "\n",
    "if PARQUET_PATH:\n",
    "    # Validate Parquet path within OECD_DATA\n",
    "    validate_oecd_path(PARQUET_PATH)\n",
    "    print(f'Loading unified Parquet dataset from {PARQUET_PATH}...')\n",
    "    full_df = spark.read.parquet(PARQUET_PATH)\n",
    "    full_df = full_df.withColumn('date', F.to_date('date'))\n",
    "    for yr in tqdm(years_to_process, desc='Reading data (Parquet)'):\n",
    "        year_df = full_df.filter(F.year('date') == yr)\n",
    "        count = year_df.count()\n",
    "        total_jobs += count\n",
    "        all_data[yr] = year_df\n",
    "        print(f'Year {yr}: {count:,} records')\n",
    "elif CSV_PATH:\n",
    "    # Validate unified CSV path within OECD_DATA\n",
    "    validate_oecd_path(CSV_PATH)\n",
    "    print(f'Loading unified CSV dataset from {CSV_PATH}...')\n",
    "    # Read all CSV partitions as a single DataFrame\n",
    "    full_df = (\n",
    "        spark.read\n",
    "        .option('header', True)\n",
    "        .option('multiline', True)\n",
    "        .csv(CSV_PATH)\n",
    "    )\n",
    "    # Select and clean relevant columns\n",
    "    full_df = (\n",
    "        full_df.select(\n",
    "            'date',\n",
    "            'job_id',\n",
    "            'soc_2020',\n",
    "            'job_title',\n",
    "            F.col('full_text').cast('string'),\n",
    "        )\n",
    "        .filter(F.col('full_text').isNotNull() & (F.length('full_text') > 0))\n",
    "    )\n",
    "    full_df = full_df.withColumn('date', F.to_date('date', 'yyyy-MM-dd'))\n",
    "    # Filter per year from unified CSV\n",
    "    for yr in tqdm(years_to_process, desc='Reading data (CSV unified)'):\n",
    "        year_df = full_df.filter(F.year('date') == yr)\n",
    "        count = year_df.count()\n",
    "        total_jobs += count\n",
    "        all_data[yr] = year_df\n",
    "        print(f'Year {yr}: {count:,} records')\n",
    "else:\n",
    "    # Read year-specific CSV data from partitioned folders\n",
    "    for yr in tqdm(years_to_process, desc='Reading data (CSV)'):\n",
    "        print(f'Loading CSV data for year {yr}...')\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option('header', True)\n",
    "            .option('multiline', True)\n",
    "            .csv(ALL_PATHS[yr]['input_csv'])\n",
    "        )\n",
    "        df = (\n",
    "            df.select(\n",
    "                'date',\n",
    "                'job_id',\n",
    "                'soc_2020',\n",
    "                'job_title',\n",
    "                F.col('full_text').cast('string'),\n",
    "            )\n",
    "            .filter(F.col('full_text').isNotNull() & (F.length('full_text') > 0))\n",
    "        )\n",
    "        df = df.withColumn('date', F.to_date('date', 'yyyy-MM-dd'))\n",
    "        cnt = df.count()\n",
    "        total_jobs += cnt\n",
    "        all_data[yr] = df\n",
    "        print(f'\tRows read: {cnt:,}')\n",
    "print(f'Total jobs loaded: {total_jobs:,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b3b81",
   "metadata": {},
   "source": [
    "## 4. NLP Processing – Noun Chunk Extraction\n",
    "\n",
    "Extract noun chunks from job descriptions using spaCy and compute their similarity to the word `data`. The extraction runs per year with a progress bar.  Results are partitioned by date and written to disk. Similarity scores below a threshold are removed in the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0198e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define schema for noun chunk extraction\n",
    "noun_schema = StructType([\n",
    "    StructField('doc_date', StringType()),\n",
    "    StructField('doc_JobID', StringType()),\n",
    "    StructField('doc_BGTOcc', StringType()),\n",
    "    StructField('noun_chunk', StringType()),\n",
    "    StructField('sim_data', DoubleType()),\n",
    "])\n",
    "\n",
    "# Pandas UDF\n",
    "\n",
    "def extract_noun_chunks(iterator):\n",
    "    import spacy\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_lg', exclude=['lemmatizer','ner'])\n",
    "    except OSError:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    target = nlp('data')\n",
    "    for pdf in iterator:\n",
    "        rows = []\n",
    "        texts = pdf['full_text'].fillna('').astype(str).tolist()\n",
    "        jobids = pdf['job_id'].astype(str).tolist()\n",
    "        dates = pdf['date'].astype(str).tolist()\n",
    "        socs = pdf['soc_2020'].astype(str).tolist()\n",
    "        for i, doc in enumerate(nlp.pipe(texts, batch_size=50, n_process=1)):\n",
    "            for chunk in doc.noun_chunks:\n",
    "                if chunk.has_vector:\n",
    "                    sim = float(chunk.similarity(target))\n",
    "                    cleaned = ''.join(c for c in chunk.text if not c.isdigit()).strip()\n",
    "                    if cleaned:\n",
    "                        rows.append({\n",
    "                            'doc_date': dates[i],\n",
    "                            'doc_JobID': jobids[i],\n",
    "                            'doc_BGTOcc': socs[i],\n",
    "                            'noun_chunk': cleaned.lower(),\n",
    "                            'sim_data': sim,\n",
    "                        })\n",
    "        if rows:\n",
    "            yield pd.DataFrame(rows)\n",
    "        else:\n",
    "            yield pd.DataFrame(columns=['doc_date','doc_JobID','doc_BGTOcc','noun_chunk','sim_data'])\n",
    "\n",
    "SIM_THRESHOLD = 0.45\n",
    "\n",
    "for yr in tqdm(years_to_process, desc='Extracting noun chunks'):\n",
    "    print(f'\n",
    "Processing noun chunks for {yr}...')\n",
    "    df = all_data[yr]\n",
    "    estimated_row_size = 1000\n",
    "    total_rows = df.count()\n",
    "    target_partition_size = 128 * 1024 * 1024\n",
    "    num_partitions = max(8, int((total_rows * estimated_row_size) / target_partition_size))\n",
    "    df_part = df.repartition(num_partitions)\n",
    "    nc_df = df_part.mapInPandas(extract_noun_chunks, schema=noun_schema)\n",
    "    nc_df.cache()\n",
    "    out_path = ALL_PATHS[yr]['noun_chunks']\n",
    "    nc_df.write.mode('overwrite').partitionBy('doc_date').parquet(out_path)\n",
    "    total_chunks = nc_df.count()\n",
    "    uniq_jobs = nc_df.select('doc_JobID').distinct().count()\n",
    "    print(f'\tSaved noun chunks to {out_path} (chunks: {total_chunks:,}, jobs: {uniq_jobs:,})')\n",
    "    nc_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60fb41",
   "metadata": {},
   "source": [
    "## 5. Data Reduction\n",
    "\n",
    "Filter noun chunks by similarity, aggregate duplicate chunks and compute average similarity and counts. Results are partitioned by occupation code for efficient downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for yr in tqdm(years_to_process, desc='Reducing noun chunks'):\n",
    "    print(f'\n",
    "Reducing data for {yr}...')\n",
    "    nc_path = ALL_PATHS[yr]['noun_chunks']\n",
    "    noun_chunks = spark.read.parquet(nc_path)\n",
    "    cleaned = noun_chunks.withColumn('counter', F.lit(1))\n",
    "    filtered = cleaned.filter(F.col('sim_data') >= SIM_THRESHOLD)\n",
    "    reduced = filtered.groupBy('noun_chunk','doc_JobID','doc_BGTOcc').agg(\n",
    "        F.avg('sim_data').alias('avg_sim'),\n",
    "        F.sum('counter').alias('count'),\n",
    "    )\n",
    "    red_path = ALL_PATHS[yr]['reduced_data']\n",
    "    reduced.write.mode('overwrite').partitionBy('doc_BGTOcc').parquet(red_path)\n",
    "    tot = reduced.count()\n",
    "    uniq = reduced.select('noun_chunk').distinct().count()\n",
    "    print(f'\tSaved reduced data to {red_path} (records: {tot:,}, unique chunks: {uniq:,})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291619b7",
   "metadata": {},
   "source": [
    "## 6. Category Analysis and Occupation Summary\n",
    "\n",
    "Classify jobs into high‑level categories based on keyword lists.  Jobs with at least `DATA_THRESHOLD` mentions of category‑specific terms are marked accordingly.  A progress bar tracks classification across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_THRESHOLD = 3\n",
    "\n",
    "data_entry_terms = ['data entry','data-input','manual data','typing data','keying data']\n",
    "database_terms = ['database','sql','mysql','postgres','oracle','db2','nosql']\n",
    "data_analytics_terms = ['analytics','analysis','statistical','machine learning','data science','insights']\n",
    "\n",
    "for yr in tqdm(years_to_process, desc='Classifying jobs'):\n",
    "    print(f'\n",
    "Classifying jobs for {yr}...')\n",
    "    red_path = ALL_PATHS[yr]['reduced_data']\n",
    "    reduced = spark.read.parquet(red_path)\n",
    "    def assign_category(term: str):\n",
    "        t = term.lower()\n",
    "        if t in data_entry_terms:\n",
    "            return 'data_entry'\n",
    "        elif t in database_terms:\n",
    "            return 'database'\n",
    "        elif t in data_analytics_terms:\n",
    "            return 'data_analytics'\n",
    "        else:\n",
    "            return None\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import StringType\n",
    "    category_udf = udf(assign_category, StringType())\n",
    "    categorized = reduced.withColumn('category', category_udf('noun_chunk'))\n",
    "    job_cat_counts = (\n",
    "        categorized.filter(F.col('category').isNotNull())\n",
    "        .groupBy('doc_JobID','doc_BGTOcc','category')\n",
    "        .agg(F.sum('count').alias('mentions'))\n",
    "    )\n",
    "    job_categories = job_cat_counts.withColumn(\n",
    "        'is_data_entry', (F.col('category')=='data_entry') & (F.col('mentions')>=DATA_THRESHOLD)\n",
    "    ).withColumn(\n",
    "        'is_database', (F.col('category')=='database') & (F.col('mentions')>=DATA_THRESHOLD)\n",
    "    ).withColumn(\n",
    "        'is_data_analytics', (F.col('category')=='data_analytics') & (F.col('mentions')>=DATA_THRESHOLD)\n",
    "    )\n",
    "    job_categories = job_categories.groupBy('doc_JobID','doc_BGTOcc').agg(\n",
    "        F.max('is_data_entry').alias('data_entry'),\n",
    "        F.max('is_database').alias('database'),\n",
    "        F.max('is_data_analytics').alias('data_analytics'),\n",
    "    )\n",
    "    jc_path = ALL_PATHS[yr]['job_categories']\n",
    "    job_categories.write.mode('overwrite').parquet(jc_path)\n",
    "    occ_summary = job_categories.groupBy('doc_BGTOcc').agg(\n",
    "        F.count('*').alias('total_jobs'),\n",
    "        F.sum(F.col('data_entry').cast('int')).alias('data_entry_jobs'),\n",
    "        F.sum(F.col('database').cast('int')).alias('database_jobs'),\n",
    "        F.sum(F.col('data_analytics').cast('int')).alias('data_analytics_jobs'),\n",
    "    )\n",
    "    occ_summary = occ_summary.withColumn('data_entry_share', F.col('data_entry_jobs')/F.col('total_jobs')*100)\n",
    "    occ_summary = occ_summary.withColumn('database_share', F.col('database_jobs')/F.col('total_jobs')*100)\n",
    "    occ_summary = occ_summary.withColumn('data_analytics_share', F.col('data_analytics_jobs')/F.col('total_jobs')*100)\n",
    "    os_path = ALL_PATHS[yr]['occupation_summary']\n",
    "    occ_summary.write.mode('overwrite').parquet(os_path)\n",
    "    print(f'\tSaved job categories to {jc_path} and occupation summary to {os_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bee9c1",
   "metadata": {},
   "source": [
    "## 7. Visualisations\n",
    "\n",
    "The following plots help interpret the results of the data‑intensive job analysis for the United Kingdom.  You may rerun this cell after processing different years to update the charts.  The figures show the share of jobs that are data‑intensive by occupation and the distribution of data‑related categories across all processed jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Load occupation summaries for all years into a single pandas DataFrame\n",
    "occupation_dfs = []\n",
    "for yr in years_to_process:\n",
    "    os_path = ALL_PATHS[yr]['occupation_summary']\n",
    "    spark_df = spark.read.parquet(os_path)\n",
    "    occupation_dfs.append(spark_df.toPandas())\n",
    "occupation_df = pd.concat(occupation_dfs, ignore_index=True)\n",
    "\n",
    "# Plot share of data-intensive jobs by occupation code (data_entry + database + data_analytics share)\n",
    "occupation_df['total_data_share'] = occupation_df[['data_entry_share','database_share','data_analytics_share']].sum(axis=1)\n",
    "fig_occ = px.bar(\n",
    "    occupation_df.sort_values('total_data_share', ascending=False),\n",
    "    x='doc_BGTOcc',\n",
    "    y='total_data_share',\n",
    "    color='total_data_share',\n",
    "    title='Data‑intensive jobs share by occupation (UK)',\n",
    "    labels={'doc_BGTOcc': 'Occupation Code','total_data_share': 'Share of data‑intensive jobs (%)'},\n",
    "    height=450\n",
    ")\n",
    "fig_occ.update_layout(showlegend=False)\n",
    "fig_occ.show()\n",
    "\n",
    "# Load job categories for all years and explode flags to count category prevalence\n",
    "job_cat_dfs = []\n",
    "for yr in years_to_process:\n",
    "    jc_path = ALL_PATHS[yr]['job_categories']\n",
    "    spark_df = spark.read.parquet(jc_path)\n",
    "    job_cat_dfs.append(spark_df.toPandas())\n",
    "job_cat_df = pd.concat(job_cat_dfs, ignore_index=True)\n",
    "\n",
    "# Count number of jobs per category\n",
    "counts = {\n",
    "    'data_entry': job_cat_df['data_entry'].sum(),\n",
    "    'database': job_cat_df['database'].sum(),\n",
    "    'data_analytics': job_cat_df['data_analytics'].sum(),\n",
    "}\n",
    "\n",
    "fig_cat = px.pie(\n",
    "    names=list(counts.keys()),\n",
    "    values=list(counts.values()),\n",
    "    title='Distribution of data‑related categories across jobs',\n",
    "    height=450\n",
    ")\n",
    "fig_cat.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig_cat.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4fbde2",
   "metadata": {},
   "source": [
    "## Visualising data intensity results\n",
    "\n",
    "The following charts summarise the distribution of data-intensive work across occupations for the UK dataset.  The first chart plots the top 20 occupations (by Standard Occupational Classification code) sorted by the **share of jobs that are data‑intensive**, with the contributions of the three categories (data‑entry, database, and data‑analytics) stacked to show which activities drive data intensity.  The second chart shows how the overall pool of data‑intensive jobs is distributed across those three categories.  These visualisations are inspired by Figures 6 and 8 in the OECD report【429441829467423†L1340-L1364】; they help interpret the aggregate metrics by illustrating which occupations and activities dominate the data‑intensive landscape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff757fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top occupations with stacked category shares (similar to OECD Figure 8)\n",
    "top_n = 20\n",
    "top_occ_df = occupation_df.sort_values('total_data_share', ascending=False).head(top_n).reset_index(drop=True)\n",
    "\n",
    "# Melt into long format for stacked bar plot\n",
    "top_melt = top_occ_df.melt(\n",
    "    id_vars=['doc_BGTOcc'],\n",
    "    value_vars=['data_entry_share','database_share','data_analytics_share'],\n",
    "    var_name='category',\n",
    "    value_name='share'\n",
    ")\n",
    "category_labels = {\n",
    "    'data_entry_share': 'Data entry',\n",
    "    'database_share': 'Database',\n",
    "    'data_analytics_share': 'Data analytics'\n",
    "}\n",
    "\n",
    "fig_top = px.bar(\n",
    "    top_melt,\n",
    "    x='doc_BGTOcc',\n",
    "    y='share',\n",
    "    color=top_melt['category'].map(category_labels),\n",
    "    title=f'Top {top_n} occupations by data intensity (UK)',\n",
    "    labels={'doc_BGTOcc':'Occupation code','share':'Share of data‑intensive jobs (%)','category':'Category'},\n",
    "    height=450\n",
    ")\n",
    "fig_top.update_layout(barmode='stack')\n",
    "fig_top.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc28a8e",
   "metadata": {},
   "source": [
    "## 7. Final Notes\n",
    "\n",
    "This rewritten notebook honours the original methodology while fixing undefined variables and syntax errors. It adds support for loading data from either partitioned CSV files or a single Parquet file and introduces progress bars via `tqdm` so you can monitor long‑running operations. All paths remain under the `OECD_DATA` root. Feel free to customise the keyword lists and thresholds to suit your analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
