# =========================================================
# DAP-SAFE NLP FEATURE EXTRACTION (DRIVER-ONLY, NO EXECUTORS)
# Goal: avoid executor death / RPC disconnects.
# Methodology: same as OECD-style you used:
#   - noun_chunks
#   - similarity to "data"
#   - SIM_THRESHOLD
#   - keep TOP_K chunks + avg similarity
#
# Tradeoff: slower, but *won't fail* on DAP because no spaCy runs on executors.
# =========================================================

import spacy
import pandas as pd
import gc
from tqdm import tqdm

from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType
)

# -----------------------------
# CONFIG
# -----------------------------
SIM_THRESHOLD = 0.55   # same as before
TOP_K = 10             # same as before
FORCE_RECOMPUTE = False

# Driver chunk size (tune if driver RAM low)
CHUNK_ROWS = 20000

# spaCy model (driver only)
print("[NLP-DRIVER] Loading spaCy on DRIVER...")
try:
    nlp = spacy.load("en_core_web_lg", exclude=["lemmatizer", "ner"])
except Exception:
    print("[NLP-DRIVER] Warning: en_core_web_lg not available, using sm")
    nlp = spacy.load("en_core_web_sm", exclude=["lemmatizer", "ner"])

target = nlp("data")

job_features_schema = StructType([
    StructField("date", StringType()),
    StructField("job_id", StringType()),
    StructField("soc_2020", StringType()),
    StructField("n_chunks_total", IntegerType()),
    StructField("n_chunks_data", IntegerType()),
    StructField("avg_sim_data", DoubleType()),
    StructField("top_chunks", ArrayType(StringType())),
    StructField("top_sims", ArrayType(DoubleType())),
    StructField("year", IntegerType()),
    StructField("month", IntegerType()),
])

def process_pdf_driver(pdf: pd.DataFrame) -> pd.DataFrame:
    pdf = pdf.copy()
    pdf["full_text"] = pdf["full_text"].fillna("").astype(str)
    texts = pdf["full_text"].tolist()

    docs = nlp.pipe(texts, batch_size=64, n_process=1)

    out = []
    for doc, job_id, soc, date in zip(docs, pdf["job_id"], pdf["soc_2020"], pdf["date"]):
        total_chunks = 0
        data_chunks = []

        for chunk in doc.noun_chunks:
            total_chunks += 1
            if chunk.has_vector:
                sim = float(chunk.similarity(target))
                if sim >= SIM_THRESHOLD:
                    cleaned = chunk.text.strip().lower()[:100]
                    data_chunks.append((cleaned, sim))

        n_data = len(data_chunks)
        if n_data > 0:
            data_chunks.sort(key=lambda x: x[1], reverse=True)
            top_k_items = data_chunks[:TOP_K]
            top_chunks = [t[0] for t in top_k_items]
            top_sims = [float(t[1]) for t in top_k_items]
            avg_sim = float(sum(s for _, s in data_chunks) / n_data)
        else:
            top_chunks, top_sims, avg_sim = [], [], 0.0

        # year/month derived from date string on driver (robust)
        # if date is malformed, set None
        try:
            dt = pd.to_datetime(date)
            yy = int(dt.year)
            mm = int(dt.month)
        except Exception:
            yy, mm = None, None

        out.append({
            "date": str(date),
            "job_id": str(job_id),
            "soc_2020": str(soc),
            "n_chunks_total": int(total_chunks),
            "n_chunks_data": int(n_data),
            "avg_sim_data": float(avg_sim),
            "top_chunks": top_chunks,
            "top_sims": top_sims,
            "year": yy,
            "month": mm
        })

    return pd.DataFrame(out)

print("\n[NLP-DRIVER] Starting driver-only extraction...")

for yr in tqdm(years_to_process, desc="NLP (driver-only)"):
    out_base = ALL_PATHS[yr]["job_features"]

    if not FORCE_RECOMPUTE:
        try:
            _df_existing, latest_path = read_latest_version(out_base)
            print(f"\n[SKIP] Year {yr} job_features already exist at: {latest_path}")
            continue
        except Exception:
            pass

    print(f"\n[NLP-DRIVER] Year {yr} starting...")

    df = all_data[yr].select("date", "job_id", "soc_2020", "full_text")

    total_rows = df.count()
    print(f"[NLP-DRIVER] Year {yr} rows={total_rows:,}")

    # Convert to a local iterator (streams partitions to driver)
    it = df.toLocalIterator()

    buffer = []
    out_paths = []

    produced_rows = 0
    batch_id = 0

    for row in it:
        buffer.append((row["date"], row["job_id"], row["soc_2020"], row["full_text"]))

        if len(buffer) >= CHUNK_ROWS:
            batch_id += 1
            pdf = pd.DataFrame(buffer, columns=["date","job_id","soc_2020","full_text"])
            out_pdf = process_pdf_driver(pdf)

            out_spark = spark.createDataFrame(out_pdf, schema=job_features_schema)

            # write incremental batches (append) to avoid huge driver RAM
            # first batch overwrite/create_version, then append to the same version folder is tricky,
            # so we write each batch as its own versioned write and keep the latest.
            # For simplicity: write OVERWRITE each batch into a temp path for the year, then one final union write.
            tmp_path = out_base + f"/_tmp_driver_batches/yr={yr}/batch={batch_id}"
            out_spark.write.mode("overwrite").parquet(tmp_path)
            out_paths.append(tmp_path)

            produced_rows += len(out_pdf)
            print(f"[NLP-DRIVER] Year {yr} wrote batch {batch_id} -> {tmp_path} (rows so far: {produced_rows:,})")

            buffer.clear()
            del pdf, out_pdf, out_spark
            gc.collect()

    # flush remaining
    if buffer:
        batch_id += 1
        pdf = pd.DataFrame(buffer, columns=["date","job_id","soc_2020","full_text"])
        out_pdf = process_pdf_driver(pdf)
        out_spark = spark.createDataFrame(out_pdf, schema=job_features_schema)

        tmp_path = out_base + f"/_tmp_driver_batches/yr={yr}/batch={batch_id}"
        out_spark.write.mode("overwrite").parquet(tmp_path)
        out_paths.append(tmp_path)

        produced_rows += len(out_pdf)
        print(f"[NLP-DRIVER] Year {yr} wrote final batch {batch_id} -> {tmp_path} (rows total: {produced_rows:,})")

        buffer.clear()
        del pdf, out_pdf, out_spark
        gc.collect()

    print(f"[NLP-DRIVER] Year {yr} merging {len(out_paths)} batch files -> versioned output...")

    merged = None
    for p in out_paths:
        part = spark.read.parquet(p)
        merged = part if merged is None else merged.unionByName(part)

    # final write (versioned) into the official out_base
    latest_written = safe_write_parquet_s3a(
        merged,
        out_base,
        mode="overwrite",
        create_version=True,
        verify_read=False
    )

    print(f"[NLP-DRIVER] Year {yr} DONE -> {latest_written}")

    del merged
    gc.collect()

print("\n[NLP-DRIVER] ALL YEARS COMPLETE.")
