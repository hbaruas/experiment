# =========================================================
# NLP JOB FEATURE EXTRACTION (DAP-OPTIMIZED)
# =========================================================

import spacy
import pandas as pd
import gc
from tqdm import tqdm

from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType
)

# -----------------------------
# USER TUNABLES (DAP-SAFE)
# -----------------------------
SIM_THRESHOLD = 0.55   
TOP_K = 10             
FORCE_RECOMPUTE = False

# Memory Management
# 16GB is tight; we increase partition size and decrease the cap to avoid overhead
ESTIMATED_ROW_SIZE_BYTES = 5000 
TARGET_PARTITION_BYTES = 256 * 1024 * 1024  # 256MB chunks
MIN_PARTITIONS = 16
MAX_PARTITIONS_CAP = 64  # Prevents spawning too many Python workers at once

# spaCy settings
SPACY_BATCH_SIZE = 25    # Reduced batch size to save RAM
SPACY_N_PROCESS = 1      
MAX_TEXT_CHARS = 50000   # Safety cap for extremely long job ads

# =========================================================
# 1) SINGLETON MODEL LOADER
# =========================================================
_SPACY_MODEL = None

def get_spacy_model():
    global _SPACY_MODEL
    if _SPACY_MODEL is None:
        print("Loading SpaCy model (md) on executor...")
        try:
            # 'md' has vectors but is much smaller than 'lg'
            _SPACY_MODEL = spacy.load("en_core_web_md", exclude=["ner", "lemmatizer"])
        except Exception:
            print("Warning: 'en_core_web_md' not found, falling back to 'sm'")
            _SPACY_MODEL = spacy.load("en_core_web_sm", exclude=["ner", "lemmatizer"])
    return _SPACY_MODEL

# =========================================================
# 2) OUTPUT SCHEMA
# =========================================================
job_features_schema = StructType([
    StructField("date", StringType()),
    StructField("job_id", StringType()),
    StructField("soc_2020", StringType()),
    StructField("n_chunks_total", IntegerType()),
    StructField("n_chunks_data", IntegerType()),
    StructField("avg_sim_data", DoubleType()),
    StructField("top_chunks", ArrayType(StringType())),
    StructField("top_sims", ArrayType(DoubleType())),
])

# =========================================================
# 3) WORKER FUNCTION
# =========================================================
def extract_job_features_oecd_style(iterator):
    nlp = get_spacy_model()
    target = nlp("data")  # Pre-vectorized "target"

    for pdf in iterator:
        # Fill missing columns and enforce string type
        for col in ["full_text", "job_id", "soc_2020", "date"]:
            if col not in pdf.columns:
                pdf[col] = ""
        
        # Trim text length to prevent OOM on massive single rows
        pdf["full_text"] = pdf["full_text"].fillna("").astype(str).str.slice(0, MAX_TEXT_CHARS)

        texts = pdf["full_text"].tolist()
        # nlp.pipe is efficient but we keep batch small for memory
        docs = nlp.pipe(texts, batch_size=SPACY_BATCH_SIZE, n_process=SPACY_N_PROCESS)

        out_rows = []
        for doc, job_id, soc, date in zip(docs, pdf["job_id"], pdf["soc_2020"], pdf["date"]):
            try:
                total_chunks = 0
                data_chunks = []

                # Requires 'parser' component (default in md/lg)
                for chunk in doc.noun_chunks:
                    total_chunks += 1
                    if chunk.has_vector:
                        sim = float(chunk.similarity(target))
                        if sim >= SIM_THRESHOLD:
                            cleaned = chunk.text.strip().lower()[:100]
                            data_chunks.append((cleaned, sim))

                n_data = len(data_chunks)
                if n_data > 0:
                    data_chunks.sort(key=lambda x: x[1], reverse=True)
                    top_k_items = data_chunks[:TOP_K]
                    top_chunks = [t[0] for t in top_k_items]
                    top_sims = [t[1] for t in top_k_items]
                    avg_sim = float(sum(s for _, s in data_chunks) / n_data)
                else:
                    top_chunks, top_sims, avg_sim = [], [], 0.0

                out_rows.append({
                    "date": str(date),
                    "job_id": str(job_id),
                    "soc_2020": str(soc),
                    "n_chunks_total": int(total_chunks),
                    "n_chunks_data": int(n_data),
                    "avg_sim_data": float(avg_sim),
                    "top_chunks": top_chunks,
                    "top_sims": top_sims,
                })
            except Exception:
                continue

        yield pd.DataFrame(out_rows)

# =========================================================
# 4) EXECUTION LOOP
# =========================================================

# Step 0: Set Spark stability configs to prevent "Connection Closed"
spark.conf.set("spark.network.timeout", "800s")
spark.conf.set("spark.executor.heartbeatInterval", "60s")

print("\n[NLP] Spark sanity:")
print("defaultParallelism:", spark.sparkContext.defaultParallelism)

for yr in tqdm(years_to_process, desc="NLP job feature extraction"):
    out_base = ALL_PATHS[yr]["job_features"]

    if not FORCE_RECOMPUTE:
        try:
            _df_existing, latest_path = read_latest_version(out_base)
            print(f"\n[SKIP] Year {yr} exists at: {latest_path}")
            continue
        except Exception:
            pass 

    print(f"\n[NLP] Year {yr} starting...")
    df = all_data[yr]
    
    needed = ["date", "job_id", "soc_2020", "full_text"]
    df = df.select(*[c for c in needed if c in df.columns])

    # Re-calculate partitions based on row count
    total_rows = df.count()
    num_partitions = max(
        MIN_PARTITIONS,
        int((total_rows * ESTIMATED_ROW_SIZE_BYTES) / TARGET_PARTITION_BYTES)
    )
    num_partitions = min(num_partitions, MAX_PARTITIONS_CAP)

    print(f"[NLP] Year {yr} rows={total_rows:,} -> partitions={num_partitions}")

    df_part = df.repartition(num_partitions)

    # Run mapInPandas
    job_feat = df_part.mapInPandas(extract_job_features_oecd_style, schema=job_features_schema)

    # Add time columns
    job_feat = (
        job_feat
        .withColumn("date_parsed", F.to_date(F.col("date")))
        .withColumn("year", F.year(F.col("date_parsed")))
        .withColumn("month", F.month(F.col("date_parsed")))
        .drop("date_parsed")
    )

    # Write Result
    latest_written = safe_write_parquet_s3a(
        job_feat,
        out_base,
        mode="overwrite",
        create_version=True,
        verify_read=False
    )

    print(f"[NLP] Year {yr} complete: {latest_written}")

    # Explicit memory cleanup
    del df_part
    del job_feat
    gc.collect()

print("\n[NLP] DONE â€” All years processed.")
