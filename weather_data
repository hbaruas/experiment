# =========================================================
# NLP JOB FEATURES (Stable DAP version, OECD-consistent)
# - No executor singleton model
# - No df.count() for partition sizing
# - Conservative batch sizes
# - Writes job-level features
# =========================================================

import gc
import pandas as pd
from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType
)
from tqdm.auto import tqdm

# -----------------------------
# Spark "stability-first" knobs
# -----------------------------
# (Safe to keep even if some keys are ignored in DAP)
try:
    spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
    spark.conf.set("spark.sql.adaptive.enabled", "true")
    spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
    spark.conf.set("spark.python.worker.reuse", "true")
    # keep shuffle partitions moderate (avoid explosion)
    spark.conf.set("spark.sql.shuffle.partitions", "200")
except Exception as e:
    print("[WARN] Could not set one or more Spark confs:", e)

print("\n[NLP] Spark sanity:")
print("  - defaultParallelism:", spark.sparkContext.defaultParallelism)
print("  - spark.sql.shuffle.partitions:", spark.conf.get("spark.sql.shuffle.partitions", "n/a"))
print("  - spark.sql.adaptive.enabled:", spark.conf.get("spark.sql.adaptive.enabled", "n/a"))
print("  - arrow enabled:", spark.conf.get("spark.sql.execution.arrow.pyspark.enabled", "n/a"))
print("  - python worker reuse:", spark.conf.get("spark.python.worker.reuse", "n/a"))

# -----------------------------
# Parameters (OECD-style)
# -----------------------------
SIM_THRESHOLD = 0.45   # keep aligned with your old notebook default
TOP_K = 7              # keep small to avoid output bloat

# -----------------------------
# Output schema (job-level)
# -----------------------------
job_features_schema = StructType([
    StructField("date", StringType()),
    StructField("job_id", StringType()),
    StructField("soc_2020", StringType()),
    StructField("n_chunks_total", IntegerType()),
    StructField("n_chunks_data", IntegerType()),
    StructField("avg_sim_data", DoubleType()),
    StructField("top_chunks", ArrayType(StringType())),
    StructField("top_sims", ArrayType(DoubleType())),
])

# -----------------------------
# mapInPandas worker function
# (Matches your old notebook style: load model INSIDE worker)
# -----------------------------
def extract_job_features_stable(iterator):
    import spacy
    import pandas as pd

    # Prefer md (lighter than lg but still has vectors for similarity)
    # Fall back to lg if md missing, then sm (but sm has NO vectors -> similarity meaningless)
    nlp = None
    for model_name in ["en_core_web_md", "en_core_web_lg", "en_core_web_sm"]:
        try:
            if model_name == "en_core_web_sm":
                nlp = spacy.load(model_name)  # sm fallback
            else:
                nlp = spacy.load(model_name, exclude=["lemmatizer", "ner"])
            chosen = model_name
            break
        except Exception:
            continue

    if nlp is None:
        # If this happens, spaCy models are not available on executors
        # Returning empty frames avoids hard crash, but you should fix the environment.
        for pdf in iterator:
            yield pd.DataFrame(columns=[f.name for f in job_features_schema.fields])
        return

    # A little safety for very long texts (prevents spaCy max_length errors)
    # Keep it modest (you can increase if you know texts are longer)
    try:
        nlp.max_length = 2_000_000
    except Exception:
        pass

    target = nlp("data")  # OECD-style semantic anchor

    for pdf in iterator:
        # Defensive: ensure required cols exist
        for c in ["full_text", "job_id", "soc_2020", "date"]:
            if c not in pdf.columns:
                # yield empty if schema mismatch
                yield pd.DataFrame(columns=[f.name for f in job_features_schema.fields])
                continue

        pdf["full_text"] = pdf["full_text"].fillna("").astype(str)

        texts = pdf["full_text"].tolist()
        job_ids = pdf["job_id"].astype(str).tolist()
        socs = pdf["soc_2020"].astype(str).tolist()
        dates = pdf["date"].astype(str).tolist()

        out_rows = []

        # conservative batch size; n_process=1 for Spark executors
        docs = nlp.pipe(texts, batch_size=20, n_process=1)

        for doc, jid, soc, dt in zip(docs, job_ids, socs, dates):
            total_chunks = 0
            data_chunks = []

            # If using sm, vectors often missing -> chunk.has_vector False
            for chunk in doc.noun_chunks:
                total_chunks += 1
                if chunk.has_vector:
                    sim = float(chunk.similarity(target))
                    if sim >= SIM_THRESHOLD:
                        cleaned = chunk.text.strip().lower()
                        # truncate aggressively to prevent huge arrays
                        cleaned = cleaned[:100]
                        if cleaned:
                            data_chunks.append((cleaned, sim))

            n_data = len(data_chunks)
            if n_data > 0:
                data_chunks.sort(key=lambda x: x[1], reverse=True)
                top_k = data_chunks[:TOP_K]
                top_chunks = [t for t, _ in top_k]
                top_sims = [s for _, s in top_k]
                avg_sim = float(sum(s for _, s in data_chunks) / n_data)
            else:
                top_chunks, top_sims, avg_sim = [], [], 0.0

            out_rows.append({
                "date": dt,
                "job_id": jid,
                "soc_2020": soc,
                "n_chunks_total": int(total_chunks),
                "n_chunks_data": int(n_data),
                "avg_sim_data": float(avg_sim),
                "top_chunks": top_chunks,
                "top_sims": top_sims,
            })

        yield pd.DataFrame(out_rows)

# -----------------------------
# Main loop
# -----------------------------
print("\n[NLP] Starting job_features extraction...")

for yr in tqdm(years_to_process, desc="NLP job feature extraction"):
    out_base = ALL_PATHS[yr]["job_features"]

    # Skip if exists (your helper)
    if not FORCE_RECOMPUTE:
        try:
            _df, latest_path = read_latest_version(out_base)
            print(f"\n[SKIP] Year {yr} job_features already exist at: {latest_path}")
            continue
        except Exception:
            pass

    print(f"\n[NLP] Year {yr} starting...")

    df = all_data[yr]

    # ---- Partitioning (no df.count(), no fancy heuristics) ----
    cur_parts = df.rdd.getNumPartitions()
    dp = spark.sparkContext.defaultParallelism or 40

    # Aim for "stable moderate" number of tasks:
    # - not tiny (causes huge partitions)
    # - not massive (causes executor churn)
    target_parts = max(40, min(4 * dp, 200))  # usually 160 if dp=40
    # Only repartition if meaningfully different
    if abs(cur_parts - target_parts) > 10:
        print(f"[NLP] partitions: current={cur_parts} -> target={target_parts} (repartition)")
        df_part = df.repartition(target_parts)
    else:
        print(f"[NLP] partitions: current={cur_parts} ~ target={target_parts} (no change)")
        df_part = df

    # ---- Compute ----
    job_feat = df_part.mapInPandas(extract_job_features_stable, schema=job_features_schema)

    # year/month columns for storage convenience
    job_feat = (
        job_feat
        .withColumn("year", F.year(F.to_date("date")))
        .withColumn("month", F.month(F.to_date("date")))
    )

    print("[NLP] job_feat columns:", job_feat.columns)
    print("[NLP] writing output...")

    latest_written = safe_write_parquet_s3a(
        job_feat,
        out_base,
        mode="overwrite",
        create_version=True,
        verify_read=False
    )

    print(f"[NLP] Year {yr} written to: {latest_written}")

    # ---- Driver cleanup ----
    try:
        job_feat.unpersist()
    except Exception:
        pass
    del df_part
    del job_feat
    gc.collect()

print("\n[NLP] DONE â€” job_features extraction completed.")
