# Spark side
print("defaultParallelism:", spark.sparkContext.defaultParallelism)
print("executor instances:", spark.sparkContext.getConf().get("spark.executor.instances"))
print("executor memory:", spark.sparkContext.getConf().get("spark.executor.memory"))
print("executor cores:", spark.sparkContext.getConf().get("spark.executor.cores"))
print("mem overhead:", spark.sparkContext.getConf().get("spark.executor.memoryOverhead"))
print("python worker reuse:", spark.sparkContext.getConf().get("spark.python.worker.reuse"))

# SpaCy side (driver check)
import spacy
print("spacy version:", spacy.__version__)
print("md installed:", spacy.util.is_package("en_core_web_md"))
print("lg installed:", spacy.util.is_package("en_core_web_lg"))


import spacy
import pandas as pd
import gc
from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType
)

# =========================================================
# Spark stability knobs (set once on driver)
# =========================================================
spark.conf.set("spark.python.worker.reuse", "true")
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")

# =========================================================
# Config
# =========================================================
TOP_K = 7
SIM_THRESHOLD = 0.35   # keep your original value if different
BATCH_SIZE = 32        # lower = less RAM spikes
MAX_CHARS = 5000       # hard cap per advert to prevent pathological texts

# =========================================================
# 1) Singleton model loader (per Python worker process)
# =========================================================
_SPACY_MODEL = None

def get_spacy_model():
    global _SPACY_MODEL
    if _SPACY_MODEL is None:
        # IMPORTANT: md is far safer than lg on clusters
        # Keep parser because noun_chunks needs it
        try:
            _SPACY_MODEL = spacy.load("en_core_web_md", exclude=["ner", "lemmatizer"])
        except OSError:
            # If md isn't available, fail loudly (sm vectors are not suitable for similarity)
            raise RuntimeError(
                "SpaCy model 'en_core_web_md' not found on executors. "
                "Install it on the cluster/image. Avoid falling back to 'sm' for vector similarity."
            )

        # Prevent extremely long texts from exploding memory
        _SPACY_MODEL.max_length = 2_000_000

    return _SPACY_MODEL

# =========================================================
# 2) Schema
# =========================================================
job_features_schema = StructType([
    StructField("date", StringType()),
    StructField("job_id", StringType()),
    StructField("soc_2020", StringType()),
    StructField("n_chunks_total", IntegerType()),
    StructField("n_chunks_data", IntegerType()),
    StructField("avg_sim_data", DoubleType()),
    StructField("top_chunks", ArrayType(StringType())),
    StructField("top_sims", ArrayType(DoubleType())),
])

# =========================================================
# 3) Worker function
# =========================================================
def extract_job_features_production(iterator):
    nlp = get_spacy_model()

    # Get target vector WITHOUT running the full pipeline
    target_vec = nlp.vocab["data"].vector

    for pdf in iterator:
        # Defensive: required columns
        for req in ["full_text", "job_id", "soc_2020", "date"]:
            if req not in pdf.columns:
                # return empty result for this batch
                yield pd.DataFrame([], columns=[f.name for f in job_features_schema.fields])
                continue

        # Clean + cap text length (critical to avoid huge memory spikes)
        texts = (
            pdf["full_text"]
            .fillna("")
            .astype(str)
            .str.slice(0, MAX_CHARS)
            .tolist()
        )

        # Parse in small batches; n_process=1 inside executors
        docs = nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=1)

        out = []
        for doc, job_id, soc, date in zip(docs, pdf["job_id"], pdf["soc_2020"], pdf["date"]):
            try:
                data_chunks = []
                total_chunks = 0

                # noun_chunks requires parser; expensive but matches your method
                for chunk in doc.noun_chunks:
                    total_chunks += 1

                    if chunk.has_vector and target_vec is not None and len(target_vec) > 0:
                        sim = float(chunk.vector.dot(target_vec) / (chunk.vector_norm * (target_vec**2).sum()**0.5 + 1e-12))
                        if sim >= SIM_THRESHOLD:
                            cleaned = chunk.text.strip().lower()[:100]
                            data_chunks.append((cleaned, sim))

                n_data = len(data_chunks)
                if n_data:
                    data_chunks.sort(key=lambda x: x[1], reverse=True)
                    top_k = data_chunks[:TOP_K]
                    top_chunks = [t for t, _ in top_k]
                    top_sims = [s for _, s in top_k]
                    avg_sim = sum(s for _, s in data_chunks) / n_data
                else:
                    top_chunks, top_sims, avg_sim = [], [], 0.0

                out.append({
                    "date": str(date),
                    "job_id": str(job_id),
                    "soc_2020": str(soc),
                    "n_chunks_total": int(total_chunks),
                    "n_chunks_data": int(n_data),
                    "avg_sim_data": float(avg_sim),
                    "top_chunks": top_chunks,
                    "top_sims": top_sims
                })
            except Exception:
                # swallow bad rows, don't kill partition
                continue

        yield pd.DataFrame(out)

# =========================================================
# 4) Execution loop
# =========================================================
for yr in tqdm(years_to_process, desc="NLP job feature extraction"):
    out_base = ALL_PATHS[yr]["job_features"]

    if not FORCE_RECOMPUTE:
        try:
            read_latest_version(out_base)
            print(f"\n[SKIP] Year {yr} job_features already exist.")
            continue
        except Exception:
            pass

    print(f"\n[NLP] Year {yr} starting...")

    df = all_data[yr]

    # CRITICAL: make tasks smaller than before for SpaCy parsing
    target_parts = max(2000, spark.sparkContext.defaultParallelism * 8)
    print(f"[NLP] Repartitioning to {target_parts} partitions")
    df_part = df.repartition(target_parts)

    job_feat = df_part.mapInPandas(extract_job_features_production, schema=job_features_schema)

    job_feat = (
        job_feat
        .withColumn("year", F.year(F.to_date("date")))
        .withColumn("month", F.month(F.to_date("date")))
    )

    latest_written = safe_write_parquet_s3a(
        job_feat,
        out_base,
        mode="overwrite",
        create_version=True,
        verify_read=False
    )
    print(f"[NLP] Year {yr} written to: {latest_written}")

    # Cleanup
    del df_part
    del job_feat
    gc.collect()
