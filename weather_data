import pandas as pd
import re

file_path = "/Users/saurabhkumar/Desktop/armaghdata.txt"
output_csv = "/Users/saurabhkumar/Desktop/armaghdata.csv"

clean_rows = []

with open(file_path, "r") as f:
    for line in f:
        # Keep only lines that start with a year

        if re.match(r"^\s*\d{4}", line):  # starts with year
            line = line.strip()
            parts = re.split(r'\s+', line)

            if len(parts) >= 7:
                # Optional: REMOVE footnotes like * or # or trailing text
                # -----------------------------------------
                #Uncomment the following block to clean footnotes:
                for i in range(2, 7):  # only clean columns tmax to sun
                    parts[i] = re.sub(r"[#*]", "", parts[i])
                if len(parts) > 7:  # If extra notes like "Provisional" exist
                    parts = parts[:7]
                # -----------------------------------------

                clean_rows.append(parts[:7])

df = pd.DataFrame(clean_rows, columns=["year", "month", "tmax", "tmin", "af", "rain", "sun"])
df.to_csv(output_csv, index=False)

print(f"✅ CSV saved to: {output_csv}")

Hi Matt,

I hope you’re well.

I wanted to share a quick update on the job classification work I’m doing, which follows the OECD methodology for grouping tasks within job descriptions into categories like data entry, data analytics, and database-related work. The method relies on identifying meaningful phrases in job ads and comparing them to example phrases using semantic similarity — essentially measuring how closely they relate in meaning.

I was able to build and test this successfully outside the DAP environment using spaCy with its en_core_web_md model, which provides access to high-quality word vectors — a key part of the OECD approach. While spaCy itself is available in DAP, unfortunately the en_core_web_md model is not installed, and due to DAP restrictions, I can’t download it directly.

As a workaround, I’m planning to rebuild the pipeline using NLTK with TF-IDF-based similarity. While this allows the process to run inside DAP, it comes with several important limitations that are worth highlighting:
	•	It can’t understand meaning — only word overlap.
For example, it will treat “spreadsheet” and “Excel file” as unrelated, even though they clearly refer to the same thing in a workplace context. This is because TF-IDF only sees the exact words used, not their meaning.
	•	It’s very sensitive to wording.
Slight changes like “processing customer data” vs “data from customers is processed” may confuse the algorithm, even though a human would see them as the same. The OECD method handles this well because of the semantic embedding layer.
	•	It misses the bigger picture.
For instance, “SQL database maintenance” would correctly match to the ‘database’ category using en_core_web_md, but the TF-IDF version might assign it incorrectly — or not assign it at all — if it hasn’t seen those exact words before.
	•	It struggles with abbreviations and acronyms.
“ETL process” (Extract, Transform, Load) might be common in job ads, but unless it exactly matches the reference text, TF-IDF won’t link it correctly. The OECD approach with vector models is more robust in these situations.
	•	It gives lower-quality classifications overall.
The OECD relies on vector-based models for a reason — they reflect the way people use language across different job sectors, which helps produce better insights and groupings.

So while I can continue with the fallback method for now, it does limit the quality and interpretability of the outputs. If there’s any way we can look into enabling en_core_web_md in DAP — either through pre-installation or making it available internally — it would make a big difference to aligning the work properly with OECD standards and reducing rework across environments.

Happy to provide more technical context or collaborate with the platform team if it helps.

Best regards,
Saurabh

