# =========================================================
# NLP JOB FEATURE EXTRACTION (DAP-SAFE, OECD-IDENTICAL LOGIC)
# =========================================================

import gc
import pandas as pd
import spacy

from tqdm import tqdm
from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType
)

# -----------------------------
# BEST-EFFORT Spark stability
# -----------------------------
def _safe_conf(k, v):
    try:
        spark.conf.set(k, v)
        print(f"[CONF] {k} = {v}")
    except Exception as e:
        print(f"[CONF-SKIP] {k}: {e}")

print("\n==============================")
print("[NLP] PRE-FLIGHT")
print("==============================")
print("[NLP] defaultParallelism:", spark.sparkContext.defaultParallelism)

_safe_conf("spark.sql.execution.arrow.pyspark.enabled", "true")
_safe_conf("spark.python.worker.reuse", "true")
_safe_conf("spark.network.timeout", "600s")
_safe_conf("spark.executor.heartbeatInterval", "60s")
_safe_conf("spark.rpc.askTimeout", "600s")
_safe_conf("spark.speculation", "false")

# -----------------------------
# OECD-ish knobs (keep yours)
# -----------------------------
# You already have these defined elsewhere — keep them if they exist.
# If not defined, these defaults are safe.
SIM_THRESHOLD = globals().get("SIM_THRESHOLD", 0.45)
TOP_K         = globals().get("TOP_K", 7)

# IMPORTANT: keep batch sizes conservative in DAP
BATCH_SIZE = 32

# Partitioning: DO NOT over-partition and DO NOT count() to size partitions.
# For 600k rows total, 40–120 partitions is usually stable.
TARGET_PARTITIONS = int(globals().get("TARGET_PARTITIONS", 80))

print(f"[NLP] SIM_THRESHOLD={SIM_THRESHOLD}, TOP_K={TOP_K}, BATCH_SIZE={BATCH_SIZE}, TARGET_PARTITIONS={TARGET_PARTITIONS}")

# =========================================================
# 1) SINGLETON MODEL LOADER (per executor)
# =========================================================
# DAP note: en_core_web_lg is frequently too heavy on executors.
# Prefer md; fallback to sm only if md missing.
_SPACY_MODEL = None

def get_spacy_model():
    global _SPACY_MODEL
    if _SPACY_MODEL is None:
        print("[NLP] Loading spaCy model on executor...")
        try:
            _SPACY_MODEL = spacy.load("en_core_web_md", exclude=["lemmatizer", "ner"])
        except Exception:
            print("[NLP] WARNING: en_core_web_md not found. Falling back to en_core_web_sm.")
            _SPACY_MODEL = spacy.load("en_core_web_sm", exclude=["lemmatizer", "ner"])
    return _SPACY_MODEL

# =========================================================
# 2) SCHEMA
# =========================================================
job_features_schema = StructType([
    StructField("date", StringType()),
    StructField("job_id", StringType()),
    StructField("soc_2020", StringType()),
    StructField("n_chunks_total", IntegerType()),
    StructField("n_chunks_data", IntegerType()),
    StructField("avg_sim_data", DoubleType()),
    StructField("top_chunks", ArrayType(StringType())),
    StructField("top_sims", ArrayType(DoubleType())),
])

# =========================================================
# 3) WORKER FUNCTION (mapInPandas)
# =========================================================
def extract_job_features(iterator):
    nlp = get_spacy_model()

    # Precompute target vector once per partition
    target = nlp("data")

    for pdf in iterator:
        # Defensive cleaning
        if "full_text" not in pdf.columns:
            yield pd.DataFrame([], columns=[f.name for f in job_features_schema.fields])
            continue

        pdf["full_text"] = pdf["full_text"].fillna("").astype(str)

        texts = pdf["full_text"].tolist()
        docs = nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=1)

        out = []
        for doc, job_id, soc, date in zip(docs, pdf["job_id"], pdf["soc_2020"], pdf["date"]):
            total_chunks = 0
            data_chunks = []  # (text, sim)

            try:
                for chunk in doc.noun_chunks:
                    total_chunks += 1
                    if chunk.has_vector:
                        sim = float(chunk.similarity(target))
                        if sim >= SIM_THRESHOLD:
                            cleaned = chunk.text.strip().lower()[:100]
                            data_chunks.append((cleaned, sim))

                n_data = len(data_chunks)
                if n_data:
                    data_chunks.sort(key=lambda x: x[1], reverse=True)
                    top_k = data_chunks[:TOP_K]
                    top_chunks = [t for t, _ in top_k]
                    top_sims   = [s for _, s in top_k]
                    avg_sim    = float(sum(s for _, s in data_chunks) / n_data)
                else:
                    top_chunks, top_sims, avg_sim = [], [], 0.0

                out.append({
                    "date": str(date),
                    "job_id": str(job_id),
                    "soc_2020": str(soc),
                    "n_chunks_total": int(total_chunks),
                    "n_chunks_data": int(n_data),
                    "avg_sim_data": float(avg_sim),
                    "top_chunks": top_chunks,
                    "top_sims": top_sims,
                })
            except Exception:
                # skip bad row; do not fail executor
                continue

        yield pd.DataFrame(out)

# =========================================================
# 4) EXECUTION LOOP (DAP-safe)
# =========================================================
print("\n==============================")
print("[NLP] START EXTRACTION")
print("==============================")

for yr in tqdm(years_to_process, desc="NLP job feature extraction"):
    out_base = ALL_PATHS[yr]["job_features"]

    # Skip if exists (your existing skip pattern)
    if not FORCE_RECOMPUTE:
        try:
            _df, latest_path = read_latest_version(out_base)
            print(f"\n[SKIP] Year {yr} job_features already exist at: {latest_path}")
            continue
        except Exception:
            pass

    print(f"\n[NLP] Year {yr} starting...")
    df = all_data[yr]

    # Make partitions stable and modest
    # (Avoid df.count() and avoid repartition(600) unless you truly need it)
    df_part = df.repartition(TARGET_PARTITIONS)

    # Run NLP
    job_feat = df_part.mapInPandas(extract_job_features, schema=job_features_schema)

    # Add year/month cols for writing
    job_feat = (
        job_feat
        .withColumn("year", F.year(F.to_date("date")))
        .withColumn("month", F.month(F.to_date("date")))
    )

    print(f"[NLP] Writing year {yr} to: {out_base}")
    latest_written = safe_write_parquet_s3a(
        job_feat,
        out_base,
        mode="overwrite",
        create_version=True,
        verify_read=False
    )
    print(f"[NLP] Year {yr} written to: {latest_written}")

    # Cleanup (driver-side)
    try:
        job_feat.unpersist()
    except Exception:
        pass

    del df_part
    del job_feat
    gc.collect()

print("\n[NLP] DONE — job feature extraction complete.")
