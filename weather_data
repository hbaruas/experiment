from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import os

# ---- Spark (can be lighter than your big compute job) ----
spark = (
    SparkSession.builder
    .appName("DataIntensity_Visualisation")
    .config("spark.sql.execution.arrow.pyspark.enabled", "true")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("ERROR")

print("Spark ready for viz-only work.")

# ---- Years & path config (re-use same BASE_S3A_PATH and scheme) ----
BASE_S3A_PATH = "s3a://em_sources/_SDD/OECD_DATA"
YEARS = "2021-2025"  # adjust to whatever years you actually ran at 1%

def get_available_years():
    # adapt to what you actually have processed
    return list(range(2021, 2026))

def parse_year_input(year_input):
    available_years = get_available_years()
    if isinstance(year_input, str):
        y = year_input.upper()
        if y == "ALL":
            years = available_years
        elif "-" in y:
            start, end = map(int, y.split("-"))
            years = list(range(start, end + 1))
        else:
            years = [int(y)]
    elif isinstance(year_input, (list, tuple)):
        years = [int(x) for x in year_input]
    else:
        years = [int(year_input)]
    invalid = [y for y in years if y not in available_years]
    if invalid:
        raise ValueError(f"Years {invalid} not available. Available: {available_years}")
    return sorted(years)

years_to_process = parse_year_input(YEARS)
print("Years to use in viz:", years_to_process)

def get_paths_for_year(year):
    base_year_path = os.path.join(BASE_S3A_PATH, "processed_data", str(year))
    return {
        "job_categories":      os.path.join(base_year_path, "job_categories"),
        "occupation_summary":  os.path.join(base_year_path, "occupation_summary"),
    }

ALL_PATHS = {yr: get_paths_for_year(yr) for yr in years_to_process}




import pandas as pd

occupation_dfs = []
job_dfs = []

for yr in years_to_process:
    occ_path = ALL_PATHS[yr]["occupation_summary"]
    jc_path  = ALL_PATHS[yr]["job_categories"]

    print(f"Reading occupation summary for {yr} from {occ_path}")
    occ_df_spark = spark.read.parquet(occ_path).withColumn("year", F.lit(yr))
    occupation_dfs.append(occ_df_spark.toPandas())

    print(f"Reading job categories for {yr} from {jc_path}")
    jc_df_spark = spark.read.parquet(jc_path).withColumn("year", F.lit(yr))
    job_dfs.append(jc_df_spark.toPandas())

occupation_df = pd.concat(occupation_dfs, ignore_index=True)
job_df        = pd.concat(job_dfs, ignore_index=True)

print("occupation_df columns:", occupation_df.columns.tolist())
print("job_df columns:", job_df.columns.tolist())







# Total data intensity at occupation level
occupation_df["total_data_share"] = (
    occupation_df["data_entry_share"]
    + occupation_df["database_share"]
    + occupation_df["data_analytics_share"]
)

# At job level, define a "data-intensive job" flag
job_df["is_data_intensive"] = (
    job_df[["data_entry", "database", "data_analytics"]].any(axis=1)
)





import plotly.express as px

econ_year = (
    job_df
    .groupby("year", as_index=False)
    .agg(
        total_jobs=("doc_JobID", "count"),
        data_intensive_jobs=("is_data_intensive", "sum"),
    )
)
econ_year["share_data_intensive"] = (
    econ_year["data_intensive_jobs"] / econ_year["total_jobs"] * 100
)

fig_econ = px.line(
    econ_year.sort_values("year"),
    x="year",
    y="share_data_intensive",
    markers=True,
    title="Share of data-intensive job adverts over time (1% sample)",
    labels={"year": "Year", "share_data_intensive": "Data-intensive jobs (%)"},
    height=450,
)
fig_econ.show()





latest_year = occupation_df["year"].max()
latest_occ = occupation_df[occupation_df["year"] == latest_year].copy()

top_n = 20
top_latest = (
    latest_occ
    .sort_values("total_data_share", ascending=False)
    .head(top_n)
    .reset_index(drop=True)
)

top_melt = top_latest.melt(
    id_vars=["doc_BGTOcc"],
    value_vars=["data_entry_share", "database_share", "data_analytics_share"],
    var_name="category",
    value_name="share",
)

category_labels = {
    "data_entry_share": "Data entry",
    "database_share": "Database",
    "data_analytics_share": "Data analytics",
}
top_melt["category_label"] = top_melt["category"].map(category_labels)

fig_top = px.bar(
    top_melt,
    x="doc_BGTOcc",
    y="share",
    color="category_label",
    title=f"Top {top_n} occupations by data intensity in {latest_year} (1% sample)",
    labels={"doc_BGTOcc": "SOC code", "share": "Share of data-intensive jobs (%)", "category_label": "Layer"},
    height=500,
)
fig_top.update_layout(barmode="stack")
fig_top.show()







def weighted_mean(group, col):
    return (group[col] * group["total_jobs"]).sum() / group["total_jobs"].sum()

agg_rows = []
for yr, g in occupation_df.groupby("year"):
    agg_rows.append({
        "year": yr,
        "data_entry_share":      weighted_mean(g, "data_entry_share"),
        "database_share":        weighted_mean(g, "database_share"),
        "data_analytics_share":  weighted_mean(g, "data_analytics_share"),
    })

occ_year = pd.DataFrame(agg_rows)

occ_year_melt = occ_year.melt(
    id_vars=["year"],
    value_vars=["data_entry_share", "database_share", "data_analytics_share"],
    var_name="category",
    value_name="share",
)
occ_year_melt["category_label"] = occ_year_melt["category"].map(category_labels)

fig_layers = px.area(
    occ_year_melt.sort_values("year"),
    x="year",
    y="share",
    color="category_label",
    title="Evolution of data-related work (entry / database / analytics)",
    labels={"year": "Year", "share": "Avg share in data-intensive jobs (%)", "category_label": "Layer"},
    height=450,
)
fig_layers.show()







top_codes = top_latest["doc_BGTOcc"].unique()
heat_df = occupation_df[occupation_df["doc_BGTOcc"].isin(top_codes)].copy()

pivot = heat_df.pivot_table(
    index="doc_BGTOcc",
    columns="year",
    values="total_data_share",
    aggfunc="mean",
)

fig_heat = px.imshow(
    pivot.sort_index(),
    aspect="auto",
    labels=dict(x="Year", y="SOC code", color="Total data intensity (%)"),
    title=f"Data intensity over time for top {top_n} occupations (1% sample)",
    height=600,
)
fig_heat.show()








target_soc = str(top_codes[0])   # or e.g. '2135' etc.

soc_trend = occupation_df[occupation_df["doc_BGTOcc"] == target_soc].copy()
soc_melt = soc_trend.melt(
    id_vars=["year"],
    value_vars=["data_entry_share", "database_share", "data_analytics_share"],
    var_name="category",
    value_name="share",
)
soc_melt["category_label"] = soc_melt["category"].map(category_labels)

fig_soc = px.line(
    soc_melt.sort_values("year"),
    x="year",
    y="share",
    color="category_label",
    markers=True,
    title=f"Data-layer mix over time for SOC {target_soc}",
    labels={"year": "Year", "share": "Share in data-intensive jobs (%)", "category_label": "Layer"},
    height=450,
)
fig_soc.show()



