# =========================================================
# NLP JOB FEATURE EXTRACTION (DAP-SAFE) — OLD PARTITION LOGIC
# (Count rows -> estimate bytes -> target 128MB partitions)
# Methodology: identical extraction logic; partitioning matches your old screenshot.
# =========================================================

import spacy
import pandas as pd
import gc
from tqdm import tqdm

from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType
)

# -----------------------------
# USER TUNABLES (KEEP SIMPLE)
# -----------------------------
SIM_THRESHOLD = 0.55   # <-- your threshold
TOP_K = 10             # <-- top chunks to store
FORCE_RECOMPUTE = False

# Partitioning (matches your old approach)
ESTIMATED_ROW_SIZE_BYTES = 1000
TARGET_PARTITION_BYTES = 128 * 1024 * 1024  # 128MB
MIN_PARTITIONS = 16
MAX_PARTITIONS_CAP = 200   # IMPORTANT on DAP to prevent too many Python workers

# spaCy batch
SPACY_BATCH_SIZE = 50
SPACY_N_PROCESS = 1        # keep 1 inside executors

# =========================================================
# 1) SINGLETON MODEL LOADER (per executor)
# =========================================================
_SPACY_MODEL = None

def get_spacy_model():
    global _SPACY_MODEL
    if _SPACY_MODEL is None:
        print("Loading SpaCy model on executor...")
        try:
            _SPACY_MODEL = spacy.load("en_core_web_lg", exclude=["lemmatizer", "ner"])
        except Exception:
            print("Warning: 'en_core_web_lg' not found, falling back to 'sm'")
            _SPACY_MODEL = spacy.load("en_core_web_sm", exclude=["lemmatizer", "ner"])
    return _SPACY_MODEL

# =========================================================
# 2) OUTPUT SCHEMA
# =========================================================
job_features_schema = StructType([
    StructField("date", StringType()),
    StructField("job_id", StringType()),
    StructField("soc_2020", StringType()),
    StructField("n_chunks_total", IntegerType()),
    StructField("n_chunks_data", IntegerType()),
    StructField("avg_sim_data", DoubleType()),
    StructField("top_chunks", ArrayType(StringType())),
    StructField("top_sims", ArrayType(DoubleType())),
])

# =========================================================
# 3) WORKER FUNCTION (mapInPandas)
# =========================================================
def extract_job_features_oecd_style(iterator):
    nlp = get_spacy_model()
    target = nlp("data")  # vector once per executor

    for pdf in iterator:
        # Defensive: ensure required columns exist
        for col in ["full_text", "job_id", "soc_2020", "date"]:
            if col not in pdf.columns:
                pdf[col] = None

        pdf["full_text"] = pdf["full_text"].fillna("").astype(str)

        texts = pdf["full_text"].tolist()
        docs = nlp.pipe(texts, batch_size=SPACY_BATCH_SIZE, n_process=SPACY_N_PROCESS)

        out_rows = []
        for doc, job_id, soc, date in zip(docs, pdf["job_id"], pdf["soc_2020"], pdf["date"]):
            try:
                total_chunks = 0
                data_chunks = []

                for chunk in doc.noun_chunks:
                    total_chunks += 1
                    if chunk.has_vector:
                        sim = float(chunk.similarity(target))
                        if sim >= SIM_THRESHOLD:
                            cleaned = chunk.text.strip().lower()[:100]
                            data_chunks.append((cleaned, sim))

                n_data = len(data_chunks)
                if n_data > 0:
                    data_chunks.sort(key=lambda x: x[1], reverse=True)
                    top_k_items = data_chunks[:TOP_K]
                    top_chunks = [t[0] for t in top_k_items]
                    top_sims = [t[1] for t in top_k_items]
                    avg_sim = float(sum(s for _, s in data_chunks) / n_data)
                else:
                    top_chunks, top_sims, avg_sim = [], [], 0.0

                out_rows.append({
                    "date": str(date),
                    "job_id": str(job_id),
                    "soc_2020": str(soc),
                    "n_chunks_total": int(total_chunks),
                    "n_chunks_data": int(n_data),
                    "avg_sim_data": float(avg_sim),
                    "top_chunks": top_chunks,
                    "top_sims": top_sims,
                })
            except Exception:
                # Do not kill partition on one bad row
                continue

        yield pd.DataFrame(out_rows)

# =========================================================
# 4) EXECUTION LOOP (OLD PARTITION LOGIC)
# Requires: years_to_process, ALL_PATHS, all_data, read_latest_version, safe_write_parquet_s3a
# =========================================================

print("\n[NLP] Spark sanity:")
print("defaultParallelism:", spark.sparkContext.defaultParallelism)

for yr in tqdm(years_to_process, desc="NLP job feature extraction"):
    out_base = ALL_PATHS[yr]["job_features"]

    # Skip if already exists (unless FORCE_RECOMPUTE)
    if not FORCE_RECOMPUTE:
        try:
            _df_existing, latest_path = read_latest_version(out_base)
            print(f"\n[SKIP] Year {yr} job_features already exist at: {latest_path}")
            continue
        except Exception:
            pass  # not found -> compute

    print(f"\n[NLP] Year {yr} starting...")

    df = all_data[yr]

    # Keep only columns needed (reduces shuffle + memory)
    needed = ["date", "job_id", "soc_2020", "full_text"]
    df = df.select(*[c for c in needed if c in df.columns])

    # ---- OLD STYLE PARTITION ESTIMATION (YOUR SCREENSHOT) ----
    print("[NLP] Counting rows to size partitions (this triggers a Spark job)...")
    total_rows = df.count()

    num_partitions = max(
        MIN_PARTITIONS,
        int((total_rows * ESTIMATED_ROW_SIZE_BYTES) / TARGET_PARTITION_BYTES)
    )
    num_partitions = min(num_partitions, MAX_PARTITIONS_CAP)

    print(f"[NLP] Year {yr} rows={total_rows:,} -> partitions={num_partitions}")

    df_part = df.repartition(num_partitions)

    # Run NLP
    job_feat = df_part.mapInPandas(extract_job_features_oecd_style, schema=job_features_schema)

    # Add year/month from date
    job_feat = (
        job_feat
        .withColumn("date_parsed", F.to_date(F.col("date")))
        .withColumn("year", F.year(F.col("date_parsed")))
        .withColumn("month", F.month(F.col("date_parsed")))
        .drop("date_parsed")
    )

    print("[NLP] job_feat columns:", job_feat.columns)
    print("[NLP] job_feat sample:")
    job_feat.limit(5).show(truncate=False)

    # Write (verify_read=False avoids expensive read-back)
    latest_written = safe_write_parquet_s3a(
        job_feat,
        out_base,
        mode="overwrite",
        create_version=True,
        verify_read=False
    )

    print(f"[NLP] Year {yr} written to: {latest_written}")

    # Cleanup (driver-side)
    del df_part
    del job_feat
    gc.collect()

print("\n[NLP] DONE — job feature extraction completed.")
