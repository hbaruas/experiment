# =========================================================
# NLP JOB FEATURE EXTRACTION (DAP-SAFE, OECD-STYLE)
# - Uses sentence "chunks" + semantic similarity to the concept "data"
# - Avoids spaCy parser / noun_chunks (too heavy for Spark at scale)
# - Throttles Spark concurrency to prevent executor deaths / RPC disconnects
# =========================================================

import gc
import pandas as pd
import spacy

from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType
)

# -----------------------------
# CONFIG (tune these safely)
# -----------------------------
SIM_THRESHOLD = 0.35   # start ~0.35; tune later (OECD-style intensity threshold)
TOP_K = 7              # keep top K chunks (truncate to avoid bloat)
BATCH_SIZE = 64        # inside executor; keep moderate
MAX_CHARS_PER_CHUNK = 160  # truncate chunk text for storage safety

# Concurrency control (THIS prevents the DAP RPC crash)
DEFAULT_PAR = int(spark.sparkContext.defaultParallelism)
# Safe partitions for heavy Python NLP: 1x to 2x default parallelism
N_PARTITIONS = max(20, min(DEFAULT_PAR * 2, 120))

print("[NLP] defaultParallelism:", DEFAULT_PAR)
print("[NLP] Using partitions:", N_PARTITIONS)
print("[NLP] SIM_THRESHOLD:", SIM_THRESHOLD, "TOP_K:", TOP_K, "BATCH_SIZE:", BATCH_SIZE)

# Recommended (safe) spark conf for Pandas UDF
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
spark.conf.set("spark.sql.execution.arrow.maxRecordsPerBatch", "10000")
spark.conf.set("spark.python.worker.reuse", "true")

# =========================================================
# 1) SINGLETON MODEL LOADER (per executor)
# =========================================================
_SPACY_MODEL = None

def get_spacy_model():
    """
    Load once per executor. Use MD vectors for semantic similarity.
    Disable heavy pipes.
    Use sentencizer to get sentences without dependency parser.
    """
    global _SPACY_MODEL
    if _SPACY_MODEL is None:
        print("Loading spaCy model on executor...")
        _SPACY_MODEL = spacy.load(
            "en_core_web_md",
            exclude=["ner", "lemmatizer", "attribute_ruler"]
        )
        # IMPORTANT: do NOT use dependency parser; we add sentencizer instead
        if "parser" in _SPACY_MODEL.pipe_names:
            _SPACY_MODEL.disable_pipes("parser")

        # Sentencizer gives doc.sents cheaply
        if "sentencizer" not in _SPACY_MODEL.pipe_names:
            _SPACY_MODEL.add_pipe("sentencizer")

        # Optional: prevent extreme text from crashing
        _SPACY_MODEL.max_length = 2_000_000

    return _SPACY_MODEL


# =========================================================
# 2) SCHEMA (matches your downstream expectations)
# =========================================================
job_features_schema = StructType([
    StructField("date", StringType()),
    StructField("job_id", StringType()),
    StructField("soc_2020", StringType()),
    StructField("n_chunks_total", IntegerType()),
    StructField("n_chunks_data", IntegerType()),
    StructField("avg_sim_data", DoubleType()),
    StructField("top_chunks", ArrayType(StringType())),
    StructField("top_sims", ArrayType(DoubleType())),
])


# =========================================================
# 3) WORKER FUNCTION (mapInPandas)
# =========================================================
def extract_job_features_oecd_style(iterator):
    nlp = get_spacy_model()
    target = nlp("data")  # target concept vector

    for pdf in iterator:
        # Defensive checks
        for col in ["full_text", "job_id", "soc_2020", "date"]:
            if col not in pdf.columns:
                # yield empty frame with expected columns
                yield pd.DataFrame(columns=[f.name for f in job_features_schema.fields])
                continue

        pdf["full_text"] = pdf["full_text"].fillna("").astype(str)

        texts = pdf["full_text"].tolist()
        docs = nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=1)

        out = []

        # iterate doc-by-doc
        for doc, job_id, soc, date in zip(docs, pdf["job_id"], pdf["soc_2020"], pdf["date"]):
            try:
                total_chunks = 0
                data_chunks = []

                # OECD-style: scan chunks of text and count "data-related" ones
                for sent in doc.sents:
                    txt = sent.text.strip()
                    if not txt:
                        continue

                    total_chunks += 1

                    # Similarity only meaningful if vectors exist
                    if sent.has_vector:
                        sim = float(sent.similarity(target))
                        if sim >= SIM_THRESHOLD:
                            cleaned = txt.lower().replace("\n", " ").strip()[:MAX_CHARS_PER_CHUNK]
                            data_chunks.append((cleaned, sim))

                n_data = len(data_chunks)

                if n_data > 0:
                    data_chunks.sort(key=lambda x: x[1], reverse=True)
                    top_items = data_chunks[:TOP_K]
                    top_chunks = [t for t, _ in top_items]
                    top_sims = [s for _, s in top_items]
                    avg_sim = float(sum(s for _, s in data_chunks) / n_data)
                else:
                    top_chunks, top_sims, avg_sim = [], [], 0.0

                out.append({
                    "date": str(date),
                    "job_id": str(job_id),
                    "soc_2020": str(soc),
                    "n_chunks_total": int(total_chunks),
                    "n_chunks_data": int(n_data),
                    "avg_sim_data": float(avg_sim),
                    "top_chunks": top_chunks,
                    "top_sims": top_sims
                })

            except Exception:
                # Never crash a partition for a few bad rows
                continue

        yield pd.DataFrame(out)


# =========================================================
# 4) EXECUTION LOOP (DAP SAFE)
# =========================================================
for yr in tqdm(years_to_process, desc="NLP job feature extraction (OECD-style)"):
    out_base = ALL_PATHS[yr]["job_features"]

    if not FORCE_RECOMPUTE:
        try:
            read_latest_version(out_base)
            print(f"\n[SKIP] Year {yr} job_features already exist.")
            continue
        except Exception:
            pass

    print(f"\n[NLP] Year {yr} starting...")

    df = all_data[yr]

    # Keep only needed cols to reduce memory/shuffle
    # Adjust column names if your raw schema differs
    needed = ["date", "job_id", "soc_2020", "full_text"]
    df = df.select(*[c for c in needed if c in df.columns])

    print(f"[NLP] Year {yr} input cols:", df.columns)

    # Throttle concurrency (prevents RPC disconnects)
    df_part = df.repartition(N_PARTITIONS)

    job_feat = df_part.mapInPandas(extract_job_features_oecd_style, schema=job_features_schema)

    # Add year/month columns safely
    job_feat = (
        job_feat
        .withColumn("date_parsed", F.to_date(F.col("date")))
        .withColumn("year", F.year(F.col("date_parsed")))
        .withColumn("month", F.month(F.col("date_parsed")))
        .drop("date_parsed")
    )

    # Optional small sanity sample (cheap)
    print("[NLP] Sample job_feat rows:")
    job_feat.limit(5).show(truncate=False)

    # Write (verify_read=False is good for huge)
    latest_written = safe_write_parquet_s3a(
        job_feat,
        out_base,
        mode="overwrite",
        create_version=True,
        verify_read=False
    )
    print(f"[NLP] Year {yr} written to: {latest_written}")

    # Cleanup
    try:
        df_part.unpersist()
    except Exception:
        pass

    del df_part
    del job_feat
    gc.collect()

print("\n[NLP] DONE — OECD-style job_features extraction completed for all years.")













print(f"\n[NLP] Year {yr} starting...")
df = all_data[yr]

# Keep only needed cols to reduce shuffle/memory
needed = ["date", "job_id", "soc_2020", "full_text"]
df = df.select(*[c for c in needed if c in df.columns])

# ------------------------------------------------------------
# PARTITIONING (OLD STYLE — SAME AS YOUR SCREENSHOT)
# ------------------------------------------------------------
print("[NLP] Counting rows to size partitions (this triggers a Spark job)...")
total_rows = df.count()

estimated_row_size = 1000  # bytes per row (your assumption)
target_partition_bytes = 128 * 1024 * 1024  # 128MB

num_partitions = max(
    16,
    int((total_rows * estimated_row_size) / target_partition_bytes)
)

# optional safety cap to avoid spawning too many python workers at once
# (important on DAP when using spaCy)
num_partitions = min(num_partitions, 200)

print(f"[NLP] Year {yr} rows={total_rows:,} partitions={num_partitions}")

df_part = df.repartition(num_partitions)

# Run the UDF
job_feat = df_part.mapInPandas(extract_job_features_oecd_style, schema=job_features_schema)

# Add year/month columns safely
job_feat = (
    job_feat
    .withColumn("date_parsed", F.to_date(F.col("date")))
    .withColumn("year", F.year(F.col("date_parsed")))
    .withColumn("month", F.month(F.col("date_parsed")))
    .drop("date_parsed")
)

print("[NLP] Sample job_feat rows:")
job_feat.limit(5).show(truncate=False)

latest_written = safe_write_parquet_s3a(
    job_feat,
    out_base,
    mode="overwrite",
    create_version=True,
    verify_read=False
)

print(f"[NLP] Year {yr} written to: {latest_written}")

# Cleanup
del df_part
del job_feat
gc.collect()
