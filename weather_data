import pandas as pd
import re

file_path = "/Users/saurabhkumar/Desktop/armaghdata.txt"
output_csv = "/Users/saurabhkumar/Desktop/armaghdata.csv"

clean_rows = []

with open(file_path, "r") as f:
    for line in f:
        # Keep only lines that start with a year

        if re.match(r"^\s*\d{4}", line):  # starts with year
            line = line.strip()
            parts = re.split(r'\s+', line)

            if len(parts) >= 7:
                # Optional: REMOVE footnotes like * or # or trailing text
                # -----------------------------------------
                #Uncomment the following block to clean footnotes:
                for i in range(2, 7):  # only clean columns tmax to sun
                    parts[i] = re.sub(r"[#*]", "", parts[i])
                if len(parts) > 7:  # If extra notes like "Provisional" exist
                    parts = parts[:7]
                # -----------------------------------------

                clean_rows.append(parts[:7])

df = pd.DataFrame(clean_rows, columns=["year", "month", "tmax", "tmin", "af", "rain", "sun"])
df.to_csv(output_csv, index=False)

One-to-One Feedback ‚Äì Talking Points for Ayoola
üìù ServiceNow Request Template (Final Version for DAP & Artifactory)


en_core_web_md (Medium model, 3.7.1)
	‚Ä¢	Hugging Face page: https://huggingface.co/spacy/en_core_web_md
	‚Ä¢	Direct .whl file: https://huggingface.co/spacy/en_core_web_md/resolve/main/en_core_web_md-any-py3-none-any.whl

en_core_web_lg (Large model, 3.7.1)
	‚Ä¢	Hugging Face page: https://huggingface.co/spacy/en_core_web_lg
	‚Ä¢	Direct .whl file: https://huggingface.co/spacy/en_core_web_lg/resolve/main/en_core_web_lg-any-py3-none-any.whl

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">




from pathlib import Path

# Font Awesome CDN to inject
FA_CDN = '<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">'

def get_local_site_path():
    """Find the local_site folder relative to this script."""
    here = Path(__file__).resolve()
    root = here.parent
    gx_path = root / "great_expectations" / "uncommitted" / "data_docs" / "local_site"
    if not gx_path.exists():
        print(f"‚ùå Could not find: {gx_path}")
        exit(1)
    return gx_path

def patch_html_file(file_path: Path):
    """Inject FA link if not already present."""
    try:
        text = file_path.read_text(encoding="utf-8")
        if FA_CDN in text:
            return False  # Already patched
        # Insert FA just after <head>
        patched_text = text.replace("<head>", f"<head>\n    {FA_CDN}")
        file_path.write_text(patched_text, encoding="utf-8")
        return True
    except Exception as e:
        print(f"‚ùå Error patching {file_path}: {e}")
        return False

def patch_all_docs():
    local_site_path = get_local_site_path()
    patched_files = 0

    for html_file in local_site_path.rglob("*.html"):
        if patch_html_file(html_file):
            print(f"‚úÖ Patched: {html_file.relative_to(local_site_path)}")
            patched_files += 1

    if patched_files == 0:
        print("‚úîÔ∏è No changes needed ‚Äî all files already patched.")
    else:
        print(f"üîß Done ‚Äî {patched_files} file(s) patched.")

if __name__ == "__main__":
    patch_all_docs()


from pyspark.sql import functions as F
from pyspark.sql import types as T
from datetime import datetime

# ----------------- CONFIGURE THESE -----------------
SOURCE_PARQUET = "s3a://your-bucket/path/to/parquet_root"
DATE_COL       = "date"                      # your date column name
KEEP_COLS      = ["date", "job_id", "job_title", "full_text"]

START_DATE     = "2019-06-01"                # inclusive
END_DATE       = "2019-06-30"                # inclusive

# Where to write the CSV result (NEW prefix so original data is unchanged)
OUT_PREFIX     = "s3a://your-bucket/path/to/exports"
# ---------------------------------------------------

# 1) Read the dataset
df = spark.read.parquet(SOURCE_PARQUET)

# 2) Parse/normalize the date column to real DATE type (robust to a few formats)
#    We'll try several common formats and fall back if it's already DateType.
fmt_attempts = [
    "yyyy-MM-dd", "yyyy/MM/dd", "dd-MM-yyyy", "dd/MM/yyyy",
    "yyyyMMdd", "MM-dd-yyyy", "MM/dd/yyyy"
]

if dict(df.dtypes)[DATE_COL] == 'date':
    df_with_date = df.withColumn("__date", F.col(DATE_COL).cast(T.DateType()))
else:
    parsed = None
    for fmt in fmt_attempts:
        expr = F.to_date(F.col(DATE_COL), fmt)
        parsed = expr if parsed is None else F.coalesce(parsed, expr)
    # if everything failed, to_date without format (last resort)
    parsed = F.coalesce(parsed, F.to_date(F.col(DATE_COL)))
    df_with_date = df.withColumn("__date", parsed)

# 3) Filter by the date range (inclusive)
start_lit = F.lit(START_DATE).cast(T.DateType())
end_lit   = F.lit(END_DATE).cast(T.DateType())
filtered  = df_with_date.filter(F.col("__date").between(start_lit, end_lit))

# 4) Select required columns (keeping original 'date' as-is)
available = [c for c in KEEP_COLS if c in filtered.columns]
missing   = [c for c in KEEP_COLS if c not in filtered.columns]
if missing:
    print("Warning: these expected columns are missing:", missing)

result = filtered.select(*available)

# 5) Count the rows returned
row_count = result.count()
print(f"Rows fetched in range [{START_DATE} .. {END_DATE}] : {row_count}")

# 6) Write to CSV on S3A under a NEW timestamped subfolder (original parquet untouched)
stamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
out_path = f"{OUT_PREFIX.rstrip('/')}/date_range={START_DATE}_to_{END_DATE}_{stamp}"

# NOTE:
# - header=True for column names
# - repartition(1) if you really want a single CSV file (can be slow/large)
#   Otherwise, keep the natural parallelism.
(result
    # .repartition(1)                     # <-- uncomment for a single CSV file
    .write
    .mode("overwrite")
    .option("header", "true")
    .csv(out_path)
)

print("CSV written to:", out_path)

# 7) Read it back to confirm
read_back = (spark.read
             .option("header", "true")
             .csv(out_path))

print("Read-back row count:", read_back.count())
read_back.show(5, truncate=False)
