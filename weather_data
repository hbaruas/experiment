import pandas as pd
import re

file_path = "/Users/saurabhkumar/Desktop/armaghdata.txt"
output_csv = "/Users/saurabhkumar/Desktop/armaghdata.csv"

clean_rows = []

with open(file_path, "r") as f:
    for line in f:
        # Keep only lines that start with a year

        if re.match(r"^\s*\d{4}", line):  # starts with year
            line = line.strip()
            parts = re.split(r'\s+', line)

            if len(parts) >= 7:
                # Optional: REMOVE footnotes like * or # or trailing text
                # -----------------------------------------
                #Uncomment the following block to clean footnotes:
                for i in range(2, 7):  # only clean columns tmax to sun
                    parts[i] = re.sub(r"[#*]", "", parts[i])
                if len(parts) > 7:  # If extra notes like "Provisional" exist
                    parts = parts[:7]
                # -----------------------------------------

                clean_rows.append(parts[:7])

df = pd.DataFrame(clean_rows, columns=["year", "month", "tmax", "tmin", "af", "rain", "sun"])
df.to_csv(output_csv, index=False)

print(f"✅ CSV saved to: {output_csv}")

Hi David,

Thanks for the follow-up.

Just to reaffirm, we’ve not yet been able to use the Textkernel data in DAP, as mentioned earlier. The key blocker remains the unavailability of the en_core_web_md model, which is essential for semantic understanding in our classification work using SpaCy. I raised this with the DAP CATS team, and I’ve been told it’s now on their improvements backlog as of a few days ago.

In terms of progress, aside from the earlier demo, the main output so far is the classification of job adverts into distinct “buckets” — such as data analytics, data product, and databases — using a sample of 10,000 adverts from Reed. I’m happy to walk through that again if helpful.

I’ll also forward the most recent interaction I had with the DAP CATS team. It might help make the case to reprioritise this request so we can begin using the model in DAP with actual data.

Best regards,
Saurabh
