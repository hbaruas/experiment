import pandas as pd
import re

file_path = "/Users/saurabhkumar/Desktop/armaghdata.txt"
output_csv = "/Users/saurabhkumar/Desktop/armaghdata.csv"

clean_rows = []

with open(file_path, "r") as f:
    for line in f:
        # Keep only lines that start with a year

        if re.match(r"^\s*\d{4}", line):  # starts with year
            line = line.strip()
            parts = re.split(r'\s+', line)

            if len(parts) >= 7:
                # Optional: REMOVE footnotes like * or # or trailing text
                # -----------------------------------------
                #Uncomment the following block to clean footnotes:
                for i in range(2, 7):  # only clean columns tmax to sun
                    parts[i] = re.sub(r"[#*]", "", parts[i])
                if len(parts) > 7:  # If extra notes like "Provisional" exist
                    parts = parts[:7]
                # -----------------------------------------

                clean_rows.append(parts[:7])

df = pd.DataFrame(clean_rows, columns=["year", "month", "tmax", "tmin", "af", "rain", "sun"])
df.to_csv(output_csv, index=False)

One-to-One Feedback ‚Äì Talking Points for Ayoola
üìù ServiceNow Request Template (Final Version for DAP & Artifactory)


en_core_web_md (Medium model, 3.7.1)
	‚Ä¢	Hugging Face page: https://huggingface.co/spacy/en_core_web_md
	‚Ä¢	Direct .whl file: https://huggingface.co/spacy/en_core_web_md/resolve/main/en_core_web_md-any-py3-none-any.whl

en_core_web_lg (Large model, 3.7.1)
	‚Ä¢	Hugging Face page: https://huggingface.co/spacy/en_core_web_lg
	‚Ä¢	Direct .whl file: https://huggingface.co/spacy/en_core_web_lg/resolve/main/en_core_web_lg-any-py3-none-any.whl

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">




from pathlib import Path

# Font Awesome CDN to inject
FA_CDN = '<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">'

def get_local_site_path():
    """Find the local_site folder relative to this script."""
    here = Path(__file__).resolve()
    root = here.parent
    gx_path = root / "great_expectations" / "uncommitted" / "data_docs" / "local_site"
    if not gx_path.exists():
        print(f"‚ùå Could not find: {gx_path}")
        exit(1)
    return gx_path

def patch_html_file(file_path: Path):
    """Inject FA link if not already present."""
    try:
        text = file_path.read_text(encoding="utf-8")
        if FA_CDN in text:
            return False  # Already patched
        # Insert FA just after <head>
        patched_text = text.replace("<head>", f"<head>\n    {FA_CDN}")
        file_path.write_text(patched_text, encoding="utf-8")
        return True
    except Exception as e:
        print(f"‚ùå Error patching {file_path}: {e}")
        return False

def patch_all_docs():
    local_site_path = get_local_site_path()
    patched_files = 0

    for html_file in local_site_path.rglob("*.html"):
        if patch_html_file(html_file):
            print(f"‚úÖ Patched: {html_file.relative_to(local_site_path)}")
            patched_files += 1

    if patched_files == 0:
        print("‚úîÔ∏è No changes needed ‚Äî all files already patched.")
    else:
        print(f"üîß Done ‚Äî {patched_files} file(s) patched.")

if __name__ == "__main__":
    patch_all_docs()


from pyspark.sql import SparkSession

# ---- your existing DF ----
# df = spark.read.parquet("s3a://bucket/path/to/dataset")

# Make sure we use the s3a scheme
parquet_path = "s3://bucket/path/to/dataset"   # <-- your path
if parquet_path.startswith("s3://"):
    parquet_path = "s3a://" + parquet_path[len("s3://"):]

# Rows, cols, names
rows = df.count()
cols = len(df.columns)

print(f"Total Rows    : {rows}")
print(f"Total Columns : {cols}")
print("Column Names  :")
for i, c in enumerate(df.columns, 1):
    print(f"  {i:>3}. {c}")

# Total size (bytes) of all files under the S3 prefix
hc = spark._jsc.hadoopConfiguration()
fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hc)
p = spark._jvm.org.apache.hadoop.fs.Path(parquet_path)
total_bytes = fs.getContentSummary(p).getLength()

print(f"Parquet size  : {total_bytes/1024/1024:.2f} MB")
