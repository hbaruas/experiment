import pandas as pd
import re

file_path = "/Users/saurabhkumar/Desktop/armaghdata.txt"
output_csv = "/Users/saurabhkumar/Desktop/armaghdata.csv"

clean_rows = []

with open(file_path, "r") as f:
    for line in f:
        # Keep only lines that start with a year

        if re.match(r"^\s*\d{4}", line):  # starts with year
            line = line.strip()
            parts = re.split(r'\s+', line)

            if len(parts) >= 7:
                # Optional: REMOVE footnotes like * or # or trailing text
                # -----------------------------------------
                #Uncomment the following block to clean footnotes:
                for i in range(2, 7):  # only clean columns tmax to sun
                    parts[i] = re.sub(r"[#*]", "", parts[i])
                if len(parts) > 7:  # If extra notes like "Provisional" exist
                    parts = parts[:7]
                # -----------------------------------------

                clean_rows.append(parts[:7])

df = pd.DataFrame(clean_rows, columns=["year", "month", "tmax", "tmin", "af", "rain", "sun"])
df.to_csv(output_csv, index=False)

One-to-One Feedback ‚Äì Talking Points for Ayoola
üìù ServiceNow Request Template (Final Version for DAP & Artifactory)


en_core_web_md (Medium model, 3.7.1)
	‚Ä¢	Hugging Face page: https://huggingface.co/spacy/en_core_web_md
	‚Ä¢	Direct .whl file: https://huggingface.co/spacy/en_core_web_md/resolve/main/en_core_web_md-any-py3-none-any.whl

en_core_web_lg (Large model, 3.7.1)
	‚Ä¢	Hugging Face page: https://huggingface.co/spacy/en_core_web_lg
	‚Ä¢	Direct .whl file: https://huggingface.co/spacy/en_core_web_lg/resolve/main/en_core_web_lg-any-py3-none-any.whl

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">




from pathlib import Path

# Font Awesome CDN to inject
FA_CDN = '<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">'

def get_local_site_path():
    """Find the local_site folder relative to this script."""
    here = Path(__file__).resolve()
    root = here.parent
    gx_path = root / "great_expectations" / "uncommitted" / "data_docs" / "local_site"
    if not gx_path.exists():
        print(f"‚ùå Could not find: {gx_path}")
        exit(1)
    return gx_path

def patch_html_file(file_path: Path):
    """Inject FA link if not already present."""
    try:
        text = file_path.read_text(encoding="utf-8")
        if FA_CDN in text:
            return False  # Already patched
        # Insert FA just after <head>
        patched_text = text.replace("<head>", f"<head>\n    {FA_CDN}")
        file_path.write_text(patched_text, encoding="utf-8")
        return True
    except Exception as e:
        print(f"‚ùå Error patching {file_path}: {e}")
        return False

def patch_all_docs():
    local_site_path = get_local_site_path()
    patched_files = 0

    for html_file in local_site_path.rglob("*.html"):
        if patch_html_file(html_file):
            print(f"‚úÖ Patched: {html_file.relative_to(local_site_path)}")
            patched_files += 1

    if patched_files == 0:
        print("‚úîÔ∏è No changes needed ‚Äî all files already patched.")
    else:
        print(f"üîß Done ‚Äî {patched_files} file(s) patched.")

if __name__ == "__main__":
    patch_all_docs()





from pyspark.sql import functions as F, types as T

SOURCE_PARQUET = "s3a://bucket/path/to/parquet_root"
OUT_PREFIX     = "s3a://bucket/path/to/exports"
DATE_COL       = "date"
KEEP_COLS      = ["date", "job_id", "job_title", "full_text"]
START_DATE     = "2019-06-01"
END_DATE       = "2019-06-30"

# 1) Read source
df = spark.read.parquet(SOURCE_PARQUET)

# 2) Normalize/parse date to real DateType (safe if it's already date)
df2 = df.withColumn("__date",
    F.coalesce(
        F.col(DATE_COL).cast(T.DateType()),
        F.to_date(F.col(DATE_COL), "yyyy-MM-dd"),   # adjust/add formats if needed
        F.to_date(F.col(DATE_COL))
    )
)

# 3) Filter by inclusive range
filtered = df2.filter(F.col("__date").between(F.lit(START_DATE).cast(T.DateType()),
                                              F.lit(END_DATE).cast(T.DateType())))

# 4) Select columns
result = filtered.select(*[c for c in KEEP_COLS if c in filtered.columns])

# 5) Count rows returned
cnt = result.count()
print(f"Rows fetched in range [{START_DATE} .. {END_DATE}]: {cnt}")

# 6) Write CSV safely to S3A (original parquet untouched)
#    - quoteAll=True ensures EVERY field is quoted (handles commas/newlines)
#    - quote='"' and escape='"' means embedded quotes become doubled ("") per RFC 4180
from datetime import datetime
stamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
out_path = f"{OUT_PREFIX.rstrip('/')}/date_range={START_DATE}_to_{END_DATE}_{stamp}"

(result
  # .repartition(1)  # uncomment if you truly want a single file
  .write
  .mode("overwrite")
  .option("header", True)
  .option("quote", '"')
  .option("escape", '"')
  .option("quoteAll", True)      # <-- key for reliable round-trip with newlines
  .option("lineSep", "\n")       # normalize line endings
  .csv(out_path)
)

print("CSV written to:", out_path)

# 7) Read back with the SAME options + explicit schema
csv_schema = T.StructType([
    T.StructField("date",      T.StringType(), True),
    T.StructField("job_id",    T.StringType(), True),
    T.StructField("job_title", T.StringType(), True),
    T.StructField("full_text", T.StringType(), True),
])

read_back = (spark.read
    .option("header", True)
    .option("quote", '"')
    .option("escape", '"')
    .option("quoteAll", True)    # harmless on read; keeps behavior consistent
    .option("multiLine", True)   # <-- allows embedded newlines in quoted fields
    .option("mode", "PERMISSIVE")
    .schema(csv_schema)
    .csv(out_path)
    .withColumn("date", F.to_date("date", "yyyy-MM-dd"))
)

print("Read-back rows:", read_back.count())
read_back.show(5, truncate=False)
